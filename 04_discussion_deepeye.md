\section{Discussion}

The present study aimed to produce a practical and reliable example of a black box solution to the problem of the inverse Yarbus problem by classifying raw timeline and image data using a CNN model architecture. To our knowledge, this study was the first to provide a solution to determining mental state from eye movement data using each of the following: (1) Non-aggregated eye tracking data (x-coordinates, y-coordinates, pupil size), (2) timeline and image data formats (see Figure \@ref(fig:ave-subset)), and (3) a _black box_ CNN architecture. This study probed the relative predictive value of the x-coordinate, y-coordinate, and pupil size components of the eye movement data using a CNN. The CNN was able to decode the image and timeline data better than chance, although only the timeline datasets were decoded with state-of-the-art accuracy. Datasets with lower classification accuracies were not able to differentiate the cognitive processes underlying the Memorization task from the cognitive processes underlying the Search and Rate tasks. Decoding subsets of the data revealed that pupil size was the least informative component of the eye movement data. This pattern of findings was consistent between the exploratory and confirmatory datasets.

Although several aggregate eye movement features have been tested as task predictors, to our knowledge, no other study has assessed the predictive value of the data format (viz., data in the format of an image). Our results suggest that although CNNs are robust image classifiers, eye movement data is decoded in the standard timeline format more effectively than in image format. This may be a consequence of the relative resolution of these data formats. Over the span of the trial (six seconds), the eye movements occasionally overlapped. When there was an overlap in the image data format, the more recent data points overwrote the older data points. This resulted in some data loss that did not occur when the data was represented in the standard timeline format. Despite the loss of overwritten data, the image format was still decoded with better than chance accuracy. To further examine the viability of classifying task from eye movement image datasets, future research might consider decoding 3-dimensional data formats, or more complex color combinations capable of representing overlapping data points.

When considering the superior performance of the timeline data (c.f., image data), we must also consider the differences in the model architectures. Because the structure of the timeline and image data formats were different, the models decoding those data structures also needed to be different. Both models were auditioned individually to the same extent on the Exploratory dataset before being tested on the confirmatory dataset. The exploratory and confirmatory pattern of results for both model architectures were the same, suggesting that these results are relatively stable. An appropriately developed CNN should be capable of learning any arbitrary fucntion, but given the atheoretical approach used to develop these models, there exists the possibility that an unknown model architecture exists which would produce equal or better classification accuracies for the image data format (c.f., timeline data format). Despite this possibility, the convergence of these findings with other studies (see Table \@ref(tab:previous-studies)) suggests that the results of this study are approaching a ceiling for the potential predictive accuracy for eye tracking data.

Datasets with lower classification accuracies confused the Memorization condition with the Search and Rate conditions. This suggests that the eye movements associated with the memorization task are likely indicative of underlying cognitive processes that are shared by the Search and Rate tasks. Previous research [i.e., @krolRightLookJob2018] has attributed the inability to differentiate one condition from the others to a lack of clarity in the data. This attribution is supported in the data by evidence that the subset data, with fewer defined variables, classified the memorization task less accurately than the other tasks. In cases when the subsets were decoded equally as well as the main dataset, the Memorize condition was classified as accurately as the other conditions.

When determining the relative contributions of the the eye movement features used in this study (x-coordinates, y-coordinates, pupil size), pupil size data was consistently the least informative. When pupil size was removed from the exploratory and confirmatory timeline and image datasets, classification accuracy remained stable (c.f., XYP dataset). Furthermore, classification of the $\varnothing\varnothing$P subset was the lowest of all of the data subsets, and in one instance, was no better than chance.

The findings from the current study support the notion that black box CNNs are a viable approach to determining task from eye movement data. In a recent review, @lukanderInferringIntentAction2017 expressed concern regarding the lack of generalizability of black box approaches when decoding eye movement data. The current study showed a consistent pattern of results for the XYP timeline and image datasets, but some inconsistency in the pattern of results for the x- and y- coordinate subset comparisons. These findings suggest that the decoding decisions for the x- and y- coordinate subsets were less reliable, and may not be generalizable. This lack of reliability may be a product of overlap in the cognitive processes underlying the three tasks. Becuase the data subsets are all missing at least one dimension of the data, this lack of reliability may be attributable to the loss of meaningful data in the subsets. When the data provide fewer meaningul distinctions, more fine-grained inferences are required to distinguish the tasks<!-- reference? -->. As shown by @cocoClassificationVisualLinguistic2014, eye movement data can be more effectively decoded when the cognitive processes underlying the tasks are explicitly distinguishable. While the cognitive processes distinguishing memorizing, searching, or rating an image are intuitively different, the eye movements elicited from these cognitive processes are not easily differentiated. To correct for potential mismatches between the level of distinction provided by the data and the level of distinction required for accurate and reliable classification of the data, future research could more definitively conceptualize the cognitive processes underlying the task-at-hand.

In reality, the level of abstraction differentiating the tasks-at-hand will depend on the application. Classifying mental state from eye movement data is often carried out in an effort to advance technology to improve educational outcomes, strengthen the independence of physically and mentally handicapped individuals, or improve HCI's [koocahakiPredictingIntentionEye2018]. To this end, the use of consistently effective and efficient black box solutions can be justified.

Given the questionable reliability and generalizability surrounding the _black box_ nature of CNN classification, the current study first tested models on an exploratory dataset, then confirmed the outcome using a second unrelated dataset. Overall, the findings appear stable and generalizable. Although the timeline data outperformed the image data format, future studies that incorporate stimulus features have the potential to provide a solution to determining task from eye movement data that surpasses the current state-of-the-art. According to @bullingEyeContextRecognitionHighlevel2013, incorporating stimulus feature information into the dataset may provide information is diagnostic beyond decoding spatial location data alone. Alternatively,  @borjiDefendingYarbusEye2014a suggested that accounting for salient features in the the stimulus might leave little to no room for the classifier to consider mental state. If the goal is to improve classification accuracies for real-life applications, the inclusion of stimulus feature information in addition to the eye movement data may boost the classification accuracy of image data beyond that of the timeline data.
