---
title             : "Convolutional neural networks can decode eye movement data: A black box approach to predicting task from eye movements"
shorttitle        : "Deep learning and eye tracking"

author: 
  - name          : "Zachary J. Cole"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "238 Burnett Hall, Lincoln, NE 68588-0308"
    
    email         : "zachary@neurophysicole.com"
  - name          : "Karl M. Kuntzelman"
    affiliation   : "1"
  - name          : "Michael D. Dodd"
    affiliation   : "1"
  - name          : "Matthew R. Johnson"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Nebraska-Lincoln"
    
authornote        : >
  The data used for the exploratory and confirmatory analyses in the present manuscript are derived from experiments funded by NIH/NEI Grant 1R01EY022974 to MDD. Work done to develop the analysis approach was supported by NSF/EPSCoR grant #1632849 (MRJ and MDD). Additionally, this work was supported by the National Institute of General Medical Sciences of the National Institutes of Health [grant number P20 GM130461 awarded to MRJ and colleagues] and the Rural Drug Addiction Research Center at the University of Nebraska-Lincoln. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the University of Nebraska.

abstract          : >
  Previous attempts to classify task from eye movement data have relied on model architectures designed to emulate theoretically defined cognitive processes, and/or data that has been processed into aggregate (e.g., fixations, saccades) or statistical (e.g., fixation density) features. _Black box_ convolutional neural networks (CNNs) are capable of identifying relevant features in raw and minimally processed data and images, but difficulty interpreting the mechanisms underlying these model architectures has contributed to challenges in generalizing lab-trained CNNs to applied contexts. In the current study, a CNN classifier was used to classify task from two eye movement datasets (Exploratory and Confirmatory) in which participants searched, memorized, or rated indoor and outdoor scene images. The Exploratory dataset was used to tune the hyperparameters of the model, and the resulting model architecture was re-trained, validated, and tested on the Confirmatory dataset. The data were formatted into raw timeline data (i.e., x-coordinate, y-coordinate, pupil size) and minimally processed images. To further understand the relative informational value of the raw components of the eye movement data, the timeline and image datasets were broken down into subsets with one or more of the components of the data systematically removed. Average classification accuracies were compared between datasets and subsets. Classification of the timeline data consistently outperformed the image data. The Memorize condition was most often confused with the Search and Rate conditions. Pupil size was the least uniquely informative eye movement component when compared with the x- and y-coordinates. The general pattern of results for the Exploratory dataset was replicated in the Confirmatory dataset. Overall, the present study provides a practical and reliable black box solution to classifying task from eye movement data.
#should be 200 words (approximately) == 274 words

keywords          : "deep learning, eye tracking, convolutional neural network, cognitive state, endogenous attention"
# wordcount         : "7960" #knit word doc to get the count

bibliography      : ["references/blackbox_manuscript.bib"]

figsintext        : yes
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

mainfont          : Helvetica

header-includes:
   - \usepackage{multirow}
   - \usepackage{graphicx}
   - \usepackage{array}
   - \usepackage{setspace}
   - \captionsetup[figure]{font={stretch=1,scriptsize}}
   - \raggedbottom
  
# appendix          : "supp_analysis.Rmd"

documentclass     : "apa6"
classoption       : "man, donotrepeattitle" #doc or man
output            : papaja::apa6_pdf #apa6_docx or apa6_pdf
fig_caption       : yes
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

<!-- Intro -->
```{r child = "01_intro_deepeye.md"}
```

<!-- Methods -->
```{r child = "02_methods_deepeye.md"}
```

<!-- Results -->
```{r child = "03_results_deepeye.md"}
```

<!-- Discussion -->
```{r child = "04_discussion_deepeye.md"}
```

\newpage
<!-- References -->
# References
```{r create_r-references}
r_refs(file = "references/blackbox_manuscript_r.bib")
my_citations <- cite_r(file = "references/blackbox_manuscript_r.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage
<!-- Appendix -->
```{r child = "05_supp_analysis.md"}
```