---
title             : "Convolutional neural networks can decode eye movement data: A black box approach to predicting task from eye movements"
shorttitle        : "Deep learning and eye tracking"

author: 
  - name          : "Zachary J. Cole"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "238 Burnett Hall, Lincoln, NE 68588-0308"
    
    email         : "zachary@neurophysicole.com"
  - name          : "Karl M. Kuntzelman"
    affiliation   : "1"
  - name          : "Michael D. Dodd"
    affiliation   : "1"
  - name          : "Matthew R. Johnson"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Nebraska-Lincoln"
    
authornote        : >
  The data used for the exploratory and confirmatory analyses in the present manuscript are derived from experiments funded by NIH/NEI Grant 1R01EY022974 to MDD. Work done to develop the analysis approach was supported by NSF/EPSCoR grant #1632849 (MRJ and MDD). Additionally, this work was supported by the National Institute of General Medical Sciences of the National Institutes of Health [grant number P20 GM130461 awarded to MRJ and colleagues] and the Rural Drug Addiction Research Center at the University of Nebraska-Lincoln. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the University of Nebraska.

abstract          : >
  Previous attempts to classify task from eye movement data have relied on model architectures designed to emulate theoretically defined cognitive processes, and/or data that has been processed into aggregate (e.g., fixations, saccades) or statistical (e.g., fixation density) features. _Black box_ convolutional neural networks (CNNs) are capable of identifying relevant features in raw and minimally processed data and images, but difficulty interpreting these model architectures has contributed to challenges in generalizing lab-trained CNNs to applied contexts. In the current study, a CNN classifier was used to classify task from two eye movement datasets (Exploratory and Confirmatory) in which participants searched, memorized, or rated indoor and outdoor scene images. The Exploratory dataset was used to tune the hyperparameters of the model, and the resulting model architecture was re-trained, validated, and tested on the Confirmatory dataset. The data were formatted into timelines (i.e., x-coordinate, y-coordinate, pupil size) and minimally processed images. To further understand the informational value of each component of the eye movement data, the timeline and image datasets were broken down into subsets with one or more components systematically removed. Classification of the timeline data consistently outperformed the image data. The Memorize condition was most often confused with Search and Rate. Pupil size was the least uniquely informative component when compared with the x- and y-coordinates. The general pattern of results for the Exploratory dataset was replicated in the Confirmatory dataset. Overall, the present study provides a practical and reliable black box solution to classifying task from eye movement data.
#should be 200 words (approximately) == 274 words

keywords          : "deep learning, eye tracking, convolutional neural network, cognitive state, endogenous attention"
# wordcount         : "7960" #knit word doc to get the count

bibliography      : ["references/blackbox_manuscript.bib"]

figsintext        : yes
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

mainfont          : Helvetica

header-includes:
   - \usepackage{multirow}
   - \usepackage{graphicx}
   - \usepackage{array}
   - \usepackage{setspace}
   - \captionsetup[figure]{font={stretch=1,scriptsize}}
   - \raggedbottom
  
# appendix          : "supp_analysis.Rmd"

documentclass     : "apa6"
classoption       : "man, donotrepeattitle" #doc or man
output            : papaja::apa6_pdf #apa6_docx or apa6_pdf
fig_caption       : yes
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

<!-- REVISION CHECKLIST -->
# Revisions to be made
- clarify the difference between the Yarbus problem and the Inverse Yarbus problem; something like what Greene et al. did
- [ just in case it is not covered by the previous bullet ] go through and clarify what the Inverse Yarbus Problem even is :: MD S
- make it more clear that the data we classified was not raw, but was actually processed
- do something with the Coco & Keller replication results ( Should we present in another supplement.. or should we put into the results section as well?? Or should we just report to the reviewers? [I think the reviewers wanted it included in the paper though...] Might be worth reading throuhg the manuscript in its current form and deciding.. )
- Clarify the terms "task", "cognitive process", and "mental state"..
- [ just in case it is not coverede by the previous bullet ] Go through and check that the wording of "task", "cognitive process", and "mental state" are used consistently throughout the manuscript
- further clarify how studies do not fit the spirit of the Inverse Yarbus Problem
 - **MD Comment:** Important to note that all images are same general structure (interior of rooms/locations, no people) and no one is asked to answer specific questions about the image as they did with Yarbus.  The point of the instruction is to not constrain how they look at the image because in Yarbus, of course if you ask about the age of someone they look at the face and if you ask about the wealth they look at material belongings.  The instructions in these studies are completely unbiased so as to not influence in any way what people do
- Need to clarify what it is we intend to find from comparing the timeline and image data formats
 - **MD Comment:** Are they asking for the image data to be removed?  I can agree with the reviewer that there might be a bit of a disconnect in the paper as to why the timeline data and image data were what was selected to be compared but if the argument is that the image model was never going to be as useful as the timeline one, then that would suggest the reason for doing the image one at all is less clear
- Provide more comprehensive model information
- [ just in case it is not coverede by the previous bullet ] Add in the learning rate information

# Reviewer 2 issues that still need to be addressed
- training/test question..
- was there an overfitting issue?
- Figure out what they mean with the ANOVA comparison issue..
- overfitting due to pupil size?



<!-- Intro -->
```{r child = "01_intro_deepeye.md"}
```

<!-- Methods -->
```{r child = "02_methods_deepeye.md"}
```

<!-- Results -->
```{r child = "03_results_deepeye.md"}
```

<!-- Discussion -->
```{r child = "04_discussion_deepeye.md"}
```

\newpage
<!-- References -->
# References
```{r create_r-references}
r_refs(file = "references/blackbox_manuscript_r.bib")
my_citations <- cite_r(file = "references/blackbox_manuscript_r.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage
<!-- Appendix -->
```{r child = "05_supp_analysis.md"}
```