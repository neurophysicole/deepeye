
@article{boisvertPredictingTaskEye2016,
  title = {Predicting Task from Eye Movements: {{On}} the Importance of Spatial Distribution, Dynamics, and Image Features},
  shorttitle = {Predicting Task from Eye Movements},
  author = {Boisvert, Jonathan F.G. and Bruce, Neil D.B.},
  date = {2016-09},
  journaltitle = {Neurocomputing},
  volume = {207},
  pages = {653--668},
  issn = {09252312},
  doi = {10.1016/j.neucom.2016.05.047},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216304131},
  urldate = {2019-10-20},
  abstract = {Yarbus' pioneering work in eye tracking has been influential to methodology and in demonstrating the apparent importance of task in eliciting different fixation patterns. There has been renewed interest in Yarbus' assertions on the importance of task in recent years, driven in part by a greater capability to apply quantitative methods to fixation data analysis. A number of recent research efforts have examined the extent to which an observer's task may be predicted from recorded fixation data. This body of recent work has raised a number of interesting questions, with some investigations calling for closer examination of the validity of Yarbus' claims, and subsequent efforts revealing some of the nuances involved in carrying out this type of analysis including both methodological, and data related considerations. In this paper, we present an overview of prior efforts in task prediction, and assess different types of statistics drawn from fixation data, or images in their ability to predict task from gaze. We also examine the extent to which relatively general task definitions (free-viewing, object-search, saliency-viewing, explicit saliency) may be predicted by spatial positioning of fixations, features co-located with fixation points, fixation dynamics and scene structure. This is accomplished in considering the data of Koehler et al. (2014) [30] affording a larger scale, and qualitatively different corpus of data for task prediction relative to existing efforts. Based on this analysis, we demonstrate that both spatial position, as well as local features are of value in distinguishing general task categories. The methods proposed provide a general framework for highlighting features that distinguish behavioural differences observed across visual tasks, and we relate new task prediction results in this paper to the body of prior work in this domain. Finally, we also comment on the value of task prediction and classification models in general in understanding facets of gaze behaviour.},
  file = {/Users/zcole/Box/Zoterox/storage/37V7ZTN4/Boisvert and Bruce - 2016 - Predicting task from eye movements On the importa.pdf},
  langid = {english}
}

@article{borjiDefendingYarbusEye2014a,
  title = {Defending {{Yarbus}}: {{Eye}} Movements Reveal Observers' Task},
  shorttitle = {Defending {{Yarbus}}},
  author = {Borji, A. and Itti, L.},
  date = {2014-03-24},
  journaltitle = {Journal of Vision},
  volume = {14},
  pages = {29--29},
  issn = {1534-7362},
  doi = {10.1167/14.3.29},
  url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/14.3.29},
  urldate = {2019-10-20},
  file = {/Users/zcole/Box/Zoterox/storage/PU6SUKS6/Borji and Itti - 2014 - Defending Yarbus Eye movements reveal observers' .pdf},
  langid = {english},
  number = {3}
}

@inproceedings{bullingEyeContextRecognitionHighlevel2013,
  title = {{{EyeContext}}: {{Recognition}} of High-Level Contextual Cues from Human Visual Behaviour},
  shorttitle = {{{EyeContext}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '13},
  author = {Bulling, Andreas and Weichel, Christian and Gellersen, Hans},
  date = {2013},
  pages = {305},
  publisher = {{ACM Press}},
  location = {{Paris, France}},
  doi = {10.1145/2470654.2470697},
  url = {http://dl.acm.org/citation.cfm?doid=2470654.2470697},
  urldate = {2020-02-09},
  abstract = {In this work we present EyeContext, a system to infer highlevel contextual cues from human visual behaviour. We conducted a user study to record eye movements of four participants over a full day of their daily life, totalling 42.5 hours of eye movement data. Participants were asked to self-annotate four non-mutually exclusive cues: social (interacting with somebody vs. no interaction), cognitive (concentrated work vs. leisure), physical (physically active vs. not active), and spatial (inside vs. outside a building). We evaluate a proofof-concept EyeContext system that combines encoding of eye movements into strings and a spectrum string kernel support vector machine (SVM) classifier. Our results demonstrate the large information content available in long-term human visual behaviour and opens up new venues for research on eye-based behavioural monitoring and life logging.},
  eventtitle = {The {{SIGCHI Conference}}},
  file = {/Users/zcole/Box/Zoterox/storage/XPZWRNU5/Bulling et al. - 2013 - EyeContext recognition of high-level contextual c.pdf},
  isbn = {978-1-4503-1899-0},
  langid = {english}
}

@article{castelhanoViewingTaskInfluences2009,
  title = {Viewing Task Influences Eye Movement Control during Active Scene Perception},
  author = {Castelhano, M. S. and Mack, M. L. and Henderson, J. M.},
  date = {2009-03-01},
  journaltitle = {Journal of Vision},
  volume = {9},
  pages = {6--6},
  issn = {1534-7362},
  doi = {10.1167/9.3.6},
  url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/9.3.6},
  urldate = {2019-01-24},
  abstract = {Expanding on the seminal work of G. Buswell (1935) and I. A. Yarbus (1967), we investigated how task instruction influences specific parameters of eye movement control. In the present study, 20 participants viewed color photographs of natural scenes under two instruction sets: visual search and memorization. Results showed that task influenced a number of eye movement measures including the number of fixations and gaze duration on specific objects. Additional analyses revealed that the areas fixated were qualitatively different between the two tasks. However, other measures such as average saccade amplitude and individual fixation durations remained constant across the viewing of the scene and across tasks. The present study demonstrates that viewing task biases the selection of scene regions and aggregate measures of fixation time on those regions but does not influence other measures, such as the duration of individual fixations.},
  file = {/Users/zcole/Box/Zoterox/storage/8N7W4SVH/Castelhano et al. - 2009 - Viewing task influences eye movement control durin.pdf},
  langid = {english},
  number = {3}
}

@article{cocoClassificationVisualLinguistic2014,
  title = {Classification of Visual and Linguistic Tasks Using Eye-Movement Features},
  author = {Coco, Moreno I. and Keller, Frank},
  date = {2014-03-01},
  journaltitle = {Journal of Vision},
  volume = {14},
  pages = {11--11},
  issn = {1534-7362},
  doi = {10.1167/14.3.11},
  url = {http://jov.arvojournals.org/article.aspx?articleid=2121494},
  urldate = {2020-02-07},
  file = {/Users/zcole/Box/Zoterox/storage/A6FQPWQ8/Coco and Keller - 2014 - Classification of visual and linguistic tasks usin.pdf;/Users/zcole/Box/Zoterox/storage/VR66K7XW/article.html},
  langid = {english},
  number = {3}
}

@article{coutrotScanpathModelingClassification2018a,
  title = {Scanpath Modeling and Classification with Hidden {{Markov}} Models},
  author = {Coutrot, Antoine and Hsiao, Janet H. and Chan, Antoni B.},
  date = {2018-02-01},
  journaltitle = {Behav Res},
  volume = {50},
  pages = {362--379},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0876-8},
  url = {https://doi.org/10.3758/s13428-017-0876-8},
  urldate = {2020-01-05},
  abstract = {How people look at visual information reveals fundamental information about them; their interests and their states of mind. Previous studies showed that scanpath, i.e., the sequence of eye movements made by an observer exploring a visual stimulus, can be used to infer observer-related (e.g., task at hand) and stimuli-related (e.g., image semantic category) information. However, eye movements are complex signals and many of these studies rely on limited gaze descriptors and bespoke datasets. Here, we provide a turnkey method for scanpath modeling and classification. This method relies on variational hidden Markov models (HMMs) and discriminant analysis (DA). HMMs encapsulate the dynamic and individualistic dimensions of gaze behavior, allowing DA to capture systematic patterns diagnostic of a given class of observers and/or stimuli. We test our approach on two very different datasets. Firstly, we use fixations recorded while viewing 800 static natural scene images, and infer an observer-related characteristic: the task at hand. We achieve an average of 55.9\% correct classification rate (chance = 33\%). We show that correct classification rates positively correlate with the number of salient regions present in the stimuli. Secondly, we use eye positions recorded while viewing 15 conversational videos, and infer a stimulus-related characteristic: the presence or absence of original soundtrack. We achieve an average 81.2\% correct classification rate (chance = 50\%). HMMs allow to integrate bottom-up, top-down, and oculomotor influences into a single model of gaze behavior. This synergistic approach between behavior and machine learning will open new avenues for simple quantification of gazing behavior. We release SMAC with HMM, a Matlab toolbox freely available to the community under an open-source license agreement.},
  file = {/Users/zcole/Box/Zoterox/storage/FZF4GS3L/Coutrot et al. - 2018 - Scanpath modeling and classification with hidden M.pdf},
  keywords = {Classification,Eye movements,Hidden Markov models,Machine-learning,Scanpath,Toolbox},
  langid = {english},
  number = {1}
}

@article{deangelusTopdownControlEye2009,
  title = {Top-down Control of Eye Movements: {{Yarbus}} Revisited},
  shorttitle = {Top-down Control of Eye Movements},
  author = {DeAngelus, Marianne and Pelz, Jeff B.},
  date = {2009-08},
  journaltitle = {Visual Cognition},
  volume = {17},
  pages = {790--811},
  issn = {1350-6285, 1464-0716},
  doi = {10.1080/13506280902793843},
  url = {http://www.tandfonline.com/doi/abs/10.1080/13506280902793843},
  urldate = {2019-01-24},
  file = {/Users/zcole/Box/Zoterox/storage/NKF4F6JQ/DeAngelus and Pelz - 2009 - Top-down control of eye movements Yarbus revisite.pdf},
  langid = {english},
  number = {6-7}
}

@article{greeneReconsideringYarbusFailure2012a,
  title = {Reconsidering {{Yarbus}}: {{A}} Failure to Predict Observers' Task from Eye Movement Patterns},
  shorttitle = {Reconsidering {{Yarbus}}},
  author = {Greene, Michelle R. and Liu, Tommy and Wolfe, Jeremy M.},
  date = {2012-06-01},
  journaltitle = {Vision Res},
  volume = {62},
  pages = {1--8},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2012.03.019},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3526937/},
  urldate = {2019-11-12},
  abstract = {In 1967, Yarbus presented qualitative data from one observer showing that the patterns of eye movements were dramatically affected by an observer's task, suggesting that complex mental states could be inferred from scan paths. The strong claim of this very influential finding has an never been rigorously tested. Our observers viewed photographs for 10 seconds each. They performed one of four image-based tasks while eye movements were recorded. A pattern classifier, given features from the static scan paths, could identify the image and the observer at above-chance levels. However, it could not predict a viewer's task. Shorter and longer (60 sec) viewing epochs produced similar results. Critically, human judges also failed to identify the tasks performed by the observers based on the static scan paths. The Yarbus finding is evocative, and while it is possible an observer's mental state might be decoded from some aspect of eye movements, static scan paths alone do not appear to be adequate to infer complex mental states of an observer.},
  eprint = {22487718},
  eprinttype = {pmid},
  pmcid = {PMC3526937}
}

@article{haji-abolhassaniInverseYarbusProcess2014,
  title = {An Inverse {{Yarbus}} Process: {{Predicting}} Observers’ Task from Eye Movement Patterns},
  shorttitle = {An Inverse {{Yarbus}} Process},
  author = {Haji-Abolhassani, Amin and Clark, James J.},
  date = {2014-10-01},
  journaltitle = {Vision Research},
  volume = {103},
  pages = {127--142},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2014.08.014},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698914002004},
  urldate = {2019-10-20},
  abstract = {In this paper we develop a probabilistic method to infer the visual-task of a viewer given measured eye movement trajectories. This method is based on the theory of hidden Markov models (HMM) that employs a first order Markov process to predict the coordinates of fixations given the task. The prediction confidence level of each task-dependent model is used in a Bayesian inference formulation, whereby the task with the maximum a posteriori (MAP) probability is selected. We applied this technique to a challenging dataset consisting of eye movement trajectories obtained from subjects viewing monochrome images of real scenes tasked with answering questions regarding the scenes. The results show that the HMM approach, combined with a clustering technique, can be a reliable way to infer visual-task from eye movements data.},
  file = {/Users/zcole/Box/Zoterox/storage/HL675L7S/Haji-Abolhassani and Clark - 2014 - An inverse Yarbus process Predicting observers’ t.pdf;/Users/zcole/Box/Zoterox/storage/7CB2TDHT/S0042698914002004.html},
  keywords = {-means clustering,Attention cognitive model,Eye movement,Hidden Markov model,Visual search,Visual-task inference},
  langid = {english}
}

@article{hendersonPredictingCognitiveState2013a,
  title = {Predicting {{Cognitive State}} from {{Eye Movements}}},
  author = {Henderson, John M. and Shinkareva, Svetlana V. and Wang, Jing and Luke, Steven G. and Olejarczyk, Jenn},
  editor = {Paterson, Kevin},
  date = {2013-05-29},
  journaltitle = {PLoS ONE},
  volume = {8},
  pages = {e64937},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0064937},
  url = {https://dx.plos.org/10.1371/journal.pone.0064937},
  urldate = {2019-12-01},
  abstract = {In human vision, acuity and color sensitivity are greatest at the center of fixation and fall off rapidly as visual eccentricity increases. Humans exploit the high resolution of central vision by actively moving their eyes three to four times each second. Here we demonstrate that it is possible to classify the task that a person is engaged in from their eye movements using multivariate pattern classification. The results have important theoretical implications for computational and neural models of eye movement control. They also have important practical implications for using passively recorded eye movements to infer the cognitive state of a viewer, information that can be used as input for intelligent human-computer interfaces and related applications.},
  file = {/Users/zcole/Box/Zoterox/storage/AIA2HAGA/Henderson et al. - 2013 - Predicting Cognitive State from Eye Movements.pdf},
  langid = {english},
  number = {5}
}

@article{iqbalUsingEyeGaze,
  title = {Using {{Eye Gaze Patterns}} to {{Identify User Tasks}}},
  author = {Iqbal, Shamsi T and Bailey, Brian P},
  pages = {6},
  abstract = {Users of today’s desktop interface often suffer from interruption overload. Our research seeks to develop an attention manager that mitigates the disruptive effects of interruptions by identifying moments of low mental workload in a user’s task sequence. To develop such a system, however, we need effective mechanisms to identify user tasks in real-time. In this paper, we show how eye gaze patterns may be used to identify user tasks. We also show that gaze patterns can indicate usability issues of an interface as well as the mental workload that the interface induces on a user. Our results can help inform the design of an attention manager and may lead to new methods to evaluate user interfaces.},
  file = {/Users/zcole/Box/Zoterox/storage/NUCPE4SW/Iqbal and Bailey - Using Eye Gaze Patterns to Identify User Tasks.pdf},
  langid = {english}
}

@article{kahnemanPupilDiameterLoad1966,
  title = {Pupil {{Diameter}} and {{Load}} on {{Memory}}},
  author = {Kahneman, Daniel and Beatty, Jackson},
  date = {1966},
  journaltitle = {Science},
  volume = {154},
  pages = {1583--1585},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  url = {https://www.jstor.org/stable/1720478},
  urldate = {2020-05-25},
  abstract = {During a short-term memory task, pupil diameter is a measure of the amount of material which is under active processing at any time. The pupil dilates as the material is presented and constricts during report. The rate of change of these functions is related to task difficulty.},
  number = {3756}
}

@inproceedings{kananPredictingObserverTask2014,
  title = {Predicting an Observer's Task Using Multi-Fixation Pattern Analysis},
  booktitle = {Proceedings of the {{Symposium}} on {{Eye Tracking Research}} and {{Applications}} - {{ETRA}} '14},
  author = {Kanan, Christopher and Ray, Nicholas A. and Bseiso, Dina N. F. and Hsiao, Janet H. and Cottrell, Garrison W.},
  date = {2014},
  pages = {287--290},
  publisher = {{ACM Press}},
  location = {{Safety Harbor, Florida}},
  doi = {10.1145/2578153.2578208},
  url = {http://dl.acm.org/citation.cfm?doid=2578153.2578208},
  urldate = {2019-10-20},
  abstract = {Since Yarbus’s seminal work in 1965, vision scientists have argued that people’s eye movement patterns differ depending upon their task. This suggests that we may be able to infer a person’s task (or mental state) from their eye movements alone. Recently, this was attempted by Greene et al. [2012] in a Yarbus-like replication study; however, they were unable to successfully predict the task given to their observer. We reanalyze their data, and show that by using more powerful algorithms it is possible to predict the observer’s task. We also used our algorithms to infer the image being viewed by an observer and their identity. More generally, we show how offthe-shelf algorithms from machine learning can be used to make inferences from an observer’s eye movements, using an approach we call Multi-Fixation Pattern Analysis (MFPA).},
  eventtitle = {The {{Symposium}}},
  file = {/Users/zcole/Box/Zoterox/storage/JZPKSJ4H/Kanan et al. - 2014 - Predicting an observer's task using multi-fixation.pdf},
  isbn = {978-1-4503-2751-0},
  langid = {english}
}

@article{karatekinAttentionAllocationDualtask2004,
  title = {Attention Allocation in the Dual-Task Paradigm as Measured through Behavioral and Psychophysiological Responses},
  author = {Karatekin, Canan and Couperus, Jane W. and Marcus, David J.},
  date = {2004},
  journaltitle = {Psychophysiology},
  volume = {41},
  pages = {175--185},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.2004.00147.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2004.00147.x},
  urldate = {2020-05-25},
  abstract = {We investigated attention allocation in a dual-task paradigm using behavioral and pupillary measures. We used an auditory digit span (DS) and a simple visual response time (RT) task. Participants were administered four conditions in which they performed neither task (no-task), a single task (DS or RT only), or both tasks (dual). Dependent variables were DS accuracy, RT, and task-evoked pupillary responses (TEPRs) to digits as estimates of mental effort. Participants maintained almost the same level of DS accuracy on dual as on DS only and sacrificed speed on the RT task. As expected, TEPRs increased linearly with memory load in both DS only and dual. Although TEPRs were initially higher in dual than in DS only, the slope of the increase was shallower in dual. Results suggest that TEPRs can elucidate mechanisms of attention allocation by distinguishing between effectiveness (level of behavioral performance) and efficiency (the costs of that performance in mental effort).},
  file = {/Users/zcole/Box/Zoterox/storage/AZ4I3ZXV/Karatekin et al. - 2004 - Attention allocation in the dual-task paradigm as .pdf;/Users/zcole/Box/Zoterox/storage/MYUCGBJN/j.1469-8986.2004.00147.html},
  keywords = {Divided attention,Dual task,Mental effort,Resources,Task-evoked pupillary responses,Working memory},
  langid = {english},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2004.00147.x},
  number = {2}
}

@inproceedings{koochakiPredictingIntentionEye2018,
  title = {Predicting {{Intention Through Eye Gaze Patterns}}},
  booktitle = {2018 {{IEEE Biomedical Circuits}} and {{Systems Conference}} ({{BioCAS}})},
  author = {Koochaki, Fatemeh and Najafizadeh, Laleh},
  date = {2018-10},
  pages = {1--4},
  issn = {2163-4025},
  doi = {10.1109/BIOCAS.2018.8584665},
  abstract = {Eye movement is a valuable (and in several cases, the only remaining) means of communication for impaired people with extremely limited motor or communication capabilities. In this paper, we present a new framework that utilizes eye gaze patterns as input, to predict user's intention for performing daily tasks. The proposed framework consists of two main modules. First, by clustering the eye gaze patterns, the regions of interest (ROIs) on the displayed image are extracted. A deep convolutional neural network is then trained and used to recognize the objects in each ROI. Finally, the intended task is predicted by using support vector machine (SVM) through learning the embedded relationship between recognized objects. The proposed framework is tested using data from 8 subjects, in an experiment considering 4 intended tasks as well as the scenario in which the user does not have a specific intention when looking at the displayed image. Results demonstrate an average accuracy of 95.68\% across all tasks, confirming the efficacy of the proposed framework.},
  eventtitle = {2018 {{IEEE Biomedical Circuits}} and {{Systems Conference}} ({{BioCAS}})},
  file = {/Users/zcole/Box/Zoterox/storage/3Q4KWQ3E/Koochaki and Najafizadeh - 2018 - Predicting Intention Through Eye Gaze Patterns.pdf;/Users/zcole/Box/Zoterox/storage/VU2BC2Y4/8584665.html},
  keywords = {Assistive Technologies,Calibration,convolution,Convolutional Neural Network,Convolutional neural networks,deep convolutional neural network,Detectors,displayed image,eye,eye gaze patterns,Eye Gaze Patterns,eye movement,Feature extraction,feedforward neural nets,gaze tracking,handicapped aids,impaired people,intended task,Intention Prediction,learning (artificial intelligence),object recognition,regions of interest,ROI,support vector machine,Support Vector Machine,support vector machines,Support vector machines,SVM,Task analysis,Training}
}

@article{krolRightLookJob2018,
  title = {The Right Look for the Job: {{Decoding}} Cognitive Processes Involved in the Task from Spatial Eye-Movement Patterns},
  shorttitle = {The Right Look for the Job},
  author = {Król, Magdalena Ewa and Król, Michał},
  date = {2018-02-20},
  journaltitle = {Psychological Research},
  issn = {1430-2772},
  doi = {10.1007/s00426-018-0996-5},
  url = {https://doi.org/10.1007/s00426-018-0996-5},
  urldate = {2019-10-20},
  abstract = {The aim of the study was not only to demonstrate whether eye-movement-based task decoding was possible but also to investigate whether eye-movement patterns can be used to identify cognitive processes behind the tasks. We compared eye-movement patterns elicited under different task conditions, with tasks differing systematically with regard to the types of cognitive processes involved in solving them. We used four tasks, differing along two dimensions: spatial (global vs. local) processing (Navon, Cognit Psychol, 9(3):353–383 1977) and semantic (deep vs. shallow) processing (Craik and Lockhart, J Verbal Learn Verbal Behav, 11(6):671–684 1972). We used eye-movement patterns obtained from two time periods: fixation cross preceding the target stimulus and the target stimulus. We found significant effects of both spatial and semantic processing, but in case of the latter, the effect might be an artefact of insufficient task control. We found above chance task classification accuracy for both time periods: 51.4\% for the period of stimulus presentation and 34.8\% for the period of fixation cross presentation. Therefore, we show that task can be to some extent decoded from the preparatory eye-movements before the stimulus is displayed. This suggests that anticipatory eye-movements reflect the visual scanning strategy employed for the task at hand. Finally, this study also demonstrates that decoding is possible even from very scant eye-movement data similar to Coco and Keller, J Vis 14(3):11–11 (2014). This means that task decoding is not limited to tasks that naturally take longer to perform and yield multi-second eye-movement recordings.},
  file = {/Users/zcole/Box/Zoterox/storage/FK86L55B/Król and Król - 2018 - The right look for the job decoding cognitive pro.pdf},
  langid = {english}
}

@article{lukanderInferringIntentAction2017,
  title = {Inferring {{Intent}} and {{Action}} from {{Gaze}} in {{Naturalistic Behavior}}: {{A Review}}},
  shorttitle = {Inferring {{Intent}} and {{Action}} from {{Gaze}} in {{Naturalistic Behavior}}},
  author = {Lukander, Kristian and Toivanen, Miika and Puolamäki, Kai},
  date = {2017-10},
  journaltitle = {International Journal of Mobile Human Computer Interaction},
  volume = {9},
  pages = {41--57},
  issn = {1942-390X, 1942-3918},
  doi = {10.4018/IJMHCI.2017100104},
  url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJMHCI.2017100104},
  urldate = {2019-10-20},
  abstract = {We constantly move our gaze to gather acute visual information from our environment. Conversely, as originally shown by Yarbus in his seminal work, the elicited gaze patterns hold information over our changing attentional focus while performing a task. Recently, the proliferation of machine learning algorithms has allowed the research community to test the idea of inferring, or even predicting action and intent from gaze behaviour. The on-going miniaturization of gaze tracking technologies toward pervasive wearable solutions allows studying inference also in everyday activities outside research laboratories. This paper scopes the emerging field and reviews studies focusing on the inference of intent and action in naturalistic behaviour. While the task-specific nature of gaze behavior, and the variability in naturalistic setups present challenges, gaze-based inference holds a clear promise for machine-based understanding of human intent and future interactive solutions.},
  file = {/Users/zcole/Box/Zoterox/storage/7Y8CMDB7/Lukander et al. - 2017 - Inferring Intent and Action from Gaze in Naturalis.pdf},
  langid = {english},
  number = {4}
}

@article{macinnesjosephGenerativeModelCognitive2018,
  title = {A {{Generative Model}} of {{Cognitive State}} from {{Task}} and {{Eye Movements}}},
  author = {MacInnes, Joseph, W. and Hunt, Amelia R. and Clarke, Alasdair D. F. and Dodd, Michael D.},
  date = {2018-10},
  journaltitle = {Cognitive Computation},
  volume = {10},
  pages = {703--717},
  issn = {1866-9956, 1866-9964},
  doi = {10.1007/s12559-018-9558-9},
  url = {http://link.springer.com/10.1007/s12559-018-9558-9},
  urldate = {2019-01-16},
  abstract = {The early eye tracking studies of Yarbus provided descriptive evidence that an observer’s task influences patterns of eye movements, leading to the tantalizing prospect that an observer’s intentions could be inferred from their saccade behavior. We investigate the predictive value of task and eye movement properties by creating a computational cognitive model of saccade selection based on instructed task and internal cognitive state using a Dynamic Bayesian Network (DBN). Understanding how humans generate saccades under different conditions and cognitive sets links recent work on salience models of low-level vision with higher level cognitive goals. This model provides a Bayesian, cognitive approach to topdown transitions in attentional set in pre-frontal areas along with vector-based saccade generation from the superior colliculus. Our approach is to begin with eye movement data that has previously been shown to differ across task. We first present an analysis of the extent to which individual saccadic features are diagnostic of an observer’s task. Second, we use those features to infer an underlying cognitive state that potentially differs from the instructed task. Finally, we demonstrate how changes of cognitive state over time can be incorporated into a generative model of eye movement vectors without resorting to an external decision homunculus. Internal cognitive state frees the model from the assumption that instructed task is the only factor influencing observers’ saccadic behavior. While the inclusion of hidden temporal state does not improve the classification accuracy of the model, it does allow accurate prediction of saccadic sequence results observed in search paradigms. Given the generative nature of this model, it is capable of saccadic simulation in real time. We demonstrated that the properties from its generated saccadic vectors closely match those of human observers given a particular task and cognitive state. Many current models of vision focus entirely on bottom-up salience to produce estimates of spatial Bareas of interestŵithin a visual scene. While a few recent models do add top-down knowledge and task information, we believe our contribution is important in three key ways. First, we incorporate task as learned attentional sets that are capable of self-transition given only information available to the visual system. This matches influential theories of bias signals by (Miller and Cohen Annu Rev Neurosci 24:167–202, 2001) and implements selection of state without simply shifting the decision to an external homunculus. Second, our model is generative and capable of predicting sequence artifacts in saccade generation like those found in visual search. Third, our model generates relative saccadic vector information as opposed to absolute spatial coordinates. This matches more closely the internal saccadic representations as they are generated in the superior colliculus.},
  file = {/Users/zcole/Box/Zoterox/storage/S9ES8B3Q/Joseph MacInnes et al. - 2018 - A Generative Model of Cognitive State from Task an.pdf},
  langid = {english},
  number = {5}
}

@article{millsExaminingInfluenceTask2011,
  title = {Examining the Influence of Task Set on Eye Movements and Fixations},
  author = {Mills, M. and Hollingworth, A. and Van der Stigchel, S. and Hoffman, L. and Dodd, M. D.},
  date = {2011-07-28},
  journaltitle = {Journal of Vision},
  volume = {11},
  pages = {17--17},
  issn = {1534-7362},
  doi = {10.1167/11.8.17},
  url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/11.8.17},
  urldate = {2019-01-16},
  abstract = {The purpose of the present study was to examine the influence of task set on the spatial and temporal characteristics of eye movements during scene perception. In previous work, when strong control was exerted over the viewing task via specification of a target object (as in visual search), task set biased spatial, rather than temporal, parameters of eye movements. Here, we find that more participant-directed tasks (in which the task establishes general goals of viewing rather than specific objects to fixate) affect not only spatial (e.g., saccade amplitude) but also temporal parameters (e.g., fixation duration). Further, task set influenced the rate of change in fixation duration over the course of viewing but not saccade amplitude, suggesting independent mechanisms for control of these parameters.},
  file = {/Users/zcole/Box/Zoterox/storage/LDM72NEB/Mills et al. - 2011 - Examining the influence of task set on eye movemen.pdf;/Users/zcole/Box/Zoterox/storage/NQI3QWUD/Mills et al. - 2011 - Examining the influence of task set on eye movemen.pdf},
  langid = {english},
  number = {8}
}

@article{porterEffortVisualSearch2007,
  title = {Effort during Visual Search and Counting: Insights from Pupillometry},
  shorttitle = {Effort during Visual Search and Counting},
  author = {Porter, Gillian and Troscianko, Tom and Gilchrist, Iain D.},
  date = {2007-02},
  journaltitle = {Quarterly Journal of Experimental Psychology (2006)},
  shortjournal = {Q J Exp Psychol (Hove)},
  volume = {60},
  pages = {211--229},
  issn = {1747-0218},
  doi = {10.1080/17470210600673818},
  abstract = {We investigated the processing effort during visual search and counting tasks using a pupil dilation measure. Search difficulty was manipulated by varying the number of distractors as well as the heterogeneity of the distractors. More difficult visual search resulted in more pupil dilation than did less difficult search. These results confirm a link between effort and increased pupil dilation. The pupil dilated more during the counting task than during target-absent search, even though the displays were identical, and the two tasks were matched for reaction time. The moment-to-moment dilation pattern during search suggests little effort in the early stages, but increasingly more effort towards response, whereas the counting task involved an increased initial effort, which was sustained throughout the trial. These patterns can be interpreted in terms of the differential memory load for item locations in each task. In an additional experiment, increasing the spatial memory requirements of the search evoked a corresponding increase in pupil dilation. These results support the view that search tasks involve some, but limited, memory for item locations, and the effort associated with this memory load increases during the trials. In contrast, counting involves a heavy locational memory component from the start.},
  eprint = {17455055},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Female,Humans,Male,Mathematics,Memory,Pupil,Reaction Time,Visual Perception},
  langid = {english},
  number = {2}
}

@book{R-base,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2018},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

@article{seeligerConvolutionalNeuralNetworkbased2018,
  title = {Convolutional Neural Network-Based Encoding and Decoding of Visual Object Recognition in Space and Time},
  author = {Seeliger, K. and Fritsche, M. and Güçlü, U. and Schoenmakers, S. and Schoffelen, J.-M. and Bosch, S.E. and van Gerven, M.A.J.},
  date = {2018-10},
  journaltitle = {NeuroImage},
  volume = {180},
  pages = {253--266},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2017.07.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811917305864},
  urldate = {2019-01-13},
  abstract = {Representations learned by deep convolutional neural networks (CNNs) for object recognition are a widely investigated model of the processing hierarchy in the human visual system. Using functional magnetic resonance imaging, CNN representations of visual stimuli have previously been shown to correspond to processing stages in the ventral and dorsal streams of the visual system. Whether this correspondence between models and brain signals also holds for activity acquired at high temporal resolution has been explored less exhaustively. Here, we addressed this question by combining CNN-based encoding models with magnetoencephalography (MEG). Human participants passively viewed 1,000 images of objects while MEG signals were acquired. We modelled their high temporal resolution source-reconstructed cortical activity with CNNs, and observed a feed-forward sweep across the visual hierarchy between 75 and 200 ms after stimulus onset. This spatiotemporal cascade was captured by the network layer representations, where the increasingly abstract stimulus representation in the hierarchical network model was reflected in different parts of the visual cortex, following the visual ventral stream. We further validated the accuracy of our encoding model by decoding stimulus identity in a left-out validation set of viewed objects, achieving state-of-the-art decoding accuracy.},
  file = {/Users/zcole/Box/Zoterox/storage/S839KJAP/Seeliger et al. - 2018 - Convolutional neural network-based encoding and de.pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{tatlerYarbusEyeMovements2010,
  title = {Yarbus, {{Eye Movements}}, and {{Vision}}},
  author = {Tatler, Benjamin W and Wade, Nicholas J and Kwan, Hoi and Findlay, John M and Velichkovsky, Boris M},
  date = {2010-04-01},
  journaltitle = {i-Perception},
  volume = {1},
  pages = {7--27},
  issn = {2041-6695},
  doi = {10.1068/i0382},
  url = {https://doi.org/10.1068/i0382},
  urldate = {2020-02-07},
  abstract = {The impact of Yarbus's research on eye movements was enormous following the translation of his book Eye Movements and Vision into English in 1967. In stark contrast, the published material in English concerning his life is scant. We provide a brief biography of Yarbus and assess his impact on contemporary approaches to research on eye movements. While early interest in his work focused on his study of stabilised retinal images, more recently this has been replaced with interest in his work on the cognitive influences on scanning patterns. We extended his experiment on the effect of instructions on viewing a picture using a portrait of Yarbus rather than a painting. The results obtained broadly supported those found by Yarbus.},
  file = {/Users/zcole/Box/Zoterox/storage/RPPTWSXE/Tatler et al. - 2010 - Yarbus, Eye Movements, and Vision.pdf},
  number = {1}
}

@article{wangArousalEffectsPupil2018,
  title = {Arousal {{Effects}} on {{Pupil Size}}, {{Heart Rate}}, and {{Skin Conductance}} in an {{Emotional Face Task}}},
  author = {Wang, Chin-An and Baird, Talia and Huang, Jeff and Coutinho, Jonathan D. and Brien, Donald C. and Munoz, Douglas P.},
  date = {2018},
  journaltitle = {Frontiers in Neurology},
  shortjournal = {Front. Neurol.},
  volume = {9},
  publisher = {{Frontiers}},
  issn = {1664-2295},
  doi = {10.3389/fneur.2018.01029},
  url = {https://www.frontiersin.org/articles/10.3389/fneur.2018.01029/full},
  urldate = {2020-05-25},
  abstract = {Arousal level changes constantly and it has a profound influence on performance during everyday activities. Fluctuations in arousal are regulated by the autonomic nervous system, which is mainly controlled by the balanced activity of the parasympathetic and sympathetic systems, commonly indexed by heart rate (HR) and galvanic skin response (GSR), respectively. Although a growing number of studies have used pupil size to indicate the level of arousal, research that directly examines the relationship between pupil size and HR or GSR is limited. The goal of this study was to understand how pupil size is modulated by autonomic arousal. Human participants fixated various emotional face stimuli, of which low-level visual properties were carefully controlled, while their pupil size, HR, GSR, and eye position were recorded simultaneously. We hypothesized that a positive correlation between pupil size and HR or GSR would be observed both before and after face presentation. Trial-by-trial positive correlations between pupil diameter and HR and GSR were found before face presentation, with larger pupil diameter observed on trials with higher HR or GSR. However, task-evoked pupil responses after face presentation only correlated with HR. Overall, these results demonstrated a trial-by-trial relationship between pupil size and HR or GSR, suggesting that pupil size can be used as an index for arousal level involuntarily regulated by the autonomic nervous system.},
  file = {/Users/zcole/Box/Zoterox/storage/TT2PBWRQ/Wang et al. - 2018 - Arousal Effects on Pupil Size, Heart Rate, and Ski.pdf},
  keywords = {locus coeruleus-norepinephrine system,Parasympathetic and sympathetic nervous system,pupil dilation,Pupillometry,trial-by-trial},
  langid = {english}
}

@online{yarbusEyeMovementsVision1967,
  title = {Eye {{Movements}} and {{Vision}}},
  author = {Yarbus, Alfred},
  date = {1967},
  url = {http://wexler.free.fr/library/files/yarbus%20(1967)%20eye%20movements%20and%20vision.pdf},
  urldate = {2019-01-24},
  file = {/Users/zcole/Box/Zoterox/storage/A6DIS7FM/yarbus (1967) eye movements and vision.pdf}
}

@incollection{zhouComparingInterpretabilityDeep2019,
  title = {Comparing the {{Interpretability}} of {{Deep Networks}} via {{Network Dissection}}},
  booktitle = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
  editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
  date = {2019},
  pages = {243--252},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-28954-6_12},
  url = {https://doi.org/10.1007/978-3-030-28954-6_12},
  urldate = {2020-02-25},
  abstract = {In this chapter, we introduce Network Dissection (The complete paper and code are available at http://netdissect.csail.mit.edu), a general framework to quantify the interpretability of the units inside a deep convolutional neural networks (CNNs). We compare the different vocabularies of interpretable units as concept detectors emerged from the networks trained to solve different supervised learning tasks such as object recognition on ImageNet and scene classification on Places. The network dissection is further applied to analyze how the units acting as semantic detectors grow and evolve over the training iterations both in the scenario of the train-from-scratch and in the stage of the fine-tuning between data sources. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure.},
  isbn = {978-3-030-28954-6},
  keywords = {Deep neural networks,Interpretable machine learning,Model visualization},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{zouLearningModelTaskOriented2016,
  title = {Learning to {{Model Task}}-{{Oriented Attention}}},
  author = {Zou, Xiaochun and Zhao, Xinbo and Wang, Jian and Yang, Yongjia},
  date = {2016},
  journaltitle = {Computational Intelligence and Neuroscience},
  volume = {2016},
  pages = {1--12},
  issn = {1687-5265, 1687-5273},
  doi = {10.1155/2016/2381451},
  url = {http://www.hindawi.com/journals/cin/2016/2381451/},
  urldate = {2019-01-13},
  file = {/Users/zcole/Box/Zoterox/storage/WE5SDIIT/Zou et al. - 2016 - Learning to Model Task-Oriented Attention.pdf},
  langid = {english}
}


