<!DOCTYPE html>
<!--[if IE 9]><html class="ie9" lang="en"><![endif]-->
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="citation_pii" content="S0042698914002004">
<meta name="citation_issn" content="0042-6989">
<meta name="citation_volume" content="103">
<meta name="citation_lastpage" content="142">
<meta name="citation_publisher" content="Pergamon">
<meta name="citation_firstpage" content="127">
<meta name="citation_fulltext_world_readable" content="">
<meta name="citation_journal_title" content="Vision Research">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.visres.2014.08.014">
<meta name="dc.identifier" content="10.1016/j.visres.2014.08.014">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="In this paper we develop a probabilistic method to infer the visual-task of a viewer given measured eye movement trajectories. This method is based on…">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914X0009X-cov150h.gif">
<meta name="citation_title" content="An inverse Yarbus process: Predicting observers’ task from eye movement patterns">
<meta property="og:title" content="An inverse Yarbus process: Predicting observers’ task from eye movement patterns">
<meta name="citation_publication_date" content="2014/10/01">
<meta name="citation_online_date" content="2014/08/28">
<meta name="citation_pdf_url" content="https://www.sciencedirect.com/science/article/pii/S0042698914002004/pdfft?md5=559279836fc0333adb4ec89bc86fcda4&amp;pid=1-s2.0-S0042698914002004-main.pdf">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR">
<title>An inverse Yarbus process: Predicting observers’ task from eye movement patterns - ScienceDirect</title>
<link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S0042698914002004">
<meta property="og:type" content="article">
<meta name="viewport" content="initial-scale=1">
<meta name="SDTech" content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
<link rel="shortcut icon" href="https://sdfestaticassets-us-east-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico" type="image/x-icon">
<link rel="icon" href="https://sdfestaticassets-us-east-1.sciencedirectassets.com/shared-assets/13/images/favSD.ico" type="image/x-icon">
<link rel="stylesheet" href="arp.css">
<link rel="dns-prefetch" href="https://w.usabilla.com/">
<link rel="dns-prefetch" href="https://www.deepdyve.com/">
<link rel="dns-prefetch" href="https://smetrics.elsevier.com/">
<iframe src="javascript:false" title="" style="width: 0px; height: 0px; border: 0px none; display: none;"></iframe><script type="text/javascript">
        
        window.pageTargeting = {"region":"us-east-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Vision Research","auth":"AE"};
        window.pageData = {"content":[{"entitlementType":"subscription","format":"MIME-XHTML","id":"sd:article:pii:S0042698914002004","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"0042-6989","volumeNumber":"103","suppl":"C"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1571545778986,"loadTime":""},"visitor":{"accessType":"ae:ANON_IP","accountId":"ae:20840","accountName":"ae:University of Nebraska-Lincoln Love Library","loginStatus":"anonymous","userId":"ae:437158","ipAddress":"129.93.224.12","appSessionId":"3c9344b9-9503-4b17-ad26-5f99bb94b5d7"}};
        window.arp = {
          config: {"reduxLogging":false,"assetsBaseUrl":"https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/72b3c75e9c9c2dbf390ead8825e79d3a0cba3150","cdnAssetsHost":"https://sdfestaticassets-us-east-1.sciencedirectassets.com","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","bosUrl":"https://feedback.recs.d.elsevier.com/raw/events","bosTimeOut":60000,"strictMode":false,"enableGoogleScholarLinks":true,"enableEzProxyForEnhancedReader":false,"enableGlobalHeader":true,"sendToEnhancedReader":true},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
      </script>

<script type="text/javascript">
        (function(){
          if(window.BOOMR && window.BOOMR.version){return;}
          var dom,doc,where,iframe = document.createElement('iframe'),win = window;

          function boomerangSaveLoadTime(e) {
            win.BOOMR_onload=(e && e.timeStamp) || new Date().getTime();
          }
          if (win.addEventListener) {
            win.addEventListener("load", boomerangSaveLoadTime, false);
          } else if (win.attachEvent) {
            win.attachEvent("onload", boomerangSaveLoadTime);
          }

          iframe.src = "javascript:false";
          iframe.title = ""; iframe.role="presentation";
          (iframe.frameElement || iframe).style.cssText = "width:0;height:0;border:0;display:none;";
          where = document.getElementsByTagName('script')[0];
          where.parentNode.insertBefore(iframe, where);

          try {
            doc = iframe.contentWindow.document;
          } catch(e) {
            dom = document.domain;
            iframe.src="javascript:var d=document.open();d.domain='"+dom+"';void(0);";
            doc = iframe.contentWindow.document;
          }
          doc.open()._l = function() {
            var js = this.createElement("script");
            if(dom) this.domain = dom;
            js.id = "boomr-if-as";
            js.src = 'https://c.go-mpulse.net/boomerang/2FBN2-NKMGU-EJKY8-ZANKZ-SUJZF';
            BOOMR_lstart=new Date().getTime();
            this.body.appendChild(js);
          };
          doc.write('<body onload="document._l();">');
          doc.close();
        })();
      </script>

<script src="satellite-5b3b560664746d57b70018c6.js"></script><script src="https://assets.adobedtm.com/376c5346e33126fdb6b2dbac81e307cbacfd7935/s-code-contents-9c0358adbc3b5986e210099b3bf1d427fc5bd286.js"></script><link rel="preload" href="https://adservice.google.com/adsid/integrator.js?domain=www.sciencedirect.com"><script type="text/javascript" src="https://adservice.google.com/adsid/integrator.js?domain=www.sciencedirect.com"></script><script src="https://securepubads.g.doubleclick.net/gpt/pubads_impl_2019101401.js" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style></head>
<body><div style="display: none;" id="lightningjs-usabilla_live"><div><iframe id="lightningjs-frame-usabilla_live" frameborder="0"></iframe></div></div>
<a class="sr-only sr-only-focusable" href="#screen-reader-main-content">Skip to main content</a>
<a class="sr-only sr-only-focusable" href="#screen-reader-main-title">Skip to article</a>
<!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://service.elsevier.com/app/answers/detail/a_id/9831">this support page</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
<div data-iso-key="_0"><div class="App" id="app" data-reactroot=""><div class="page"><section><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-lg"><a id="gh-branding" class="u-flex-center-ver" href="https://www.sciencedirect.com/" aria-label="Science Direct home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"><svg xmlns="http://www.w3.org/2000/svg" role="img" version="1.1" height="30" width="138" viewBox="0 0 138 30" class="gh-wordmark u-margin-s-left" fill="#f36d21" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path class="a" d="M4.23,21a9.79,9.79,0,0,1-4.06-.83l.29-2.08a7.17,7.17,0,0,0,3.72,1.09c2.13,0,3-1.22,3-2.39C7.22,13.85.3,13.43.3,9c0-2.37,1.56-4.29,5.2-4.29a9.12,9.12,0,0,1,3.77.75l-.1,2.08a7.58,7.58,0,0,0-3.67-1c-2.24,0-2.91,1.22-2.91,2.39,0,3,6.92,3.61,6.92,7.8C9.5,19.1,7.58,21,4.23,21Z"></path><path class="a" d="M20.66,20A6.83,6.83,0,0,1,16.76,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H18.81c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z"></path><path class="a" d="M23.75,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,23.75,6.9ZM22.76,9h2V20.74h-2Z"></path><path class="a" d="M29.55,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,32.77,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM35.63,13c-.08-2.29-1.09-2.7-3-2.7A3.78,3.78,0,0,0,31,10.7,3.7,3.7,0,0,0,29.76,13Z"></path><path class="a" d="M49.7,20.74h-2s.1-2.73.08-5.1c0,0,0-1.56,0-2.5-.05-1.79-.21-2.7-2-2.7a4.87,4.87,0,0,0-1.64.31,12.11,12.11,0,0,0-1.95,2.08v7.9h-2v-8.5a19.47,19.47,0,0,0-.1-2.34L39.95,9h1.85l.31,1.74a4.71,4.71,0,0,1,3.82-2.05c2.11,0,3.54.68,3.74,3.09.1,1.17.08,2.34.08,3.51C49.75,17.2,49.7,20.74,49.7,20.74Z"></path><path class="a" d="M61.5,20A6.83,6.83,0,0,1,57.6,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H59.66c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z"></path><path class="a" d="M64.75,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,68,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM70.84,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,65,13Z"></path><path class="a" d="M81.21,20.74H75.83V5h5.62c5.54,0,7.46,4.21,7.46,7.8C88.91,16.26,86.93,20.74,81.21,20.74Zm-.1-14H77.88V19.07h3c4,0,5.75-2.31,5.75-6.24C86.59,10.15,85.34,6.7,81.11,6.7Z"></path><path class="a" d="M92.86,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,92.86,6.9ZM91.87,9h2V20.74h-2Z"></path><path class="a" d="M104.48,10.83l-1.64.47c0-.18-.08-1-.83-1-1.14,0-2.08,1.9-2.5,2.91v7.49h-2V12.18a18.78,18.78,0,0,0-.1-2.29L97.3,9h1.85l.34,1.87a3.22,3.22,0,0,1,2.68-2.16,2,2,0,0,1,2.26,1.72c0,.18.05.29.05.31Z"></path><path class="a" d="M107.44,14.6V15c0,2.81,1.38,4.34,3.85,4.34A6.37,6.37,0,0,0,115,18.11l.16,1.82A7.94,7.94,0,0,1,110.67,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM113.53,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,107.65,13Z"></path><path class="a" d="M126.24,20a6.83,6.83,0,0,1-3.9,1.09c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H124.4c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z"></path><path class="a" d="M134.51,20.45a7.36,7.36,0,0,1-2.7.62c-1.53,0-2.63-.86-2.63-2.94V10.52H127V9h2.13V5.81h2V9h3.09v1.56h-3.09v7c0,1.33.34,1.85,1.25,1.85a5.66,5.66,0,0,0,2-.55Z"></path></g></svg></a><div class="gh-nav-cnt"><div class="gh-nav-links-container gh-nav-links-container-h"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor button-link-primary gh-nav-action" href="https://www.sciencedirect.com/browse/journals-and-books" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text">Journals &amp; Books</span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-search-toggle"><a class="anchor button-link-primary gh-nav-action gh-icon-btn" href="https://www.sciencedirect.com/search/advanced" data-aa-button="search-in-header-opened-from-article" aria-label="Opens ScienceDirect Search"><span class="anchor-text"></span><svg focusable="false" viewBox="0 0 100 128" aria-hidden="true" alt="Search" width="18.75" height="24" class="icon icon-search gh-icon"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></a></li><li class="gh-nav-item gh-move-to-spine gh-help-button"><a class="anchor button-link-primary gh-nav-action gh-icon-btn" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" target="_blank" rel="noopener noreferrer" aria-label="Science Direct help page opens in new window"><span class="anchor-text"></span><svg focusable="false" viewBox="0 0 114 128" aria-hidden="true" alt="ScienceDirect help page" width="21.375" height="24" class="icon icon-help gh-icon"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg></a></li></ul></nav></div></div><div class="gh-profile-container gh-move-to-spine"><a class="link-button u-margin-m-left link-button-secondary link-button-small" role="button" href="https://www.sciencedirect.com/user/register?targetURL=%2Fscience%2Farticle%2Fpii%2FS0042698914002004" id="gh-cta-btn" data-aa-region="header" data-aa-name="Register"><span class="link-button-text">Create account</span></a><a class="link-button u-margin-m-left link-button-primary link-button-small" role="button" href="https://www.sciencedirect.com/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0042698914002004" id="gh-signin-btn" data-aa-region="header" data-aa-name="Sign in"><span class="link-button-text">Sign in</span></a></div><div class="gh-lib-banner gh-lb-legacy"><div class="gh-lb-info u-flex-center-ver"><div class="gh-lb-message"><span class="u-clr-grey5 u-margin-xs-right">Brought to you by:</span><a class="anchor" href="http://iris.unl.edu/"><span class="anchor-text">University of Nebraska-Lincoln</span></a></div></div></div><div id="gh-mobile-menu" class="mobile-menu"><div class="gh-hamburger u-fill-grey7 u-margin-m-left"><button class="button-link u-flex-center-ver button-link-primary" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="18" width="40"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg><span class="button-link-text"> </span></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"><header id="gh-drawer-header" class="u-padding-s text-s"><div class="gh-dh-opt"><div><p class="u-margin-s-bottom"></p><div class="gh-dh-actions"><a class="anchor" href="https://www.sciencedirect.com/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0042698914002004" data-aa-region="header" data-aa-name="Sign in"><span class="anchor-text">Sign in</span></a><a class="anchor u-margin-m-left" href="https://www.sciencedirect.com/user/register?targetURL=%2Fscience%2Farticle%2Fpii%2FS0042698914002004" data-aa-region="header" data-aa-name="Register"><span class="anchor-text">Create account</span></a></div></div></div></header><div class="gh-nav-cnt"><div class="gh-nav-links-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-v"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item"><a class="anchor button-link-primary gh-nav-action" href="https://www.sciencedirect.com/browse/journals-and-books" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text">Journals &amp; Books</span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-v"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item"><a class="anchor button-link-primary gh-nav-action" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" target="_blank" rel="noopener noreferrer"><span class="anchor-text">Help</span></a></li></ul></nav></div></div><div id="gh-mob-inst-cnt"><div class="gh-inst-cnt"><p class="gh-inst-lbl u-margin-s-bottom text-m">Brought to you by</p><div class="gh-lib-banner gh-lb-dropdown text-s"><div class="gh-lb-info u-flex-center-ver"><div class="gh-il-placeholder u-flex-center u-margin-s-right"><svg focusable="false" viewBox="0 0 106 128" width="19" height="24" class="icon icon-institution u-fill-white"><path d="m84 98h1e1v1e1h-82v-1e1h1e1v-46h14v46h1e1v-46h14v46h1e1v-46h14v46zm-72-61.14l41-20.84 41 20.84v5.14h-82v-5.14zm92 15.14v-21.26l-51-25.94-51 25.94v21.26h1e1v36h-1e1v3e1h102v-3e1h-1e1v-36h1e1z"></path></svg></div><div class="gh-lb-message"><a class="anchor" href="http://iris.unl.edu/"><span class="anchor-text">University of Nebraska-Lincoln</span></a></div></div></div></div></div></div></div></div></header><div class="Article" id="mathjax-container"><div class="sticky-outer-wrapper"><div class="sticky-inner-wrapper" style="position: relative; z-index: 1; transform: translate3d(0px, 0px, 0px);"><div class="Toolbar" role="region" aria-label="download options and search" id="screen-reader-main-content"><div class="toolbar-container"><div class="u-show-from-lg col-lg-6">&nbsp;</div><div class="buttons text-s pull-left pad-left"><button class="button show-toc-button button-anchor u-hide-from-lg u-margin-s-right" type="button"><svg focusable="false" viewBox="0 0 104 128" width="19.5" height="24" class="icon icon-list"><path d="m2e1 95a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm14 55h68v1e1h-68zm0-3e1h68v1e1h-68zm0-3e1h68v1e1h-68z"></path></svg><span class="button-text"><span>Outline</span></span></button><div class="PdfDownloadButton"><div class="popover download-pdf-popover" id="download-pdf-popover"><div id="popover-trigger-download-pdf-popover"><button id="pdfLink" class="button button-anchor u-padding-0-left" role="button" aria-expanded="false" aria-haspopup="true" aria-label="Download PDF options" type="button"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor pdf-icon"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="button-text"><span class="pdf-download-label u-show-inline-from-lg">Download PDF</span><span class="pdf-download-label-short u-hide-from-lg">Download</span></span></button></div></div></div><div class="Social" id="social"><div class="popover social-popover" id="social-popover" aria-label="Share article on social media"><div id="popover-trigger-social-popover"><button class="button button-anchor" role="button" aria-expanded="false" aria-haspopup="true" type="button"><span class="button-text">Share</span></button></div></div></div><div class="ExportCitation" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover" aria-label="Export or save citation"><div id="popover-trigger-export-citation-popover"><button class="button button-anchor" role="button" aria-expanded="false" aria-haspopup="true" type="button"><span class="button-text">Export</span></button></div></div></div></div><div class="quick-search-container pull-right pad-right u-show-from-md"><form id="quick-search" class="QuickSearch u-margin-xs-right" action="/search/advanced#submit" method="get"><input class="query" aria-label="Search ScienceDirect" name="qs" placeholder="Search ScienceDirect" type="search"><button class="button button-primary" type="submit" aria-label="Submit search"><span class="button-text"><svg focusable="false" viewBox="0 0 100 128" height="20" width="18.75" class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></span></button><a class="advanced-search-link" href="https://www.sciencedirect.com/search/advanced#submit">Advanced</a><input name="origin" value="article" type="hidden"><input name="zone" value="qSearch" type="hidden"></form></div></div></div></div></div><div class="article-wrapper u-padding-m-top grid row"><div class="u-show-from-lg col-lg-6"><div class="TableOfContents u-margin-l-bottom" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li><a href="#ab005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Highlights">Highlights</a></li><li><a href="#ab010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Abstract">Abstract</a></li><li><a href="#kg005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Keywords">Keywords</a></li><li><a href="#s0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction">1. Introduction</a></li><li><a href="#s0025" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. An Inverse Yarbus process via Bayesian inference">2. An Inverse Yarbus process via Bayesian inference</a></li><li><a href="#s0030" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3">3. State positioning of HMMs using <em>K</em>-means clustering</a></li><li><a href="#s0035" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Experiment">4. Experiment</a></li><li><a href="#s0050" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. Conclusion">5. Conclusion</a></li><li><a href="#s0055" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="6. Ethical considerations">6. Ethical considerations</a></li><li><a href="#ak005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Acknowledgment">Acknowledgment</a></li><li><a href="#s0070" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Appendix A. Supplementary material">Appendix A. Supplementary material</a></li><li><a href="#bi005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="References">References</a></li></ol><button class="button button-anchor" aria-expanded="false" data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type="button"><span class="button-text">Show full outline</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (9)</h2><ol><li><a href="#f0005" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 1. Eye trajectories measured by Yarbus by viewers carrying out different tasks" src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr1.sml" style="max-width: 219px; max-height: 89px;"></div></a></li><li><a href="#f0010" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 2. (a) General architecture of bottom-up attention model by Itti and Koch (2001)" src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr2.sml" style="max-width: 219px; max-height: 73px;"></div></a></li><li><a href="#f0015" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 3. (a) A first-order, finite-state, discrete-time Markov chain (DTMC) with two…" src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr3.sml" style="max-width: 219px; max-height: 76px;"></div></a></li><li><a href="#f0020" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 4. (a) A HMM with two states (i" src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr4.sml" style="max-width: 219px; max-height: 77px;"></div></a></li><li><a href="#f0025" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 5. (a) Eye trajectories recorded while executing the task of “counting the number…" src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr5.sml" style="max-width: 219px; max-height: 117px;"></div></a></li><li><a href="#f0030" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 6. (a) Original Image" src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr6.sml" style="max-width: 219px; max-height: 58px;"></div></a></li></ol><button class="button button-anchor" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type="button"><span class="button-text">Show all figures</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Tables" id="toc-tables"><h2 class="u-h4">Tables (2)</h2><ol class="u-padding-s-bottom"><li><a href="#t0005" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Numerical values of the confusion matrix for task classification using the HMM-based model. To obtain the results we set σ=4.5o and did LOO cross validation over all task dependent eye trajectories."><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 1</a></li><li><a href="#t0010" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Numerical values of the confusion matrix for task classification using the DTMC-based model. To obtain the results we used the same setup (number of clusters) as in the HMMs and did LOO cross validati..."><svg focusable="false" viewBox="0 0 98 128" width="18.375" height="24" class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 2</a></li></ol><div class="PageDivider"></div></div><div class="Extras" id="toc-extras"><h2 class="u-h4">Extras (1)</h2><ol class="u-padding-s-bottom"><li><a href="#m0005" data-aa-button="sd:product:journal:article:type=anchor:region=outline:name=sdf" title="Document"><svg focusable="false" viewBox="0 0 95 128" width="17.8125" height="24" class="icon icon-pdf-download"><path d="m82 108h-7e1v-49c0-6.08 4.92-11 11-11h17v-2e1h-6c-2.2 0-4 1.8-4 4v6h-7c-3.32 0-6.44 0.78-9.22 2.16 2.46-5.62 7.28-11.86 13.5-17.1 2.34-1.98 5.3-3.06 8.32-3.06h46.4v4e1h1e1v-5e1h-56.4c-5.38 0-10.62 1.92-14.76 5.4-9.1 7.68-18.84 20.14-18.84 32.1v70.5h9e1v-18h-1e1v8zm-25.94-39.4c-0.84-0.38-1.84-0.56-2.98-0.56h-9.04v23.6h5.94v-9h2.18c2.48 0 4.36-0.62 5.66-1.84s1.92-3.06 1.92-5.5c0-1.04-0.12-2-0.4-2.88-0.26-0.88-0.66-1.64-1.22-2.3s-1.22-1.14-2.06-1.52zm-3.12 8.94c-0.4 0.46-0.98 0.7-1.74 0.7h-1.22v-5.76h1.22c1.56 0 2.34 0.96 2.34 2.88 0 0.98-0.2 1.72-0.6 2.18zm21.84-8.52c-0.96-0.66-2.32-0.98-4.06-0.98h-8.72v23.6h8.72c1.74 0 3.1-0.32 4.06-0.98s1.7-1.52 2.18-2.62 0.78-2.34 0.88-3.76 0.16-2.9 0.16-4.44-0.06-3.02-0.16-4.44-0.4-2.68-0.88-3.76c-0.48-1.1-1.2-1.96-2.18-2.62zm-2.78 14.66c-0.06 0.96-0.18 1.72-0.38 2.24s-0.48 0.88-0.84 1.04-0.86 0.24-1.48 0.24h-1.32v-14.74h1.32c0.62 0 1.12 0.08 1.48 0.24s0.64 0.52 0.84 1.04 0.32 1.28 0.38 2.24c0.06 0.98 0.08 2.24 0.08 3.84s-0.02 2.9-0.08 3.86zm13.98-6.58v-4.02h7.74v-5.04h-13.7v23.6h5.96v-9.72h7.26v-4.82z"></path></svg>Document</a></li></ol><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" role="main" lang="en"><noscript>JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page.</noscript><div class="Publication" id="publication"><div class="publication-brand u-show-from-sm"><a href="https://www.sciencedirect.com/science/journal/00426989"><img class="publication-brand-image" src="elsevier-non-solus.png" alt="Elsevier"></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="publication-title-link" title="Go to Vision Research on ScienceDirect" href="https://www.sciencedirect.com/science/journal/00426989">Vision Research</a></h2><div class="text-xs"><a title="Go to table of contents for this volume/issue" href="https://www.sciencedirect.com/science/journal/00426989/103/supp/C">Volume 103</a>, October 2014, Pages 127-142</div></div><div class="publication-cover u-show-from-sm"><a href="https://www.sciencedirect.com/science/journal/00426989/103/supp/C"><img class="publication-cover-image" src="1-s2.gif" alt="Vision Research"></a></div></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">An inverse Yarbus process: Predicting observers’ task from eye movement patterns</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups text-xs"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><a class="author size-m workspace-trigger" name="bau005" href="#%21"><span class="content"><span class="text given-name">Amin</span><span class="text surname">Haji-Abolhassani</span><svg focusable="false" viewBox="0 0 106 128" width="19.875" height="24" class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable="false" viewBox="0 0 102 128" width="19.125" height="24" class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name="bau010" href="#%21"><span class="content"><span class="text given-name">James J.</span><span class="text surname">Clark</span><svg focusable="false" viewBox="0 0 102 128" width="19.125" height="24" class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a></div></div></div><button class="show-hide-details u-font-sans" type="button" aria-expanded="false"><svg viewBox="0 0 9 9" class="icon-expand"><path d="M5 7H4V5H2V4h2V2h1v2h2v1H5z"></path><path d="M0 0v9h9V0zm1 1h7v7H1z"></path></svg>Show more</button></div><div class="DoiLink" id="doi-link"><a class="doi" href="https://doi.org/10.1016/j.visres.2014.08.014" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier">https://doi.org/10.1016/j.visres.2014.08.014</a><a class="rights-and-content" target="_blank" rel="noreferrer noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0042698914002004&amp;orderBeanReset=true">Get rights and content</a></div><div class="LicenseInfo"><div class="License"><span>Under an Elsevier </span><a target="_blank" rel="noreferrer noopener" href="http://www.elsevier.com/open-access/userlicense/1.0/">user license</a></div><div class="OpenAccessLabel">open archive</div></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author-highlights" id="ab005" lang="en"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Highlights</h2><div id="as005"><p id="sp005"><dl class="list"><dt class="list-label">•</dt><dd class="list-description"><p id="p0475">A computational model is developed for visual-task inference given eye trajectories.</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0480">Hidden Markov Models are used to predict fixation coordinates contingent on task.</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0485">The model allows for both overt and covert shifts of attention.</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0490">The model locates attended targets and identifies the task.</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0495">The results support the findings of Yarbus on task-dependent eye movements.</p></dd></dl></p></div></div><div class="abstract author" id="ab010" lang="en"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="as010"><p id="sp010">In
 this paper we develop a probabilistic method to infer the visual-task 
of a viewer given measured eye movement trajectories. This method is 
based on the theory of hidden Markov models (HMM) that employs a first 
order Markov process to predict the coordinates of fixations given the 
task. The prediction confidence level of each task-dependent model is 
used in a Bayesian inference formulation, whereby the task with the 
maximum a posteriori (MAP) probability is selected. We applied this 
technique to a challenging dataset consisting of eye movement 
trajectories obtained from subjects viewing monochrome images of real 
scenes tasked with answering questions regarding the scenes. The results
 show that the HMM approach, combined with a clustering technique, can 
be a reliable way to infer visual-task from eye movements data.</p></div></div></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary" href="https://www.sciencedirect.com/science/article/pii/S0042698914001916"><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-left"><path d="m1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text"><strong>Previous </strong><span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><button class="button-alternative button-alternative-tertiary" disabled="disabled" type="button"><span class="button-alternative-text"><strong>Next </strong><span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span><svg focusable="false" viewBox="0 0 54 128" width="32" height="32" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></button></li></ul><div class="Keywords u-font-serif"><div id="kg005" class="keywords-section"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="k0005" class="keyword"><span>Visual-task inference</span></div><div id="k0010" class="keyword"><span>Attention cognitive model</span></div><div id="k0015" class="keyword"><span><em>K</em>-means clustering</span></div><div id="k0020" class="keyword"><span>Visual search</span></div><div id="k0025" class="keyword"><span>Eye movement</span></div><div id="k0030" class="keyword"><span>Hidden Markov model</span></div></div></div><div class="Body u-font-serif" id="body"><div><section id="s0005"><h2 id="st005" class="u-h3 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><div><p id="p0005">It is well known that low-level visual features, such as color and intensity contrasts, influence eye movements <a name="bb0155" href="#b0155" class="workspace-trigger">Findlay (1981)</a>, <a name="bb0560" href="#b0560" class="workspace-trigger">Zelinsky et al. (1997)</a>.
 However, it is also observed that the task being performed by the 
viewer can also influence the pattern of eye movements. For example, 
someone that is viewing a web page on a computer monitor could be 
engaged in, among others, the tasks of reading text, searching for a 
specific object, counting objects, or recognizing faces. Each of these 
tasks would produce a different pattern of eye movements.The influence 
of task on eye movements was vividly demonstrated in the celebrated 
study of <a name="bb0555" href="#b0555" class="workspace-trigger">Yarbus (1967)</a>
 who recorded the eye movements of a subject while viewing a painting. 
The subject was asked different questions regarding the painting, such 
as to determine the wealth of the family depicted in the painting’. As 
shown in <a name="bf0005" href="#f0005" class="workspace-trigger">Fig. 1</a>, different trajectories emerged depending on the specific question that the viewer was answering.</p><figure class="figure text-xs" id="f0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr1.jpg" alt="" aria-describedby="cn005" height="287"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (292KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (292KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn005"><p id="sp015"><span class="label">Fig. 1</span>. 
Eye trajectories measured by Yarbus by viewers carrying out different 
tasks. (a) No specific task. (b) Estimate the wealth of the family. (c) 
Give the ages of the people in the painting. (d) Summarize what the 
family had been doing before the arrival of the “unexpected visitor”. 
(e) Remember the clothes worn by the people. (f) Remember the position 
of the people and objects in the room. (g) Estimate how long the 
“unexpected visitor” had been away from the family. Image adapted from <a name="bb0555" href="#b0555" class="workspace-trigger">Yarbus (1967)</a> with permission from Springer Publishing Company.</p></span></span></figure></div><p id="p0010">Several
 other studies have also reproduced the original finding of Yarbus using
 new equipment and stimuli, and with larger numbers of subjects. For 
instance, in <a name="bb0500" href="#b0500" class="workspace-trigger">Tatler et al. (2010)</a> the results obtained by Yarbus were confirmed in an experiment that studied the effect of <em>instructions</em>
 in viewing a portrait of Yarbus. While the effect of visual-task on eye
 movement pattern has been thoroughly investigated, there has been 
little done for the inverse process – to infer the visual-task from the 
eye movements. Knowledge of the visual-task being carried out by a 
viewer has many potential uses. For example, one can envisage an 
‘intelligent display’ which modifies what is being displayed in a way 
which facilitates the task. An intelligent web page could detect if a 
viewer is reading text and highlight or magnify the text, or if it 
detected the viewer was engaged in a counting or search behavior, it 
could highlight the target object. The goal of the work described in 
this paper is to develop such an <em>inverse Yarbus process</em>, whereby the visual-task is inferred given measurements of the eye movements of the viewer.</p><p id="p0015">There is some doubt as to whether development of such an inverse Yarbus process is possible at all. In a study by <a name="bb0190" href="#b0190" class="workspace-trigger">Greene et al., 2012</a>, <a name="bb0185" href="#b0185" class="workspace-trigger">Greene et al., 2011</a>
 two attempts were made to produce the inverse Yarbus problem. The first
 approach attempted to train humans to solve the inverse Yarbus problem,
 while the second tried to train a machine learning system to solve the 
problem. To obtain data for training and testing they recorded eye 
movements of several subjects, each performing a specific visual task on
 an image, and extracted a feature vector from the eye movement records.
 The feature vector used was a set of seven summary statistics of eye 
movements, which are often used in scanpath analysis (<a name="bb0075" href="#b0075" class="workspace-trigger">Castelhano &amp; Henderson, 2008</a>; <a name="bb0360" href="#b0360" class="workspace-trigger">Mika et al., 1999</a>).
 This feature vector included, among others, the number of fixations, 
the mean fixation duration, the mean saccade amplitude and the portion 
of the image covered by fixations. The machine learning approaches used 
three different classifiers based on linear discriminant analysis (<a name="bb0355" href="#b0355" class="workspace-trigger">Mika et al., 1999</a>), correlational methods <a name="bb0215" href="#b0215" class="workspace-trigger">Haxby et al. (2001)</a> and support vector machines (<a name="bb0235" href="#b0235" class="workspace-trigger">Hearst et al., 1998</a>).
 The results showed that both humans and the machine classifiers can 
only infer the task at a chance level. Based on these results (<a name="bb0185" href="#b0185" class="workspace-trigger">Greene, Liu, and Wolfe (2011)</a>) concluded that: <em>“The
 famous Yarbus figure may be compelling but, sadly, its message appears 
to be misleading. Neither humans nor machines can use scanpaths to 
identify the task of the viewer.”</em>. A similar result was obtained in <a name="bb0280" href="#b0280" class="workspace-trigger">Kanan et al. (2014)</a>, where a radial-basis kernel function support vector machine (C-SVN) (<a name="bb0195" href="#b0195" class="workspace-trigger">Gunn, 1998</a>) was used to classify the eye trajectories represented by their summary statistics. In their results (<a name="bb0280" href="#b0280" class="workspace-trigger">Kanan et al., 2014</a>) could only achieve an accuracy of 26.3% (95% CI&nbsp;=&nbsp;21.4–31.1%, <em>p</em>&nbsp;=&nbsp;0.61) which is not significantly better than the chance level.</p><p id="p0020">Summary statistics of eye movements are not sufficient to identify the visual task that was performed by the subject. <a name="bb0080" href="#b0080" class="workspace-trigger">Castelhano, Mack, and Henderson (2009)</a>
 looked at the influence of task on a group of summary statistics 
(including the ones used in Greene’s experiment) for the two tasks of 
memorization and visual search. After considering various features of 
eye trajectories, they came to the conclusion that the visual-task does 
not influence the features obtained from individual fixations. A similar
 result was obtained in <a name="bb0360" href="#b0360" class="workspace-trigger">Mika et al. (1999)</a>, where they also used the same features as in <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012)</a>.
 However, even though it is evident that summary statistics are not well
 suited for implementing an inverse Yarbus process, it may still be the 
case that other, more informative, features could do the job. For 
instance, it is shown in <a name="bb0035" href="#b0035" class="workspace-trigger">Borji and Itti (2014)</a>
 that using the spatial information along with the summary statistics of
 the eye movements can marginally improve the results. In their 
experiment, <a name="bb0035" href="#b0035" class="workspace-trigger">Borji and Itti (2014)</a>
 replicated Greene’s experiment and showed that by adding the spatial 
information to the aggregate eye movement features a slightly, but 
significantly (34.12% correct versus 25% chance level; binomial test, <span class="math"><math><mrow is="true"><mi is="true">p</mi><mo is="true">=</mo><mn is="true">1.07</mn><mo is="true">×</mo><msup is="true"><mrow is="true"><mn is="true">10</mn></mrow><mrow is="true"><mo is="true">-</mo><mn is="true">4</mn></mrow></msup></mrow></math></span>), better accuracy can be obtained in decoding the observers’ task.</p><p id="p0025">To motivate our method for implementing the inverse Yarbus process, it is worthwhile to first examine the <em>forward Yarbus process</em>,
 in which the task is given as the input and the measured task-dependent
 eye trajectory is the output. The first question to ask regarding the 
forward Yarbus process is what, if anything, determines the gaze 
direction while viewing a scene. The fundamental premise in this regard 
is that gaze follows the allocation of <em>selective visual attention</em>.
 Then, the assumption is that viewer task modulates, in some fashion, 
the allocation of attention, which is then reflected in the overt gaze 
shifts. Let us first review the approaches that have been developed for 
modeling visual attention, and then consider how task modulates 
attention.</p><section id="s0010"><h3 id="st010" class="u-h4 u-margin-m-top u-margin-xs-bottom">1.1. Attention modeling</h3><p id="p0030">In
 every second a vast quantity of visual information enters our eyes, 
only a fraction of which can be processed by the limited neuronal 
hardware available to our visual system. However, the human brain has 
the ability to process the visual information in real time thanks to the
 mechanisms of <em>visual attention</em>. Visual attention is the 
process that is responsible for selecting a subset of information to be 
processed in the higher levels of the visual system (<a name="bb0105" href="#b0105" class="workspace-trigger">Desimone &amp; Duncan, 1995</a>). This selection process can be interpreted as the directing of a <em>focus of attention</em> (FOA) to a circumscribed region in the visual field (<a name="bb0375" href="#b0375" class="workspace-trigger">Niebur &amp; Koch, 1998, chap. 9</a>).</p><p id="p0035">An influential concept in attention modeling is that of <em>salience</em>,
 a term which can be loosely defined as the prominence or conspicuity of
 region or object in a scene. Salient regions are, in this view, <em>attractive</em>
 to attention, and attention will therefore be preferentially directed 
to these regions. Gaze shifts would then be expected to follow the 
attention shifts to these salient points. The extent to which a 
salience-based model of attention predicts the direction of gaze is 
often used as a measure of performance for that model.</p><div><p id="p0040">The earliest saliency-based attention models were <em>bottom-up</em>
 models, which defined salience solely on features derived from the 
visual input. These models were typically task-independent. In the case 
of bottom-up attention models, the allocation of attention is based on 
the characteristics of the visual stimuli, and does not employ any 
top-down guidance or task information to shift attention. One of the 
most advanced saliency models is the one proposed by <a name="bb0260" href="#b0260" class="workspace-trigger">Itti and Koch (2001)</a>.
 In this model the FOA is guided by a map that conveys the saliency of 
each location in the field of view. The saliency map is built by 
linearly combining the <em>feature maps</em>, which are the outputs from different filters tuned to simple visual attributes, such as color, intensity and orientation (see <a name="bf0010" href="#f0010" class="workspace-trigger">Fig. 2</a>a).</p><figure class="figure text-xs" id="f0010"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr2.jpg" alt="" aria-describedby="cn010" height="261"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (286KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (286KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn010"><p id="sp020"><span class="label">Fig. 2</span>. (a) General architecture of bottom-up attention model by <a name="bb0260" href="#b0260" class="workspace-trigger">Itti and Koch (2001)</a>.
 The saliency map is built by linearly combining the feature maps which 
are the outputs from different filters tuned to simple visual 
attributes, such as color, intensity and orientation. (b) Influence of 
top-down, task-dependent priors on bottom-up attention models. The 
influence can be modeled as a weight vector modulating the linear 
combination of the feature maps (<a name="bb0265" href="#b0265" class="workspace-trigger">Itti &amp; Koch, 2001</a>).</p></span></span></figure></div><p id="p0045">Although
 image salience models have been extensively researched and are quite 
well-developed, empirical evaluation of such models show that they are 
poor at accounting for actual attention allocations when a visual-task 
is involved (<a name="bb0135" href="#b0135" class="workspace-trigger">Einhäuser, Rutishauser, &amp; Koch, 2008</a>). In a study by <a name="bb0275" href="#b0275" class="workspace-trigger">Judd, Durand, and Torralba (2012)</a>
 a database of 11,700 eye trajectories obtained from 39 subjects viewing
 a dataset of 300 natural images was created. The study compared the 
performance of 10 different salience-based models of visual attention in
 predicting the eye trajectories of the database. The results of this 
study indicate that early bottom-up attention models and their 
variations, such as the saliency model of <a name="bb0260" href="#b0260" class="workspace-trigger">Itti and Koch (2001)</a>,
 perform only slightly above the chance level when it comes to 
predicting fixations on natural images. The study also showed that 
although more recent models, such as the information-theoretic model of <a name="bb0040" href="#b0040" class="workspace-trigger">Bruce and Tsotsos (2009)</a> and the context-based models of <a name="bb0505" href="#b0505" class="workspace-trigger">Torralba et al. (2006)</a> and <a name="bb0175" href="#b0175" class="workspace-trigger">Goferman, Zelnik-Manor, and Tal (2012)</a>,
 perform better than the early models, their performance in predicting 
the location of fixations is still less than that obtained simply by 
using the fixations of a single human viewer. This conclusion was based 
on an evaluation of the accuracy of a fixation map from one human 
observer in predicting the fixation map of the other 38 observers. The 
accuracy was averaged over all images in the database and indicated how 
well the fixation map of a single human can predict an average fixation 
map of humans. This indicates the importance of incorporating actual eye
 movement patterns into attention models.</p><p id="p0050">Much of the shortfall in performance of the approaches considered in the <a name="bb0275" href="#b0275" class="workspace-trigger">Judd, Durand, and Torralba (2012)</a>
 study can be ascribed to the lack of task-dependence in the models. 
Attention is not just a passive enhancement of the visual stimuli, 
rather, it actively selects certain parts of a scene based on the needs 
of the ongoing visual task. This has led to the development of <em>top-down</em> models, which modulate the bottom-up features based on high level reasoning, volition and viewer task (<a name="bb0100" href="#b0100" class="workspace-trigger">Connor, Egeth, &amp; Yantis, 2004</a>). In a recent study by <a name="bb0030" href="#b0030" class="workspace-trigger">Borji and Itti (2013)</a>
 65 state-of-the-art models of attention were studied and categorized as
 either bottom-up or top-down. In each category the models were 
qualitatively compared over 13 experimental criteria. One of these 
criteria was the accuracy with which a model predicts real-world eye 
movement patterns as quantified by a spatial correlation coefficient. In
 order to evaluate the statistical relationship of the saliency models 
with the eye movement datasets, the recorded eye trajectories can be 
combined to form a <em>ground-truth saliency map</em> that takes on a 
form similar to that of the saliency map produced by saliency-based 
attention models. This map, along with other features that are studied 
in <a name="bb0030" href="#b0030" class="workspace-trigger">Borji and Itti (2013)</a>, are often used in studies of attention to objectively evaluate the models.</p><p id="p0055"><a name="bf0010" href="#f0010" class="workspace-trigger">Fig. 2</a>b shows an illustration of the interaction between top-down and bottom-up models as proposed by <a name="bb0265" href="#b0265" class="workspace-trigger">Itti and Koch, 2001</a>, <a name="bb0440" href="#b0440" class="workspace-trigger">Rutishauser and Koch, 2007</a>. In this model different tasks enforce different weight vectors in the linear combination phase. <a name="bb0130" href="#b0130" class="workspace-trigger">Ehinger et al. (2009)</a>
 achieved a 94% agreement with human eye movements in a visual search 
task by combining saliency maps with scene context and target features. <a name="bb0505" href="#b0505" class="workspace-trigger">Torralba et al. (2006)</a>
 also used contextual information for facilitating object search in 
natural scenes. The contextual guidance model of attention uses the 
bottom-up saliency map, scene context, and top-down mechanisms at an 
early stage of visual processing and combines them into a unified 
attention map. <a name="bb0285" href="#b0285" class="workspace-trigger">Kanan et al. (2009)</a> used the knowledge about how and where objects tend to appear in a scene in order to derive an appearance-based saliency model.</p><p id="p0060">Although
 salience-based top-down models address the problem of task independence
 of the bottom-up models, they are based on assumptions that can degrade
 their performance. The development of salience-based attention models 
generally proceeded under a <em>picture-viewing</em> paradigm, wherein a
 static 2-dimensional image or photograph was viewed. In addition, such 
models were typically created to handle simple situations where viewers 
performed search or detection tasks where the targets can be defined by 
simple conjunctions of high contrast visual features. <a name="bb0490" href="#b0490" class="workspace-trigger">Tatler et al. (2011)</a>
 showed that gaze allocation models that are based on salience are 
limited in accounting for many aspects of free viewing of complex scenes
 and often fail when applied in the context of natural (as opposed to 
artificially constrained search) task performance. They argued for 
moving away from models based on the picture-viewing paradigm and 
focusing on the principles governing gaze allocation in a broader range 
of experimental contexts.</p></section><section id="s0015"><h3 id="st015" class="u-h4 u-margin-m-top u-margin-xs-bottom">1.2. Linking attention and gaze direction</h3><p id="p0065">An
 important aspect of the (forward) Yarbus process is that attention 
allocation (suitably modulated by task) determines the direction of 
gaze. The most straightforward implementation of this is to direct the 
gaze to the most salient scene point. There is compelling evidence that 
the mammalian visual-motor system employs such a targeting scheme (<a name="bb0245" href="#b0245" class="workspace-trigger">Henderson, 1992</a>, <a name="bb0085" href="#b0085" class="workspace-trigger">Clark, 1999</a>)
 at least in simple constrained situations. One of the arguments for the
 use of salience maps in modeling natural visual behavior is that 
spatial deviations of low-level features from the local surround are 
cognitively relevant. However, while the contrast of low-level features 
in fixated locations are shown to be statistically higher than control 
locations in an image, this correlation is relatively weak in more 
complex situations (<a name="bb0350" href="#b0350" class="workspace-trigger">Mannan et al., 1997</a>, <a name="bb0395" href="#b0395" class="workspace-trigger">Parkhurst et al., 2002</a>, <a name="bb0430" href="#b0430" class="workspace-trigger">Reinagel and Zador, 1999</a>).
 This lack of explanatory power for image salience in the context of 
active tasks is evident in studies of natural tasks such as hitting a 
ball (<a name="bb0005" href="#b0005" class="workspace-trigger">Ballard and Hayhoe, 2009</a>, <a name="bb0330" href="#b0330" class="workspace-trigger">Land and McLeod, 2000</a>), tea making (<a name="bb0335" href="#b0335" class="workspace-trigger">Land, Mennie, &amp; Rusted, 1999</a>) and sandwich making (<a name="bb0230" href="#b0230" class="workspace-trigger">Hayhoe et al., 2003</a>).
 In these tasks saccades are often directed to the expected points of 
contact, which can exhibit low salience. Due to this lack of explanatory
 power of image salience models, another class of task-dependent visual 
attention models is emerging which emphasizes cognitive relevance 
hypotheses in predicting fixation locations. In cognitive relevance 
models an object-based representation of the scene is used to select 
fixation locations based on the needs of the cognitive system in 
relation to the current task, and saccade targets are ranked based on 
the cognitive relevance of the objects to the task (<a name="bb0385" href="#b0385" class="workspace-trigger">Nuthmann &amp; Henderson, 2010</a>).
 In some hybrid models, the cognitive relevance and image salience are 
combined to include both low-level, image-based and medium-level, 
proto-object-based representations of the attentional map into a 
coherent architecture based on real cognitive behavior of the visual 
system in the presence of visual task (<a name="bb0530" href="#b0530" class="workspace-trigger">Wischnewski et al., 2010</a>; <a name="bb0535" href="#b0535" class="workspace-trigger">Wischnewski et al. (2009)</a>).</p><p id="p0070"><a name="bb0490" href="#b0490" class="workspace-trigger">Tatler et al. (2011)</a>
 highlighted another deficiency of simplistic salience models, which is 
that the decision about where to fixate in these approaches is commonly 
made by a winner-takes-all process that selects the most conspicuous 
location on a salience map. This selection criterion, however, fails to 
account for the decrease in acuity with eccentricity. Moreover, in order
 to allow attention to move on from the most salient location in the 
map, these models assume a process known as <em>inhibition of return (IOR)</em>
 to inhibit the focus of attention from returning to the recently 
attended locations. Although IOR is supported by many classical 
psychophysical studies (<a name="bb0295" href="#b0295" class="workspace-trigger">Klein, 1980</a>, <a name="bb0300" href="#b0300" class="workspace-trigger">Klein, 2000</a>, <a name="bb0305" href="#b0305" class="workspace-trigger">Klein and MacInnes, 1999</a>, <a name="bb0420" href="#b0420" class="workspace-trigger">Posner and Cohen, 1984</a>), recent empirical evidence in viewing photographic images argues against such an effect (<a name="bb0475" href="#b0475" class="workspace-trigger">Smith and Henderson, 2009</a>, <a name="bb0495" href="#b0495" class="workspace-trigger">Tatler and Vincent, 2008</a>). <a name="bb0490" href="#b0490" class="workspace-trigger">Tatler et al. (2011)</a>
 wrote of the importance of temporal information about the eye 
movements, which is usually neglected in the simple salience-based 
models. The primary goal of salience models is to spatially model 
fixations, and the temporal aspects of viewing behavior is usually 
ignored. Evidence from studies of gaze during the performance of natural
 tasks emphasizes the need to consider fixation duration as well as 
fixation location (<a name="bb0125" href="#b0125" class="workspace-trigger">Droll et al., 2005</a>; <a name="bb0225" href="#b0225" class="workspace-trigger">Hayhoe, Bensinger, &amp; Ballard, 1998</a>; <a name="bb0335" href="#b0335" class="workspace-trigger">Land, Mennie, &amp; Rusted, 1999</a>).</p><p id="p0075">Another
 limitation of current salience models lies in their postulating that 
saccades are precisely directed to the target locations for processing (<a name="bb0490" href="#b0490" class="workspace-trigger">Tatler et al., 2011</a>).
 While this appears to be a plausible assumption in simple viewing 
tasks, in the context of natural tasks this assumption is generally 
invalid. For instance, <a name="bb0270" href="#b0270" class="workspace-trigger">Johansson et al. (2001)</a>
 showed that, for a task of moving an object past an obstacle, foveating
 the target within 3 degrees of visual angle was sufficient. Similarly, 
in a tea making task (<a name="bb0270" href="#b0270" class="workspace-trigger">Johansson et al., 2001</a>)
 corrective saccades of amplitude less than 2.5 degrees were infrequent,
 suggesting that, in natural behavior, fixations land close to the 
targets only in the case of attention demanding targets but typically do
 not precisely follow the focus of attention. It has long been known 
that short latency saccades, in which target-directed eye movements are 
made quickly in response to the onset of a target, frequently miss the 
target, instead being directed to the <em>center-of-mass</em> of the visual grouping of the target object and its surround (<a name="bb0095" href="#b0095" class="workspace-trigger">Coëffé &amp; O’regan, 1987</a>).</p><p id="p0080">The
 final aspect of the (forward) Yarbus process to be considered is the 
link between the gaze direction and the visual-task. While certain 
statistical features of eye movements remain unchanged across different 
tasks, the COG tends to be directed to targets that are relevant to the 
task at hand. This effect can be seen in the eye trajectories of Yarbus,
 in which the viewers fixated on the targets that were informative for 
the task. For instance, in the task of age estimation, faces were more 
likely to get fixated, while for the task of wealth estimation inanimate
 objects in the room became of more interest to the viewer. Many other 
studies of eye movements during natural behaviors have likewise 
indicated that there is a link between the gaze location and informative
 locations and the immediate task goals (<a name="bb0145" href="#b0145" class="workspace-trigger">Epelboim et al., 1995</a>; <a name="bb0230" href="#b0230" class="workspace-trigger">Hayhoe et al., 2003</a>; <a name="bb0320" href="#b0320" class="workspace-trigger">Land &amp; Furneaux, 1997</a>; <a name="bb0335" href="#b0335" class="workspace-trigger">Land, Mennie, &amp; Rusted, 1999</a>; <a name="bb0400" href="#b0400" class="workspace-trigger">Patla and Vickers, 1997</a>, <a name="bb0410" href="#b0410" class="workspace-trigger">Pelz and Canosa, 2001</a>). In the visual attention model of <a name="bb0460" href="#b0460" class="workspace-trigger">Schneider (1995)</a>,
 target selection was partially governed by the action being performed. 
This selection for action was highlighted by the fact that the gaze 
targets were concentrated in the task-relevant areas in an image while a
 visual-task was being performed (<a name="bb0230" href="#b0230" class="workspace-trigger">Hayhoe et al., 2003</a>; <a name="bb0335" href="#b0335" class="workspace-trigger">Land, Mennie, &amp; Rusted, 1999</a>), whereas before beginning the task, eye fixations were scattered over the image (<a name="bb0230" href="#b0230" class="workspace-trigger">Hayhoe et al., 2003</a>; <a name="bb0435" href="#b0435" class="workspace-trigger">Rothkopf, Ballard, &amp; Hayhoe, 2007</a>).</p><p id="p0085">To better demonstrate the gaze deployment under the influence of task, <a name="bb0435" href="#b0435" class="workspace-trigger">Rothkopf, Ballard, and Hayhoe (2007)</a>
 carried out a series of experiments conducted in a virtual environment,
 where subjects executed the two tasks of “approaching” and “avoiding” 
objects while navigating along a walkway. In these experiments they 
showed that the distribution of fixations on an object changes according
 to the task and suggested that human gaze is directed toward regions in
 a scene determined primarily by the task requirements.</p><p id="p0090">Besides the distribution of gaze locations, visual-task influences other metrics of eye movements. <a name="bb0485" href="#b0485" class="workspace-trigger">Tatler, Baddeley, and Vincent (2006)</a> showed that visual task also affects the temporal statistics of eye movements in viewing natural images. <a name="bb0080" href="#b0080" class="workspace-trigger">Castelhano, Mack, and Henderson (2009)</a> looked at eye movements during <em>memorization</em> and <em>search</em>
 tasks and showed that the task influences a number of eye movement 
measures, including the number of fixations and gaze duration on 
specific objects, while leaving unchanged other parameters, such as the 
average saccade amplitude and individual fixation durations. They also 
showed that the task biases the selection of scene regions and temporal 
measures on those regions. In <a name="bb0270" href="#b0270" class="workspace-trigger">Johansson et al. (2001)</a>
 a temporal coupling between vision and action was demonstrated. In 
their experiment they detected the onset of gaze shifts towards the next
 target relative to the hand movements as the subject maneuvered an 
object past an obstacle. The gaze shift was shown to be linked with the 
execution of the task, as the gaze moved to the next target as soon as 
the object cleared the obstacle. Temporal coupling between action and 
vision was also demonstrated for the tasks of driving (<a name="bb0325" href="#b0325" class="workspace-trigger">Land and Lee, 1994</a>, <a name="bb0340" href="#b0340" class="workspace-trigger">Land and Tatler, 2001</a>), tea making (<a name="bb0230" href="#b0230" class="workspace-trigger">Hayhoe et al., 2003</a>), sandwich making (<a name="bb0335" href="#b0335" class="workspace-trigger">Land, Mennie, &amp; Rusted, 1999</a>), music sight reading (<a name="bb0170" href="#b0170" class="workspace-trigger">Furneaux &amp; Land, 1999</a>), walking (<a name="bb0405" href="#b0405" class="workspace-trigger">Patla &amp; Vickers, 2003</a>) and reading aloud (<a name="bb0060" href="#b0060" class="workspace-trigger">Buswell, 1920</a>). In <a name="bb0330" href="#b0330" class="workspace-trigger">Land and McLeod (2000)</a>
 the eye movements of cricket players were studied and it was shown that
 different skill levels of the players in performing the task generally 
result in different latencies in directing the gaze towards predicted 
locations of the incoming ball. This temporal coupling between action 
and vision shows that models of visual-motor system function must 
consider task influence on the temporal characteristics of eye movement 
as well as on the spatial characteristics.</p><p id="p0095">The task also affects the pattern, or sequencing, of eye movements. In the aforementioned study of <a name="bb0330" href="#b0330" class="workspace-trigger">Land and McLeod (2000)</a>
 it was shown that while watching a cricket game the gaze is directed 
according to the ongoing events in the game. In another experiment, the 
eye movements of subjects were recorded while watching a person stack a 
set of blocks <a name="bb0160" href="#b0160" class="workspace-trigger">Flanagan and Johansson (2003)</a>.
 In this block-sorting task, the viewers’ gaze was shown to be 
anticipating the expected points of interaction. In another 
block-copying experiment (<a name="bb0010" href="#b0010" class="workspace-trigger">Ballard, Hayhoe, &amp; Pelz, 1995</a>)
 the eye movements showed similar patterns through the progression of 
the task that could be interpreted in terms of momentary information 
processing needs. <a name="bb0090" href="#b0090" class="workspace-trigger">Clark and O’Regan (1998)</a> studied the spatial characteristics of eye movements for the task of reading and showed that when reading a text, the <em>center of gaze</em>
 (COG) lands on the locations that minimize the ambiguity of the word 
arising from the incomplete recognition of the letters. In a seminal 
study, <a name="bb0510" href="#b0510" class="workspace-trigger">Treisman and Gelade (1980)</a> developed the <em>feature integration theory</em> that modeled the attentional deployment in the task of visual search. In <a name="bb0545" href="#b0545" class="workspace-trigger">Wolfe, Cave, and Franzel (1989)</a> an improved model called <em>Guided Search</em> was suggested that studied how our brain directs attention through a scene during a search task. <a name="bb0220" href="#b0220" class="workspace-trigger">Hayhoe and Ballard (2005)</a>
 reviewed the goal-directed behavior of the visual-motor system, and 
provided a comprehensive set of references to studies of task influence 
on eye movements.</p><p id="p0100">It can be seen from the material 
presented in this section that visual task does influence the spatial 
and temporal patterns of eye movements. It is therefore conceivable that
 it should be possible to invert this process. At a more general level, 
eye movement patterns can serve as a window into the brain, and be used 
to infer the mental states of observers. This has been studied by many 
researchers. <a name="bb0050" href="#b0050" class="workspace-trigger">Bulling et al. (2009)</a>, <a name="bb0055" href="#b0055" class="workspace-trigger">Bulling et al. (2011)</a>
 successfully used eye movement analysis for recognizing the physical 
activity of subjects while copying a text, reading a printed paper, 
taking hand-written notes, watching a video, browsing the web or being 
idle. It would be of obvious utility to know the mental state of people 
engaged in safety–critical attention-demanding activities such as 
driving a car or flying a plane. Detection of tiredness or distraction 
of the operator could be used to trigger alarms or machine backup 
systems (<a name="bb0115" href="#b0115" class="workspace-trigger">Di Stasi et al., 2012</a>). As an example of how this could be done, in <a name="bb0120" href="#b0120" class="workspace-trigger">Di Stasi et al. (2010)</a> the maximum eye velocity during saccadic movements was shown to be inversely proportional to the <em>mental workload</em> of subjects in a simulated driving task. In a study by <a name="bb0025" href="#b0025" class="workspace-trigger">Benson et al. (2012)</a> eye movement analysis was used to detect schizophrenia. In <a name="bb0455" href="#b0455" class="workspace-trigger">Schleicher et al. (2008)</a>
 blink duration, delay of lid reopening, blink interval, and 
standardized lid closure speed were identified as indicators of mental 
fatigue. These studies share a common conclusion, which is that it is 
possible to predict an observer’s cognitive state by analyzing his eye 
movement behavior. Continuing along this line of thinking, we consider 
the visual-task being carried out as an aspect of the cognitive state 
and therefore aim to predict or infer the task by analyzing the 
observer’s eye movement behavior.</p><p id="p0105">The inversion process
 should use features of the eye movement trajectories that are more 
informative than summary statistics, and should be able to model <em>covert</em> attention allocation rather than just the position of the eyes (or <em>overt</em>
 attention). The inversion technique should be applicable to complex 
natural scenes and abstract tasks such as those in the original Yarbus 
experiment. To this end, we propose to use Hidden-Markov-Models (HMMs) 
to relax the inherent assumptions in the simplistic salience models and 
use real-world eye movements to train task-dependent models that can 
infer the visual-task on natural images. In the following sections of 
the paper we will show how HMMs accomplish this by modeling the fixation
 distributions with a Gaussian distribution function that allows for 
fixations well away from the target (assumed to be associated with the 
covert attentional locus). Moreover, by analyzing the eye trajectories 
as time-series we give the temporal features of eye movements the same 
importance as the spatial features. The modeling of the cognitive 
relevance of the low-level features is facilitated by the HMM approach, 
as the Gaussian distributions are allowed to move away from salient 
objects to more cognitively relevant targets in an image. The Gaussian 
distributions also account for overshooting and undershooting of targets
 when directing the gaze. Consequently, the assumption of precise 
targeting inherent in the salience models is relaxed in the HMMs by 
using the observation distributions over the targets. Moreover, using 
HMMs to model the transition of the attentional focus from one location 
to another overcomes the inherent shortcomings of the target selection 
processes used in the salience models. In an HMM-based model the target 
selection is governed by a statistical process that is trained on 
natural eye trajectories measured during task execution, which replaces 
processes such as winner-takes-all and inhibition of return that are 
associated with target selection in salience-based models.</p></section><section id="s0020"><h3 id="st020" class="u-h4 u-margin-m-top u-margin-xs-bottom">1.3. Attention tracking using Hidden Markov models</h3><p id="p0110">In
 the previous section we observed that classical models of attention are
 limited in terms of accounting for real-world eye movements of 
observers while viewing natural images. This can be seen in the 
benchmark presented in <a name="bb0275" href="#b0275" class="workspace-trigger">Judd, Durand, and Torralba (2012)</a>,
 which compared the performances of salience models in predicting eye 
fixations made on natural images. One of the most striking experiments 
done in this study was to compare the performance of the best salience 
model and a model based on real eye trajectories. It was shown that even
 the best model performs worse than the fixation map of just one human 
observer in terms of prediction rate of the eye trajectories. Thus, we 
base the development of our attention model on actual task-dependent eye
 trajectories recorded while viewing natural images. To do so, we use 
Hidden Markov models (HMMs) as a tool for time-series analysis of the 
eye trajectories to encode the dynamics of natural eye movements into 
task-dependent models. One of the benefits of our HMM model is its 
trainability on natural eye movements to capture their spatial and 
temporal patterns rather than purely depending on analyzing the patterns
 of image features in fixated regions, as done in the salience models.</p><p id="p0115">Hidden Markov models (HMMs) are a group of generative models that are used in supervised and semi-supervised learning (<a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner, 1990</a>).
 Similar to the first-order, finite-state, discrete-time Markov chain 
(DTMC), HMMs govern the transition between the states by a first-order 
Markov process.</p><p id="p0120">A typical DTMC can be defined by a set of parameters, <span class="math"><math><mrow is="true"><mi is="true">γ</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><mi is="true">A</mi><mtext is="true">,</mtext><mi is="true">Π</mi><mo stretchy="false" is="true">}</mo></mrow></math></span>, where:<dl class="list"><dt class="list-label">•</dt><dd class="list-description"><p id="p0500"><span class="math"><math><mrow is="true"><mi is="true">A</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math></span> is <em>the state transition probability distribution</em><span class="display"><span id="e0005" class="formula"><span class="label">(1)</span><span class="math"><math><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mspace width="2em" is="true"></mspace><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">i</mi><mtext is="true">,</mtext><mi is="true">j</mi><mo is="true">⩽</mo><mi is="true">N</mi></math></span></span></span></p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0505"><span class="math"><math><mrow is="true"><mi is="true">Π</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">π</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math></span> is <em>the initial state distribution</em></p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0510"><span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">π</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math></span> is the probability of starting a sequence at state <em>i</em><span class="display"><span id="e0010" class="formula"><span class="label">(2)</span><span class="math"><math><msub is="true"><mrow is="true"><mi is="true">π</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mspace width="2em" is="true"></mspace><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">i</mi><mo is="true">⩽</mo><mi is="true">N</mi></math></span></span></span></p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0515"><span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">∈</mo><mi is="true">S</mi></mrow></math></span> and <span class="math"><math><mrow is="true"><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">t</mi><mo is="true">⩽</mo><mi is="true">T</mi></mrow></math></span> is the state at time <em>t</em></p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0520"><span class="math"><math><mrow is="true"><mi is="true">S</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">N</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math></span> is the <em>state space</em></p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0525"><em>N</em> is the number of states in the model</p></dd></dl></p><p id="p0125">In a more general view, both HMMs and DTMCs are classes of finite state machines (FSMs) (<a name="bb0020" href="#b0020" class="workspace-trigger">Bengio &amp; Frasconi, 1995</a>) that at each time step generates an observation sample vector <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub></mrow></math></span> <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">∈</mo><mo stretchy="false" is="true">[</mo><mn is="true">1</mn><mtext is="true">,</mtext><mi is="true">T</mi><mo stretchy="false" is="true">]</mo><mo stretchy="false" is="true">)</mo></mrow></math></span>
 according to the state currently being visited. Therefore, in each 
traverse of these FSMs we will obtain an observation sequence <span class="math"><math><mrow is="true"><mi is="true" mathvariant="bold">O</mi></mrow></math></span>, where:<dl class="list"><dt class="list-label">•</dt><dd class="list-description"><p id="p0530"><span class="math"><math><mrow is="true"><mi is="true" mathvariant="bold">O</mi></mrow></math></span> is a sequence of <em>T</em> observations <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math></span></p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0535"><span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub></mrow></math></span> (<span class="math"><math><mrow is="true"><mi is="true">t</mi><mo is="true">∈</mo><mo stretchy="false" is="true">[</mo><mn is="true">1</mn><mtext is="true">,</mtext><mi is="true">T</mi><mo stretchy="false" is="true">]</mo></mrow></math></span>) is an observation sample vector consisted of <em>M</em> feature values <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">o</mi></mrow><mrow is="true"><mi is="true">t</mi><mtext is="true">,</mtext><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">o</mi></mrow><mrow is="true"><mi is="true">t</mi><mtext is="true">,</mtext><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">o</mi></mrow><mrow is="true"><mi is="true">t</mi><mtext is="true">,</mtext><mi is="true">M</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math></span></p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0540"><em>M</em> is the number of feature values in each observation.</p></dd></dl></p><div><p id="p0130">In
 a DTMC, each state can only generate a specific set of observation 
vectors, meaning that there is no overlap between the observation 
vectors of different states. <a name="bf0015" href="#f0015" class="workspace-trigger">Fig. 3</a>a shows a DTMC with two states (i.e., <span class="math"><math><mrow is="true"><mi is="true">N</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math></span>). At time step <span class="math"><math><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math></span>, the process starts by entering one of the states <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></math></span> or <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></mrow></math></span> with the probability of <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">π</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></math></span> and <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">π</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></mrow></math></span>, respectively. In the following time steps, the process chooses the next state according to the transition probabilities <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub></mrow></math></span>. At each time step an observation is generated according to the current state, which reveals the current state of the DTMC.</p><figure class="figure text-xs" id="f0015"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr3.jpg" alt="" aria-describedby="cn015" height="131"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (89KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (89KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn015"><p id="sp025"><span class="label">Fig. 3</span>. (a) A first-order, finite-state, discrete-time Markov chain (DTMC) with two states (i.e., <span class="math"><math><mrow is="true"><mi is="true">N</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math></span>). The DTMC is defined by a <em>state space</em> <span class="math"><math><mrow is="true"><mi is="true">S</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">:</mo><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">i</mi><mo is="true">⩽</mo><mi is="true">N</mi><mo stretchy="false" is="true">}</mo></mrow></math></span>, a <em>state transition matrix</em> <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">A</mi></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">N</mi></mrow></msub><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><mo is="true">:</mo><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">i</mi><mtext is="true">,</mtext><mi is="true">j</mi><mo is="true">⩽</mo><mi is="true">N</mi><mo stretchy="false" is="true">}</mo></mrow></math></span> and a set of <em>initial state distribution</em> <span class="math"><math><mrow is="true"><mi is="true">Π</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">π</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">:</mo><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">i</mi><mo is="true">⩽</mo><mi is="true">N</mi><mo stretchy="false" is="true">}</mo></mrow></math></span>.
 (b) A sample trajectory that is generated by the DTMC. In the 
trajectory the states are overt and the observer can see which state is 
visited at each time step.</p></span></span></figure></div><p id="p0135"><a name="bf0015" href="#f0015" class="workspace-trigger">Fig. 3</a>b shows a sample state sequence of the process, <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">:</mo><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">t</mi><mo is="true">⩽</mo><mn is="true">3</mn><mo stretchy="false" is="true">}</mo></mrow></math></span>, where <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math></span> is the state that the sequence is visiting at time <em>t</em>. The overall observation sequence is in the form <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">:</mo><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">t</mi><mo is="true">⩽</mo><mn is="true">3</mn><mo stretchy="false" is="true">}</mo></mrow></math></span>,
 which is equivalent to a unique state sequence due to the 
non-overlapping characteristic of the observation space between the 
states.</p><p id="p0140">The Markov process of an HMM is also defined by
 the parameters of the underlying DTMC. The only difference between the 
DTMC and the HMM is that in HMMs the observations are generated 
according to a state-specific density function, <em>B</em>, called <em>the observation pdf</em>.
 In contrast to the observations of a DTMC, in an HMM the observation 
pdf of different states can overlap and might generate the same 
observation as the output. Therefore, in HMMs we cannot directly map an 
observation to a unique state, which makes the states hidden to the 
observer.</p><p id="p0145">A typical discrete-time, continuous HMM, <span class="math"><math><mrow is="true"><mi is="true">λ</mi></mrow></math></span>, can be defined by a set of parameters, <span class="math"><math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><mi is="true">A</mi><mtext is="true">,</mtext><mi is="true">B</mi><mtext is="true">,</mtext><mi is="true">Π</mi><mo stretchy="false" is="true">}</mo></mrow></math></span>, where <span class="math"><math><mrow is="true"><mi is="true">B</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">}</mo></mrow></math></span> is the <em>observation probability density function</em> in the state <em>j</em> and<span class="display"><span id="e0015" class="formula"><span class="label">(3)</span><span class="math"><math><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mspace width="2em" is="true"></mspace><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">j</mi><mo is="true">⩽</mo><mi is="true">N</mi><mtext is="true">,</mtext><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">t</mi><mo is="true">⩽</mo><mi is="true">T</mi></math></span></span></span></p><div><p id="p0150"><a name="bf0020" href="#f0020" class="workspace-trigger">Fig. 4</a>a shows an HMM with two states, similar to the DTMC shown in <a name="bf0015" href="#f0015" class="workspace-trigger">Fig. 3</a>a. In this example, each observation (i.e., <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub></mrow></math></span>) is a 2D vector generated according to the state-specific, 2D Gaussian distribution functions.</p><figure class="figure text-xs" id="f0020"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr4.jpg" alt="" aria-describedby="cn020" height="134"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (95KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (95KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn020"><p id="sp030"><span class="label">Fig. 4</span>. (a) A HMM with two states (i.e., <span class="math"><math><mrow is="true"><mi is="true">N</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math></span>) is shown in this figure. In addition to the parameters of the underlying DTMC (i.e., <em>A</em> and <span class="math"><math><mrow is="true"><mi is="true">Π</mi></mrow></math></span>), an HMM has an extra parameter called <em>the observation pdf</em>, <em>B</em>,
 which gives the probability distribution over different observations in
 each state. (b) In the trajectories generated by HMMs, the state 
sequence is hidden to the observer and at each time step, an observation
 is generated according to a density function, <em>B</em>.</p></span></span></figure></div><p id="p0155"><a name="bf0020" href="#f0020" class="workspace-trigger">Fig. 4</a>b shows a sample outcome of the HMM of <a name="bf0020" href="#f0020" class="workspace-trigger">Fig. 4</a>a. The outcome of the process is an observation sequence <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">:</mo><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">t</mi><mo is="true">⩽</mo><mn is="true">3</mn><mo stretchy="false" is="true">}</mo></mrow></math></span>, where <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub></mrow></math></span> is the observation at time <em>t</em>.</p><p id="p0160">As mentioned in Section <a name="bs0010" href="#s0010" class="workspace-trigger">1.1</a>,
 classical attention models are based on a spatial saliency map that 
defines the conspicuous locations, which are potential targets of the 
fixations. In addition to the saliency maps, high-order processes are 
also observed to influence the selection of targets in an eye movement 
trajectory and are used as a source of information for attention 
allocation. <em>Proximity preference</em> is a cognitive process that facilitates fixations near the currently fixated target and <em>similarity preference</em> is a cognitive process that favors objects similar in appearance to the one that is currently being fixated (<a name="bb0310" href="#b0310" class="workspace-trigger">Koch &amp; Ullman, 1985</a>). <em>Inhibition of return</em> (IOR) (<a name="bb0300" href="#b0300" class="workspace-trigger">Klein, 2000</a>) is a high-level process that discourages fixation on the target that have been visited in the preceding period of time.</p><p id="p0165">These
 higher-level processes affect the selection of the next target based on
 the recently fixated ones, which suggests a Markov cognitive process as
 the target selection model of the visual motor mechanism. The HMMs use 
Markov processes as their underlying models for generating time-series 
observations similar to the eye trajectories. This feature of HMMs 
allows us to incorporate these higher-level processes into a coherent 
model for eye movement analysis.</p><p id="p0170">Markov processes have 
been considered in prior studies on eye movement generation, and have 
been shown to generate similar patterns to those produced by the 
mammalian visual-motor system. <a name="bb0200" href="#b0200" class="workspace-trigger">Hacisalihzade, Stark, and Allen (1992)</a>
 used Markov processes to model visual fixations of observers. They 
showed that the eyes visit the features of an object cyclically, 
following somewhat regular scanpaths<a name="bfn1" href="#fn1" class="workspace-trigger"><sup>1</sup></a> rather than crisscrossing it at random. <a name="bb0480" href="#b0480" class="workspace-trigger">Stark and Ellis (1981)</a> also proposed using Markov processes as a general model of fixation placement during the task of reading. <a name="bb0415" href="#b0415" class="workspace-trigger">Pieters, Rosbergen, and Wedel (1999)</a> observed a similar pattern in the scanpaths of the observers while looking at printed advertisements.</p><p id="p0175">If
 we consider each target in an image as a state, the saliency map and 
the Markov process define the probability of transitions from one state 
to another in an eye trajectory. This interpretation forms a 
finite-state, discrete-time Markov chain that gives us the likelihood of
 an eye trajectory based on the loci of fixations. Moreover, if we posit
 a first-order Markov process as the underlying process that governs the
 transitions between the targets (which was shown to be a valid 
assumption for eye movements (<a name="bb0200" href="#b0200" class="workspace-trigger">Hacisalihzade, Stark, &amp; Allen, 1992</a>)), we can train a first-order DTMC for each task. This model was used by <a name="bb0140" href="#b0140" class="workspace-trigger">Elhelw et al. (2008)</a>, where they successfully used a first-order DTMC to model eye movement dynamics.</p><div><p id="p0180">One
 of the main deficits of classical models is that they assume that 
tracking the FOA is equivalent to tracking the COG. However, as noted in
 the introduction, the COG does not necessarily follow the FOA and in 
fact they can be quite some distance from each other. <a name="bf0025" href="#f0025" class="workspace-trigger">Fig. 5</a>a
 shows an eye trajectory recorded when a viewer was asked to count the 
number of people in the image. While fixations mainly land on the 
targets of interest (<em>overt attention</em>), the person on the left 
does not get any fixation. The fact that the answer given by the viewer 
to the question was correct suggests that the COG does not necessarily 
follow the FOA and sometimes our <em>awareness</em> of a target does not imply foveation on that target (<em>covert attention</em>).</p><figure class="figure text-xs" id="f0025"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr5.jpg" alt="" aria-describedby="cn025" height="268"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (238KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (238KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn025"><p id="sp035"><span class="label">Fig. 5</span>. 
(a) Eye trajectories recorded while executing the task of “counting the 
number of people in the image”. In the trajectories straight lines 
depict saccades between two consecutive fixations (shown by dots). While
 the viewer gave the correct answer in the trial, one of the targets 
(the leftmost person in the image) is not fixated. The target that did 
not get any fixations is assumed to have been attended covertly. (b) 
Overlay of the 2D observation Gaussian distributions on top of an image.
 The combination of the Gaussian pdfs form the HMM that is trained for 
the task of counting faces on the image. The overall model can generate 
synthetic eye-trajectories based on the parameters of the HMM. The 
transitions between the states are governed by the transition 
probabilities, and at each time step, the state’s observation pdf 
generates the 2D coordinates of the next fixation. The trajectory shown 
in the image is the real eye movements of a viewer while performing the 
task. As we can see, all fixations are covered by the observation pdfs, 
which makes the whole trajectory a plausible outcome of the HMM.</p></span></span></figure></div><p id="p0185">The
 disparity between the FOA and the COG can be attributed to several 
other factors other than covert attention. Accidental 
attention-independent movement of eye, eye-tracking equipment bias, 
undershooting or overshooting of the target (<a name="bb0015" href="#b0015" class="workspace-trigger">Becker, 1972</a>), or the phenomenon of <em>center-of-gravity fixations</em> (<a name="bb0560" href="#b0560" class="workspace-trigger">Zelinsky et al., 1997</a>; <a name="bb0240" href="#b0240" class="workspace-trigger">He and Kowler, 1989</a>, <a name="bb0370" href="#b0370" class="workspace-trigger">Najemnik and Geisler, 2005</a>) are some of the most common sources of recurrent divergence between the COG and the FOA.</p><p id="p0190">In
 terms of a DTMC-based model of attention, the coordinates of the COGs 
recorded by the eye tracker comprise the observation sequence, which is 
taken as equivalent to the attentional states. Thus, in the DTMC 
approach covert attention, where the attentional state is different than
 the gaze coordinates, cannot be modeled by classical models of 
attention. However, in HMMs the states are hidden and can be different 
than the overt observations. Therefore, in our view, the HMMs can serve 
as a better alternative to the DTMCs in modeling the overt and covert 
shifts of attention. When entering a state of a HMM, a Gaussian 
distribution function generates an observation that is overt to the 
viewer (<a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner, 1990</a>).
 Thus, in our proposed model the states represent the FOAs, and the COGs
 form the observation sequences. In terms of the problem at hand, the 
hidden states of the HMM correspond to the covert attention loci and the
 observations of the HMM correspond to the eye positions or overt 
attention loci.</p><p id="p0195"><a name="bf0025" href="#f0025" class="workspace-trigger">Fig. 5</a>b
 shows an HMM that is trained on the eye trajectories recorded while 
executing the task of counting people in the image. Each 2D Gaussian 
probability density function (pdf) is depicted by a <em>heat map</em>, 
where the heat represents probability values. Each Gaussian pdf 
represents the distribution or probability of an attentional state, and 
at each time step a COG coordinate pair is generated by drawing a random
 outcome from these pdfs. For instance, directing the FOA (covert 
attention) to the face of the person on the left can result in a 
fixation (overt attention) that is further away from the physical 
boundaries of the face. The capability of Gaussian HMMs in representing 
off-target fixations is illustrated in this image by overlaying the 
trajectory of <a name="bf0025" href="#f0025" class="workspace-trigger">Fig. 5</a>a
 on the image. While the classical salience-base attention models fail 
to account for off-target fixations, here we show that the Gaussian 
observation function can properly model them.</p><p id="p0200">The theory of HMMs has been used in different fields, such as speech recognition (<a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner, 1990</a>), anomaly detection in video surveillance (<a name="bb0365" href="#b0365" class="workspace-trigger">Nair et al., 2002</a>) and hand writing recognition (<a name="bb0255" href="#b0255" class="workspace-trigger">Hu, Brown, &amp; Turin, 1996</a>). HMMs have also been used in analysis of eye movements. In <a name="bb0450" href="#b0450" class="workspace-trigger">Salvucci and Goldberg (2000)</a> HMMs were used to automatically label the recorded eye movements as either fixations or saccades. In another study (<a name="bb0445" href="#b0445" class="workspace-trigger">Salvucci &amp; Anderson, 2001</a>) developed an HMM-based model for analysis of eye movements during the task of equation solving. <a name="bb0470" href="#b0470" class="workspace-trigger">Simola, Salojärvi, and Kojo (2008)</a> modeled three cognitive states of visual process during a reading task by the hidden states of HMMs. <a name="bb0515" href="#b0515" class="workspace-trigger">Van Der Lans et al. (2008)</a> split a visual search task into two stages of <em>localization</em> and <em>identification</em> and mapped each of these cognitive states into one of the states of a two-state HMM.</p><p id="p0205">Recently <a name="bb0210" href="#b0210" class="workspace-trigger">Haji-Abolhassani and Clark (2013)</a>
 showed that HMMs can also serve as a good model for the visual 
attention process. They proposed an attention model that allowed for 
covert shifts of attention as well as overt ones. They used their model 
in tracking attention during visual search tasks that were conducted on 
synthetic images. However, in their model they assumed that the targets 
can be defined in advance and built their model based on the known 
location of targets. For instance, in the task of counting faces in <a name="bf0025" href="#f0025" class="workspace-trigger">Fig. 5</a>b,
 they assumed that the foci of attention will be on the faces. This 
assumption is valid for simple tasks with objective results (such as 
number of red objects and number of horizontal bars), but in more 
abstract tasks, such as the one used in the <a name="bb0555" href="#b0555" class="workspace-trigger">Yarbus (1967)</a> and the <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012)</a>
 experiments, defining the potential targets of attention is not 
straightforward. Another problematic aspect of the Haji-Abolhassani and 
Clark model is that the number of states has to be defined before 
training. This is only possible in images with a predefined number of 
targets (as in the synthetic images used in their experiments). However,
 in natural scenes the targets can appear anywhere in the image and 
typically no prior information about the location of the targets is 
available to the model.</p><p id="p0210">In this paper we present an 
HMM-based attention model that can be applied on natural images. The 
approach begins by first using the <em>K</em>-means clustering technique (<a name="bb0290" href="#b0290" class="workspace-trigger">Kaufman &amp; Rousseeuw, 2009</a>)
 to locate potential targets in an image and then using the HMM-based 
method to decode the eye trajectories. The overall method is then used 
to infer the visual-task in the same dataset that was used in <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012)</a>.</p></section></section><section id="s0025"><h2 id="st025" class="u-h3 u-margin-l-top u-margin-xs-bottom">2. An Inverse Yarbus process via Bayesian inference</h2><p id="p0215">HMMs
 are a class of semi-supervised learning methods. Being generative 
models, they classify the test data in a probabilistic manner that can 
be readily applied to the Bayesian inference framework. One of the many 
advantages of Bayesian inference is the ability to merge other sources 
of information in the a priori term and give an a posteriori probability
 distribution function over the possible outcomes, whereby a higher 
level process can make an inference and select a task as the result.</p><p id="p0220">Suppose observations obtained from the eye tracker are in the form of <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">〈</mo><mi is="true" mathvariant="bold">O</mi><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">〉</mo></mrow></math></span>, where <span class="math"><math><mrow is="true"><mi is="true">θ</mi><mo is="true">∈</mo><mi is="true">Θ</mi></mrow></math></span> is the task label, selected from the set of all task labels <span class="math"><math><mrow is="true"><mi is="true">Θ</mi></mrow></math></span>, and <span class="math"><math><mrow is="true"><mi is="true" mathvariant="bold">O</mi></mrow></math></span> is the observation sequence of fixation locations <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math></span>. Each <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math></span>, itself, is a vector <span class="math"><math><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math></span> containing the coordinates of a fixation at time <em>i</em>.</p><p id="p0225">In order to infer the visual-task from an eye trajectory we need to evaluate the posterior probability of different tasks, <span class="math"><math><mrow is="true"><mi is="true">θ</mi><mo is="true">∈</mo><mi is="true">Θ</mi></mrow></math></span>, given an observation sequence, <span class="math"><math><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow></math></span>. According to Bayes rule, this can be calculated as:<span class="display"><span id="e0020" class="formula"><span class="label">(4)</span><span class="math"><math><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true">θ</mi><mo stretchy="false" is="true">|</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">)</mo></mrow></mfrac><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><msub is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">θ</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo is="true">∈</mo><mi is="true">Θ</mi></mrow></msub><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><msup is="true"><mrow is="true"><mi is="true">θ</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">θ</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></mfrac><mtext is="true">.</mtext></math></span></span></span>In this equation <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo></mrow></math></span> is the <em>prior probability</em> of each task <span class="math"><math><mrow is="true"><mi is="true">θ</mi><mo is="true">∈</mo><mi is="true">Θ</mi></mrow></math></span>, and <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo></mrow></math></span> is the task conditional distribution, which is also referred to as the <em>likelihood</em>
 function. The prior distribution assigns a probability distribution to 
the tasks based on our prior knowledge. This is where we can apply other
 sources of information about the tasks and improve the inference. The 
likelihood term of the equation gives the probability of observing the 
sequence <span class="math"><math><mrow is="true"><mi is="true" mathvariant="bold">O</mi></mrow></math></span> while executing the task <span class="math"><math><mrow is="true"><mi is="true">θ</mi></mrow></math></span>. The likelihood term can be broken down to the conditional probabilities:<span class="display"><span id="e0025" class="formula"><span class="label">(5)</span><span class="math"><math><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">.</mtext></math></span></span></span></p><div><p id="p0230">In
 classical saliency-based attention models, the likelihood can be 
quantified as proportional to the amplitude of the saliency map on 
different targets in the image. <a name="bf0030" href="#f0030" class="workspace-trigger">Fig. 6</a> shows an example of a saliency map obtained using the <em>Saliency Toolbox</em> <a name="bb0525" href="#b0525" class="workspace-trigger">Walther and Koch (2006)</a>. <a name="bf0030" href="#f0030" class="workspace-trigger">Fig. 6</a>a
 shows a synthetic image that comprises a combination of “A” symbols and
 horizontal and vertical bars in three different colors. The objects are
 placed at the vertices of a <span class="math"><math><mrow is="true"><mn is="true">5</mn><mo is="true">×</mo><mn is="true">6</mn></mrow></math></span> grid, on a featureless black background. <a name="bf0030" href="#f0030" class="workspace-trigger">Fig. 6</a>b shows the bottom-up saliency map according to the attention model of <a name="bb0260" href="#b0260" class="workspace-trigger">Itti and Koch (2001)</a> shown in <a name="bf0010" href="#f0010" class="workspace-trigger">Fig. 2</a>a.
 The feature maps are obtained by applying color, intensity and 
orientation filters to the input image and integrated into the saliency 
map by a linear combination.</p><figure class="figure text-xs" id="f0030"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr6.jpg" alt="" aria-describedby="cn030" height="175"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (97KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (97KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn030"><p id="sp040"><span class="label">Fig. 6</span>. (a) Original Image. (b) Saliency map of the bottom-up attention model presented in <a name="bb0260" href="#b0260" class="workspace-trigger">Itti and Koch (2001)</a>. (c) Saliency map of the same image using a top-down attention model (<a name="bb0265" href="#b0265" class="workspace-trigger">Itti &amp; Koch, 2001</a>).</p></span></span></figure></div><p id="p0235">While bottom-up models combine the maps with constant weights, top-down models (shown in the block diagram of <a name="bf0010" href="#f0010" class="workspace-trigger">Fig. 2</a>b) modulate the weights according to the task <a name="bb0265" href="#b0265" class="workspace-trigger">Itti and Koch (2001)</a>. <a name="bf0030" href="#f0030" class="workspace-trigger">Fig. 6</a>c
 shows the saliency map of the same image tuned to the task of 
“searching for the characters”. As we can see, the locations of the 
characters are more conspicuous (lighter) in the top-down saliency map.</p><p id="p0240">Since
 in the bottom-up models the allocation of attention is merely based on 
the characteristics of the visual stimuli, the fixation locations are 
independent of the ongoing task (i.e., <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo></mrow></math></span> is assumed to be equal to <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">)</mo></mrow></math></span>) and the likelihood term becomes:<span class="display"><span id="e0030" class="formula"><span class="label">(6)</span><span class="math"><math><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">.</mtext></math></span></span></span>In
 top-down models, saliency of the targets is modulated by the task, 
which makes the likelihood term task-dependent. Moreover, if we use a 
discrete-time Markov chain (DTMC) to model high-level processes (<a name="bb0200" href="#b0200" class="workspace-trigger">Hacisalihzade, Stark, &amp; Allen, 1992</a>) such as inhibition of return (<a name="bb0300" href="#b0300" class="workspace-trigger">Klein, 2000</a>), proximity and similarity preference (<a name="bb0310" href="#b0310" class="workspace-trigger">Koch &amp; Ullman, 1985</a>), the likelihood term of Eq. <a name="be0025" href="#e0025" class="workspace-trigger">(5)</a> reduces to:<span class="display"><span id="e0035" class="formula"><span class="label">(7)</span><span class="math"><math><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">.</mtext></math></span></span></span>In Eq. <a name="be0035" href="#e0035" class="workspace-trigger">(7)</a> each task, <span class="math"><math><mrow is="true"><mi is="true">θ</mi></mrow></math></span>,
 is represented by the corresponding DTMC that is trained on the eye 
movements of the viewers who performed that task. The task-dependent 
DTMCs are represented by <span class="math"><math><mrow is="true"><mi is="true">γ</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><mi is="true">A</mi><mtext is="true">,</mtext><mi is="true">Π</mi><mo stretchy="false" is="true">}</mo></mrow></math></span>, a 2-state example of which is shown in <a name="bf0015" href="#f0015" class="workspace-trigger">Fig. 3</a>a.
 Since in DTMCs each sequence of fixations corresponds to a unique 
sequence of states having the parameters of the DCMCs, calculating the 
likelihood term is a matter of multiplying the state transitions that 
emerge in the trajectory.<span class="display"><span id="e0040" class="formula"><span class="label">(8)</span><span class="math"><math><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">T</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">T</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">T</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">.</mtext></math></span></span></span>In
 the HMMs, however, the states are hidden and the likelihood term cannot
 be evaluated directly. In the theory of HMMs there are three 
fundamental problems: <em>evaluation</em>, <em>decoding</em> and <em>training</em>. Assume we have an HMM <span class="math"><math><mrow is="true"><mi is="true">λ</mi></mrow></math></span> and a sequence of observation <span class="math"><math><mrow is="true"><mi is="true" mathvariant="bold">O</mi></mrow></math></span>. Evaluation or scoring is the calculation of the probability of the observation sequence given the HMM, i.e., <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">λ</mi><mo stretchy="false" is="true">)</mo></mrow></math></span>.
 Decoding is the process of finding the best state sequence that can 
give rise to the observation sequence. Finally, training is the 
adjusting of model parameters to maximize the probability of generating a
 given training observation sequence. The algorithms that cope with 
evaluation, decoding and training problems are called the forward, 
Viterbi and Baum–Welch algorithms, respectively (see <a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner (1990)</a> for details).</p><p id="p0245">In order to find the likelihood term of Eq. <a name="be0035" href="#e0035" class="workspace-trigger">(7)</a> we need to solve the evaluation problem for <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub></mrow></math></span>, which is the HMM trained to the task <span class="math"><math><mrow is="true"><mi is="true">θ</mi></mrow></math></span> using the Baum–Welch algorithm on the training database of task-dependent eye trajectories. The method used in <a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner (1990)</a> to calculate the term <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math></span> is an iterative method based on dynamic programming called <em>forward algorithm</em>. In this method we define <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">α</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">=</mo><mi is="true">i</mi><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math></span> as the probability of observations <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></math></span> to <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub></mrow></math></span> with state sequence terminating in state <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math></span>, given HMM <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub></mrow></math></span>. We can, then, estimate the probability <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math></span> by iterating over the following steps until the termination criterion is met:<dl class="list"><dt class="list-label">•</dt><dd class="list-description"><p id="p0545"><em>Initialization</em>: <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">α</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">π</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">i</mi><mo is="true">⩽</mo><mi is="true">N</mi></mrow></math></span>,</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0550"><em>Induction</em>: <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">α</mi></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mo stretchy="false" is="true">[</mo><msubsup is="true"><mrow is="true"><mi is="true">Σ</mi></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">N</mi></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">α</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><mo stretchy="false" is="true">]</mo><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">O</mi></mrow><mrow is="true"><mo is="true">→</mo></mrow></mover></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">t</mi><mo is="true">⩽</mo><mi is="true">T</mi><mo is="true">-</mo><mn is="true">1</mn><mspace width="1em" is="true"></mspace><mi is="true" mathvariant="italic">and</mi><mspace width="1em" is="true"></mspace><mn is="true">1</mn><mo is="true">⩽</mo><mi is="true">j</mi><mo is="true">⩽</mo><mi is="true">N</mi></mrow></math></span>,</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0555"><em>Termination</em>: <span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><msubsup is="true"><mrow is="true"><mi is="true">Σ</mi></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">N</mi></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">α</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo></mrow></math></span>.</p></dd></dl></p><p id="p0250">With <em>T</em> observations and <em>N</em> states, we require approximately <span class="math"><math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi is="true">T</mi></mrow></math></span> operations.</p></section><section id="s0030"><h2 id="st030" class="u-h3 u-margin-l-top u-margin-xs-bottom">3. State positioning of HMMs using <em>K</em>-means clustering</h2><p id="p0255">So far we have explained how we can use the parameters of a task-dependent HMM <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub></mrow></math></span> to infer the underlying task of an eye trajectory <span class="math"><math><mrow is="true"><mi is="true" mathvariant="bold">O</mi></mrow></math></span>. However, the training to obtain the parameters of <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true">θ</mi></mrow></msub></mrow></math></span> still remains to be explained. In order to train task-dependent HMMs, we first need to come up with a design for a <em>generic HMM</em>. We can then use task-dependent eye trajectories to train the generic HMM by using the Baum–Welch method to make <em>task-specific</em> HMMs.</p><p id="p0260">As
 we explained before, in HMMs the states are hidden and only the 
observations are overt to the viewer. Therefore, in application to the 
problem of attention tracking, we used the states to represent the FOA 
and used the coordinates of the COG as the observations. In our model, 
each state is composed of a 2D Gaussian observation pdf that is centered
 on a target. <a name="bf0025" href="#f0025" class="workspace-trigger">Fig. 5</a>b
 shows an example of an HMM trained for the task of “counting the number
 of faces in the image”. As we can see, targets learned for this task 
roughly correspond to the faces in the image. The positioning of the 
Gaussian pdfs in a synthetic image with discrete targets, such as the 
one shown in <a name="bf0030" href="#f0030" class="workspace-trigger">Fig. 6</a>a,
 is also straightforward as we can assign a state to each of the targets
 in the image and remove the task-irrelevant ones during the training.</p><p id="p0265">However, positioning of the states’ observation pdfs is not always trivial. When executing tasks, such as the ones used in <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012)</a>
 (e.g., “Memorizing the picture” or “determining the wealth of the 
people in the picture”), on natural images, predefining the attentional 
targets in the generic HMM needs to be done manually and requires 
knowledge about the relevance of the objects in the image to the task.</p><p id="p0270">In
 order to automatically position the observation pdfs of the generic HMM
 on task-relevant objects, we use a clustering technique to locate the 
“hot spots” that are informative for execution of the task. To do so, we
 propose to use <em>K</em>-means clustering (<a name="bb0290" href="#b0290" class="workspace-trigger">Kaufman &amp; Rousseeuw, 2009</a>)
 on the ensemble of the fixations of the training set. Since the 
training set comprises all the fixations of the subjects performing a 
specific task, the ensemble reveals the potential attention demanding 
targets in the image for that task.</p><div><p id="p0275"><a name="bf0035" href="#f0035" class="workspace-trigger">Fig. 7</a>b
 and c shows the gaze opacity maps of a training set of eye movements 
recorded while performing the task of “determining how well the people 
in the picture know each other (people)” and “determining the wealth of 
the people in the picture (wealth)” on the image of <a name="bf0035" href="#f0035" class="workspace-trigger">Fig. 7</a>a.
 The gaze opacity map is obtained by applying a mask overlaying the 
image. The opacity of this mask at a given point in the image is 
inversely proportional to the number of fixations in a region about this
 point. In these maps the areas with a large number of fixations are 
shown clearly, whereas the areas with no, or few, fixations are masked. 
As can be seen, the areas near the faces get more fixations in the <em>people</em>
 task and the areas around objects such as the telephone, tie, pipe and 
the objects on the desk, are more likely to get fixated in the <em>wealth</em> task.</p><figure class="figure text-xs" id="f0035"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr7.jpg" alt="" aria-describedby="cn035" height="231"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr7_lrg.jpg" target="_blank" download="" title="Download high-res image (174KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (174KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr7.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn035"><p id="sp045"><span class="label">Fig. 7</span>. 
Compilation of the fixation spots during two visual-tasks in the form of
 opacity maps. (a) The original image on which the tasks were executed. 
(b) The gaze opacity for the task of “determining how well the people in
 the picture know each other (people)”. (c) The opacity map for the task
 of “determining the wealth of the people in the picture (wealth)”.</p></span></span></figure></div><p id="p0280">By
 using this simple technique we can get a sense of the conspicuous 
locations for different tasks with a computational complexity of <span class="math"><math><mrow is="true"><mi is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math></span> <a name="bb0550" href="#b0550" class="workspace-trigger">Xu and Wunsch (2005)</a>. The <em>K</em>-means clustering will provide us with <em>K</em> points that indicate the centroids of the top <em>K</em>
 fixated areas in the training set. In the generic HMM, we will use 
these centroids as the initial means of the observation pdfs of <em>K</em>
 states. This initial placement of the 2D Gaussians of the generic HMM 
on the image, however, is only an estimate of their eventual positions, 
which may change during the Baum–Welch training.</p></section><section id="s0035"><h2 id="st035" class="u-h3 u-margin-l-top u-margin-xs-bottom">4. Experiment</h2><p id="p0285">To
 validate our HMM-based approach, we carried out an experiment in which 
human observers carried out abstract tasks while viewing photographs of 
complex natural scenes. In order to benchmark our results against those 
of <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012)</a>,
 we used the same database of natural images as they used in their 
experiment. The image set comprises 64 gray-scale photographs taken from
 the LIFE magazine photo archive hosted by <a name="bb0180" href="#b0180" class="workspace-trigger">Google and photo archive hosted by Google (2013)</a>, an example of which is shown in <a name="bf0035" href="#f0035" class="workspace-trigger">Fig. 7</a>a.
 The date of the images span the years between 1930 and 1979. In each 
image there are at least two people, and the images do not display faces
 or locations that were familiar to our test subjects.</p><p id="p0290">For
 the sake of comparison of the results, in building a database of 
task-dependent eye trajectories, we followed the same procedure as in <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012)</a>.
 In total, we ran 1280 trials and recorded the eye movements of five 
subjects while performing a set of pre-defined visual-tasks. Five 
graduate students (one female and four males), aged between 18 and 30, 
with normal or corrected-to-normal vision volunteered to participate in 
this experiment. We used the same four tasks as in the Greene et al. 
experiment:<dl class="list"><dt class="list-label">•</dt><dd class="list-description"><p id="p0560">Memorize the picture (<em>memory</em>).</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0565">Determine the decade in which the picture was taken (<em>decade</em>).</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0570">Determine how well the people in the picture know each other (<em>people</em>).</p></dd><dt class="list-label">•</dt><dd class="list-description"><p id="p0575">Determine the wealth of the people in the picture (<em>wealth</em>).</p></dd></dl></p><p id="p0295">The images were displayed on a <span class="math"><math><mrow is="true"><mn is="true">1920</mn><mo is="true">×</mo><mn is="true">1080</mn></mrow></math></span> pixel LCD monitor with a screen size of <span class="math"><math><mrow is="true"><mn is="true">53.3</mn><mo is="true">×</mo><mn is="true">30</mn><mspace width="0.25em" is="true"></mspace><mtext is="true">cm</mtext></mrow></math></span>. The viewing distance was 45&nbsp;cm. Each image had a resolution of <span class="math"><math><mrow is="true"><mn is="true">800</mn><mo is="true">×</mo><mn is="true">800</mn></mrow></math></span> pixels, which subtended 28 degrees of visual angle. The background pixels were all set to black.</p><p id="p0300">Each
 subject did four segments of trials during his/her experiment. Each 
segment consisted of four blocks of 16 images. The subject was informed 
of the task by an instruction image at the beginning of each block. 
During each segment, each of the 64 images were displayed once and 
subjects had 10&nbsp;s to view each image. In order to better engage the
 subjects in the tasks, after each image in the “decade”, “people” and 
“wealth” blocks, a question in form of a five-alternative-forced-choice 
was presented to the subject. The subjects were asked to select the best
 answer by clicking on one of the five choices. (We used the same 
routine and questions as in the Greene et al. experiment.)</p><p id="p0305">After
 each segment, a mandatory rest period was assigned to the subject, 
followed by the next segment of 64 images. In each segment we rotated 
the task order so that each subject performs all the tasks on all the 
images. In the end, we obtained five trajectories per task, per image, 
from which we selected the test and training set using leave-one-out 
(LOO) cross-validation.</p><p id="p0310">A Tobii X120 eye tracker was used to record the participants’ eye positions, running at an acquisition rate of 120&nbsp;Hz. The eye tracker’s spatial resolution is approximately 0.2° and its accuracy following calibration is about <span class="math"><math><mrow is="true"><msup is="true"><mrow is="true"><mn is="true">0.5</mn></mrow><mrow is="true"><mi is="true">o</mi></mrow></msup></mrow></math></span>. The subjects used both eyes when conducting the experiments.</p><p id="p0315">At
 the beginning of each segment, we calibrated the eye tracker using the 
built-in, five-point, changing diameter, moving dot calibration routine 
in Tobii Studio software (ver. 3.2.0, Tobii Technology, Stockholm, 
Sweden). The calibration grid spanned the entire display.</p><p id="p0320">After
 the recording of the eye movements, data analysis was carried out on 
each trial, wherein we removed the blinks and outliers from the data and
 classified the eye movement data as either saccades or fixations using 
the velocity-threshold identification (I-VT) method provided in the 
Tobii Studio software. The outliers were all fixations that appeared to 
be out of the screen area, which might have been caused by errors in the
 interpolation or real fixations at points outside of the screen 
boundary. It is generally agreed that visual and cognitive processing 
primarily occurs during fixations and little or no visual processing can
 be achieved during a saccade (<a name="bb0165" href="#b0165" class="workspace-trigger">Fuchs, 1971</a>). Therefore, in our analysis we only considered the fixation points.</p><section id="s0040"><h3 id="st040" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.1. Methods</h3><p id="p0325">In
 this experiment we use the proposed HMM-based model to infer the 
visual-task. The inference is made by applying Bayes rule (Eq. <a name="be0020" href="#e0020" class="workspace-trigger">(4)</a>)
 to the likelihood term calculated by the forward algorithm. A uniform 
distribution is used for the a priori task probabilities, which makes 
the inference a maximum likelihood estimation of the task. However, in 
practical applications we typically would have some prior information 
about the tasks, which can be applied to the a priori term and increase 
the accuracy of the inference.</p><p id="p0330">In order to obtain the likelihood term (<span class="math"><math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">O</mi><mo stretchy="false" is="true">|</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo></mrow></math></span>), we need to train the parameters of an HMM for each task (<span class="math"><math><mrow is="true"><mi is="true">θ</mi></mrow></math></span>)
 by using the training eye movements of the corresponding task. To do 
so, first we need to define the structure of the generic HMM and then 
customize it by training it with eye movements of that task. For the 
generic HMM we assign an ergodic, or fully connected, structure wherein 
we can go to any state of the model in a single step no matter what the 
current state of the model.<a name="bfn2" href="#fn2" class="workspace-trigger"><sup>2</sup></a> This is consistent with the characteristics of eye movement, where we can also move our COG to any target in a given stimulus.</p><div><p id="p0335">As explained in Section <a name="bs0030" href="#s0030" class="workspace-trigger">3</a>, we use <em>K</em>-means
 clustering to define the initial locations (means) of the observation 
pdfs in the generic HMM. For each task-image pair, we examine different 
values for the number of clusters ranging from <span class="math"><math><mrow is="true"><mi is="true">K</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math></span> to 10 and use the value that gives the maximum a-posterior probability (MAP) to the training data, i.e.:<span class="display"><span id="e0045" class="formula"><span class="label">(9)</span><span class="math"><math><mi is="true">K</mi><mo is="true">=</mo><mi is="true" mathvariant="normal">arg</mi><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mi is="true" mathvariant="normal">max</mi></mrow><mrow is="true"><mi is="true">K</mi><mo is="true">=</mo><mn is="true">2</mn><mo is="true">:</mo><mn is="true">10</mn></mrow></munder></mstyle><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mo stretchy="false" is="true">〈</mo><mi is="true" mathvariant="bold">O</mi><mtext is="true">,</mtext><mi is="true">θ</mi><msub is="true"><mrow is="true"><mo stretchy="false" is="true">〉</mo></mrow><mrow is="true"><mi is="true" mathvariant="italic">training</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true">N</mi><mo is="true">=</mo><mi is="true">K</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext></math></span></span></span>where <em>N</em>
 is the number of states. If we use a very small number of clusters, the
 HMM will not be able to capture the transition patterns between the 
objects and will be less task-dependent. On the other hand if we assign a
 large number to <em>K</em>, the training algorithm will diverge and 
will not find a feature set that maximizes the likelihood of the 
training set. We expect that the value of <em>K</em> will be highly dependent on the number of task relevant targets in an image. For instance, for the people task model (<span class="math"><math><mrow is="true"><mi is="true">θ</mi><mo is="true">=</mo><mi is="true" mathvariant="italic">people</mi></mrow></math></span>) of the image in <a name="bf0040" href="#f0040" class="workspace-trigger">Fig. 8</a>, where we have six faces, <span class="math"><math><mrow is="true"><mi is="true">K</mi><mo is="true">=</mo><mn is="true">6</mn></mrow></math></span> gives us the best result, suggesting that a 6-state HMM would be the best choice for <span class="math"><math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">people</mi></mrow></msub></mrow></math></span> of the image.</p><figure class="figure text-xs" id="f0040"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr8.jpg" alt="" aria-describedby="cn040" height="247"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr8_lrg.jpg" target="_blank" download="" title="Download high-res image (242KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (242KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr8.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn040"><p id="sp050"><span class="label">Fig. 8</span>. 
In this figure it is shown how an HMM is trained for the task of 
“determining how well the people in the picture know each other 
(people)” on a given picture. To begin the training we first need to 
define the main structure of the HMM in the form of a generic HMM. The 
generic HMM is composed of six 2D Gaussian pdfs centered on the 
centroids of the <em>K</em>-means clustering conducted on the training 
set. The standard deviation used in the covariance matrix of each 
Gaussian is set to 126 pixels and a uniform transition matrix is used 
for governing the transitions between the states (each Gaussian 
represents a state). Part (a) shows the model that is used as the 
generic HMM for training the task-dependent HMM. Each Gaussian 
observation pdf is shown by a heat-map, centered on the centroids of its
 corresponding cluster. The generic HMM is used in the Baum–Welch 
algorithm to train the task-specific HMM. Part (b) shows the final, 
task-specific, model after training the generic HMM by the eye 
trajectories of the training set.</p></span></span></figure></div><p id="p0340">To define the covariance of the Gaussian distributions, we use a technique called <em>parameter tieing</em> (<a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner, 1990</a>)
 to force a unique covariance matrix across all the Gaussian 
distributions. We also fix the off-diagonal elements of the covariance 
matrix to zero, which leads to fully circular Gaussian observation 
distributions:<span class="display"><span id="e0050" class="formula"><span class="label">(10)</span><span class="math"><math><mi is="true" mathvariant="italic">COV</mi><mo stretchy="false" is="true">(</mo><mi is="true">B</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><msup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi is="true">I</mi><mo stretchy="false" is="true">(</mo><mi is="true">N</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext></math></span></span></span>where <span class="math"><math><mrow is="true"><mi is="true">I</mi><mo stretchy="false" is="true">(</mo><mi is="true">N</mi><mo stretchy="false" is="true">)</mo></mrow></math></span> is the identity matrix of size <span class="math"><math><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">N</mi></mrow></math></span>.
 These two provisions allow us to obtain convergence in training the 
HMMs with the very limited number of observations in the training 
database, since the number of parameters to train the covariance 
matrices decreases from <span class="math"><math><mrow is="true"><mn is="true">3</mn><mi is="true">K</mi></mrow></math></span>
 to 1. Moreover, a fully diagonal covariance matrix results in a 
circularly-symmetric Gaussian distribution, which is similar to the 
quasi-circular FOA of the human visual system (<a name="bb0150" href="#b0150" class="workspace-trigger">Eriksen &amp; James (1986)</a>).</p><p id="p0345">For defining the standard deviation (<span class="math"><math><mrow is="true"><mi is="true">σ</mi></mrow></math></span>)
 used in the covariance matrix we tested several values ranging from 14 
pixels (0.5°) to 210 pixels (15°) in 14 pixels steps (0.5°) and obtained
 the best result for 126 pixels (4.5°).</p><p id="p0350">As stated in <a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner (1990)</a>, a uniform distribution assumption suffices as the initial pdf of the <em>initial state distribution</em> (<span class="math"><math><mrow is="true"><mi is="true">Π</mi></mrow></math></span>) and <em>the state transition probability distribution</em> (<em>A</em>).</p><p id="p0355">Having
 defined the structure of the generic HMM, we can obtain a 
task-dependent HMM by training it with task-specific eye trajectories by
 using the expectation maximization-based (EM-based) algorithm of 
Baum–Welch <a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner (1990)</a>.</p><p id="p0360"><a name="bf0040" href="#f0040" class="workspace-trigger">Fig. 8</a>a shows the generic HMM for the task of <em>people</em>, superimposed on the original image. The standard deviation of the observation distribution is set to 126 pixels and <span class="math"><math><mrow is="true"><mi is="true">K</mi><mo is="true">=</mo><mn is="true">6</mn></mrow></math></span> centroids are used for clustering. The result of training the generic HMM to the task-specific trajectories of the <em>people</em> task is shown in <a name="bf0040" href="#f0040" class="workspace-trigger">Fig. 8</a>b.
 As we can see, the states (pdf means) move from their positions in the 
generic HMM to be compatible with the observations in the training set.</p></section><section id="s0045"><h3 id="st045" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.2. Results</h3><p id="p0365">In Section <a name="bs0040" href="#s0040" class="workspace-trigger">4.1</a> we remarked that the best value for the standard deviation is <span class="math"><math><mrow is="true"><mi is="true">σ</mi><mo is="true">=</mo><mn is="true">4.5</mn><mi is="true">°</mi></mrow></math></span>.
 This value is the standard deviation used in the covariance matrix 
which defines the area covered by each of the 2D Gaussian observation 
pdfs. In other words, this value shows the best scale for the diameters 
of the Gaussians shown in <a name="bf0040" href="#f0040" class="workspace-trigger">Fig. 8</a>.
 Increasing this value will expand the overlapped area between different
 observation pdfs, which in turn relaxes the overtness constraint of the
 attentional spot. However, too large values of the standard deviation 
causes too much overlap in the observation pdfs, which flattens the 
likelihood distribution function of Eq. <a name="be0025" href="#e0025" class="workspace-trigger">(5)</a>.</p><div><p id="p0370">As mentioned in the introduction, the confusion matrix given in the study by <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012)</a> indicated that task inference was at the chance level (25%). <a name="bf0045" href="#f0045" class="workspace-trigger">Fig. 9</a>b shows the confusion matrix obtained using our HMM model. The numerical values of the confusion matrix are shown in <a name="bt0005" href="#t0005" class="workspace-trigger">Table 1</a>.
 The diagonal elements of the confusion matrix show the percentage of 
trajectories whose task labels were correctly classified (<em>hits</em>) and the off-diagonal elements comprise the <em>misses</em> in the classification.</p><figure class="figure text-xs" id="f0045"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr9.jpg" alt="" aria-describedby="cn045" height="206"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr9_lrg.jpg" target="_blank" download="" title="Download high-res image (197KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (197KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-gr9.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cn045"><p id="sp055"><span class="label">Fig. 9</span>. 
(a) Accuracy of task classification versus standard deviation (STD) of 
the Gaussian observations. The accuracy is obtained by averaging the 
diagonal elements of the confusion matrix of all 64 images and the error
 bars show the standard error of the mean (SEM). The table at the bottom
 of the figure shows the values of the means and SEMs. (b) Confusion 
matrix of task inference using the HMM-based model.</p></span></span></figure><div class="tables frame-topbot rowsep-0 colsep-0" id="t0005"><span class="captions"><span id="cn050"><p id="sp060"><span class="label">Table 1</span>. Numerical values of the confusion matrix for task classification using the HMM-based model. To obtain the results we set <span class="math"><math><mrow is="true"><mi is="true">σ</mi><mo is="true">=</mo><msup is="true"><mrow is="true"><mn is="true">4.5</mn></mrow><mrow is="true"><mi is="true">o</mi></mrow></msup></mrow></math></span> and did LOO cross validation over all task dependent eye trajectories.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left"></th><th scope="col" class="align-char"><span class="monospace"><strong>MEMORY</strong></span></th><th scope="col" class="align-char"><span class="monospace"><strong>DECADE</strong></span></th><th scope="col" class="align-char"><span class="monospace"><strong>PEOPLE</strong></span></th><th scope="col" class="align-char"><span class="monospace"><strong>WEALTH</strong></span></th></tr></thead><tbody><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>MEMORY</strong></span></td><td class="align-char"><strong>59.35</strong></td><td class="align-char">13.76</td><td class="align-char">12.98</td><td class="align-char">13.91</td></tr><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>DECADE</strong></span></td><td class="align-char">11.86</td><td class="align-char"><strong>55.91</strong></td><td class="align-char">18.84</td><td class="align-char">13.39</td></tr><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>PEOPLE</strong></span></td><td class="align-char">12.56</td><td class="align-char">11.57</td><td class="align-char"><strong>65.84</strong></td><td class="align-char">10.03</td></tr><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>WEALTH</strong></span></td><td class="align-char">15.44</td><td class="align-char">11.64</td><td class="align-char">15.46</td><td class="align-char"><strong>57.46</strong></td></tr></tbody></table></div><p class="legend"><p id="sp9000">Bold numbers show the maximum number in each row.</p></p></div></div><p id="p0375"><a name="bf0045" href="#f0045" class="workspace-trigger">Fig. 9</a>a
 shows the accuracy of task classification versus standard deviation 
(STD) of the Gaussian observations. The accuracy is obtained by 
averaging the diagonal elements of the confusion matrix and the error 
bars show the standard error of the mean (SEM). The table at the bottom 
of the figure shows the values of the means and the SEMs. In the 
experiment we use a leave-one-out cross-validation to define the 
training set and use the average accuracy across all images to represent
 the overall accuracy. The SEMs are the sample estimate of the 
population standard deviation of the accuracies across all images 
divided by the square root of the number of images.</p><p id="p0380">The diagonal values of the confusion matrix in <a name="bf0045" href="#f0045" class="workspace-trigger">Fig. 9</a>b
 are well above the chance level. The model is able to infer the 
visual-task with average accuracy of 59.64%, as given by averaging the 
diagonal elements of the confusion matrix.</p><div><p id="p0385">In 
order to show the advantage of HMMs over DTMCs, we used the same 
database and did the task inference using DTMCs. To do so, we used the 
same set up as in the HMMs (using <em>K</em>-Means clustering), but 
rather than setting an observation pdf to each state, we used Euclidean 
nearest-neighbors to select the current state of a fixation. This is 
equivalent to assuming that covert attention is the same as the overt 
attention (i.e. that attention is allocated to the same location as the 
eye fixation). The confusion matrix obtained when using DTMC has an 
average accuracy of 31.54% and is shown in <a name="bt0010" href="#t0010" class="workspace-trigger">Table 2</a>.
 Comparing the results of HMM and DTMC highlights the importance of 
allowing for off-target fixations in our model for inferring the task in
 real images.</p><div class="tables frame-topbot rowsep-0 colsep-0" id="t0010"><span class="captions"><span id="cn055"><p id="sp065"><span class="label">Table 2</span>. 
Numerical values of the confusion matrix for task classification using 
the DTMC-based model. To obtain the results we used the same setup 
(number of clusters) as in the HMMs and did LOO cross validation over 
all task dependent eye trajectories. In order to define the presumably 
overt state, we set the state to the closest state using the Euclidean 
nearest neighbor.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left"></th><th scope="col" class="align-char"><span class="monospace"><strong>MEMORY</strong></span></th><th scope="col" class="align-char"><span class="monospace"><strong>DECADE</strong></span></th><th scope="col" class="align-char"><span class="monospace"><strong>PEOPLE</strong></span></th><th scope="col" class="align-char"><span class="monospace"><strong>WEALTH</strong></span></th></tr></thead><tbody><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>MEMORY</strong></span></td><td class="align-char"><strong>23.54</strong></td><td class="align-char">28.64</td><td class="align-char">32.57</td><td class="align-char">15.25</td></tr><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>DECADE</strong></span></td><td class="align-char">13.43</td><td class="align-char"><strong>27.68</strong></td><td class="align-char">21.64</td><td class="align-char">37.25</td></tr><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>PEOPLE</strong></span></td><td class="align-char">10.63</td><td class="align-char">28.47</td><td class="align-char"><strong>45.29</strong></td><td class="align-char">15.61</td></tr><tr class="valign-top"><td class="align-left"><span class="monospace"><strong>WEALTH</strong></span></td><td class="align-char">24.74</td><td class="align-char">16.47</td><td class="align-char">29.14</td><td class="align-char"><strong>29.65</strong></td></tr></tbody></table></div><p class="legend"><p id="sp9005">Bold numbers show the maximum number in each row.</p></p></div></div></section></section><section id="s0050"><h2 id="st050" class="u-h3 u-margin-l-top u-margin-xs-bottom">5. Conclusion</h2><p id="p0390">In
 this article we presented a probabilistic framework for task inference 
in natural images. This work was motivated in part by previously 
reported difficulties in developing a reliable approach for implementing
 an inverse Yarbus process. In particular, we examined the study of <a name="bb0185" href="#b0185" class="workspace-trigger">Greene et al., 2011</a>, <a name="bb0190" href="#b0190" class="workspace-trigger">Greene et al., 2012</a>,
 who concluded that visual-task cannot be inferred using eye movements, 
and tried to understand why their approach was not successful. We 
hypothesized that the difficulty lay in the lack of explanatory power of
 the summary statistics that were used, such as the number of fixations,
 and duration of fixations, that were used to classify the trajectories.
 These features, however, have been shown (e.g., <a name="bb0075" href="#b0075" class="workspace-trigger">Castelhano &amp; Henderson (2008)</a>) to be unreliable in task inference.</p><p id="p0395">Another
 reason for the failure of the aggregate-based method in inferring the 
task is that no contextual information about the image is used in 
classification. This is in spite of the fact that image context has been
 shown to have a major effect on eye movement behavior (<a name="bb0505" href="#b0505" class="workspace-trigger">Torralba et al., 2006</a>; <a name="bb0175" href="#b0175" class="workspace-trigger">Goferman, Zelnik-Manor, and Tal, 2012</a>).</p><p id="p0400">To
 handle these problems we used features that are more informative than 
summary statistics, and provided a way to incorporate local (contextual)
 information. To validate our approach in relation to the results of the
 <a name="bb0185" href="#b0185" class="workspace-trigger">Greene et al., 2011</a>, <a name="bb0190" href="#b0190" class="workspace-trigger">Greene et al., 2012</a>, experiments, we used the same database of natural images and the same experimental protocol.</p><p id="p0405">One could argue that the negative results in the <a name="bb0185" href="#b0185" class="workspace-trigger">Greene et al., 2011</a>, <a name="bb0190" href="#b0190" class="workspace-trigger">Greene et al., 2012</a>,
 experiments imply that it is not possible to perform the inverse Yarbus
 process. However, there is evidence that such a process is in fact 
possible, provided by work on predicting the cognitive state of an 
observer from eye movements. Eye movement measurements have been used in
 the recognition of physical activity <a name="bb0050" href="#b0050" class="workspace-trigger">Bulling et al. (2009)</a>, <a name="bb0055" href="#b0055" class="workspace-trigger">Bulling et al. (2011)</a>, detection of tiredness or distraction (<a name="bb0115" href="#b0115" class="workspace-trigger">Di Stasi et al., 2012</a>), estimating mental workload levels (<a name="bb0120" href="#b0120" class="workspace-trigger">Di Stasi et al., 2010</a>), diagnosis of schizophrenia (<a name="bb0025" href="#b0025" class="workspace-trigger">Benson et al., 2012</a>) and detection of mental fatigue (<a name="bb0455" href="#b0455" class="workspace-trigger">Schleicher et al., 2008</a>).
 As the cognitive state of a viewer carrying out a visual task is 
presumably affected by the nature of the task it is reasonable to expect
 that viewer task can likewise be detected from eye movement 
measurements. The results of applying our technique support this 
conclusion.</p><p id="p0410">Our approach is based on the idea that 
visual task is revealed by the spatio-temporal patterns of the 
allocation of visual attention. In practice, attention has most often 
been tracked using eye movements, and models of attention are frequently
 evaluated based on how well they can predict eye trajectories. However,
 classical salience-based models of eye movement generation exhibit 
limited performance in accounting for eye movements in real-world 
situations of viewing complex natural scenes. This is due, in part, to 
their pure bottom-up dependence on low-level image features. In such 
situations single human observers outperform even the best 
salience-based models in predicting eye trajectories (<a name="bb0275" href="#b0275" class="workspace-trigger">Judd, Durand, and Torralba (2012)</a>). Low-level features often have low salience in areas near fixations (<a name="bb0005" href="#b0005" class="workspace-trigger">Ballard &amp; Hayhoe, 2009</a>; <a name="bb0230" href="#b0230" class="workspace-trigger">Hayhoe et al., 2003</a>; <a name="bb0330" href="#b0330" class="workspace-trigger">Land &amp; McLeod, 2000</a>; <a name="bb0335" href="#b0335" class="workspace-trigger">Land, Mennie, &amp; Rusted, 1999</a>).
 Therefore we developed our model based on real, task-dependent eye 
trajectories recorded while viewing natural images. To go beyond the 
simple salience-based approaches we used Hidden Markov models (HMMs) as a
 tool for time-series analysis of the eye trajectories. This allows us 
to encode the dynamics of natural eye movements into task-dependent 
models.</p><p id="p0415">The HMM-based method not only allows us to 
track overt foci of attention (i.e. fixation locations), but also allows
 for the tracking of covert attention and other sources of discrepancy 
between the center of gaze (COG) and the focus of attention (FOA). A 
deviation between the COG and FOA can arise by a variety of mechanisms. 
For example, in the phenomenon known as <em>the center-of-gravity</em> (also known as the <em>global effect</em>) (<a name="bb0560" href="#b0560" class="workspace-trigger">Zelinsky et al., 1997</a>; <a name="bb0240" href="#b0240" class="workspace-trigger">He and Kowler, 1989</a>, <a name="bb0370" href="#b0370" class="workspace-trigger">Najemnik and Geisler, 2005</a>),
 the target of the eye movement is actually the center-of-mass of a set 
of visually-salient objects, one of which would correspond to the FOA. 
The resulting COG at the center-of-mass location would generally not 
correspond to a location of high salience. As <a name="bb0095" href="#b0095" class="workspace-trigger">Coëffé and O’regan (1987)</a>
 point out, the global effect is less pronounced when saccadic latencies
 are long, as is typically the case when visual search is being carried 
out in a slow, deliberate manner. But when a task is being done quickly,
 then significant deviations between the COG and FOA can rise. The 
advantage of decoupling the COG and the FOA becomes clear by comparing 
the results of our Discrete-Time-Markov-Chain (DTMC) and HMM models, 
since the only difference between these two models is the linkage 
between the FOA and the COG. The HMMs allow for decoupling the COG and 
the FOA by means of the state-specific Gaussian distribution functions, 
whereas in the DTMC they are assumed to be the same. The Gaussian 
distribution functions used in the model definition of the HMMs span an 
area around the covert attentional loci. The actual eye fixation points 
are then considered as a random outcome of the Gaussian process, which 
can be result in locations well away from the covert attention locus. 
The experimental results show that by separating the COG and FOA leads 
to better performance in inferring viewer task.</p><p id="p0420">The 
improvement in performance in inferring visual task with the HMM 
approach as compared to the DTMC approach also provides indirect 
experimental evidence for separation between the COG and the FOA in 
real-world picture viewing tasks. This indication of the separation of 
covert and overt attention is an important by-product of our approach, 
since it is not easy to demonstrate such separations in scene viewing 
eye movement recordings. The possibility of this dissociation between 
the COG and FOA has been raised before in oculomotor studies by 
indirectly tracing attentional spot on non-fixated targets. In a study 
by <a name="bb0390" href="#b0390" class="workspace-trigger">O’Regan et al. (2000)</a>
 COG-FOA decoupling is implied from observers’ lack of awareness of 
changes in an image 40% of the time, even though they were directly 
fixating the change location. Declines in reaction times to attentional 
probes away from fixation (<a name="bb0250" href="#b0250" class="workspace-trigger">Hoffman &amp; Subramaniam, 1995</a>; <a name="bb0315" href="#b0315" class="workspace-trigger">Kowler et al., 1995</a>; <a name="bb0110" href="#b0110" class="workspace-trigger">Deubel and Schneider, 1996</a>, <a name="bb0465" href="#b0465" class="workspace-trigger">Schneider and Deubel, 2002</a>),
 have also been used to indicate allocation of covert attention away 
from fixation. To avoid the problems that arise from using measurements 
of the COG to track the FOA, other more direct techniques for tracking 
covert attention could be incorporated in our model. These include 
techniques such as the dot-probe task (<a name="bb0345" href="#b0345" class="workspace-trigger">MacLeod, Mathews, &amp; Tata, 1986</a>), detecting microsaccades (<a name="bb0205" href="#b0205" class="workspace-trigger">Hafed &amp; Clark, 2002</a>), or fMRI recording (<a name="bb0540" href="#b0540" class="workspace-trigger">Wojciulik, Kanwisher, &amp; Driver, 1998</a>).
 However, these methods can interfere with the on-going task and provide
 limited spatial and temporal resolution. Thus they are not appropriate 
for practical applications of the inverse Yarbus process.</p><p id="p0425">Time-series
 analysis incorporates temporal information about fixations as well as 
spatial into an attention model. In previous approaches to analyzing eye
 movement behavior, usually only spatial information is considered and 
temporal information of fixations is simply omitted. For example, the <a name="bb0185" href="#b0185" class="workspace-trigger">Greene et al., 2011</a>, <a name="bb0190" href="#b0190" class="workspace-trigger">Greene et al., 2012</a>,
 studies ignored the temporal aspect of the viewer eye movement 
trajectories. However, it is becoming increasingly clear that temporal 
analysis of eye movement is as important as its spatial counterpart in 
describing the underlying mechanisms. The temporal order of fixations is
 an important feature in describing the underlying mechanism of the 
visual behavior. The question of whether and how the temporal order of 
fixations matters in modeling eye movements has been considered often, 
beginning with the pioneering studies of <a name="bb0065" href="#b0065" class="workspace-trigger">Buswell, 1935</a>, <a name="bb0555" href="#b0555" class="workspace-trigger">Yarbus, 1967</a>. In salience-based models of attention the temporal order of fixations is not usually considered in training the models (<a name="bb0030" href="#b0030" class="workspace-trigger">Borji &amp; Itti, 2013, Figure 7</a>). From a statistical point of view, these models postulate a <em>na</em>ï<em>ve Bayes assumption</em> in evaluating the likelihood probability of Eq. <a name="be0025" href="#e0025" class="workspace-trigger">(5)</a>,
 which assumes independence between consecutive fixation locations. In 
contrast to these models, consecutive fixations have been shown to be 
highly dependent on each other. <a name="bb0200" href="#b0200" class="workspace-trigger">Hacisalihzade, Stark, and Allen (1992)</a>
 recorded the eye movements of observers during the task of recognizing 
an object and showed that the fixations loosely follow a Markov process.
 They showed that the eyes visit the features of an object cyclically, 
following regular scanpaths rather than moving randomly. <a name="bb0140" href="#b0140" class="workspace-trigger">Elhelw et al. (2008)</a> used a first-order, discrete-time, discrete-state-space Markov chain to model eye movement dynamics. <a name="bb0480" href="#b0480" class="workspace-trigger">Stark and Ellis (1981)</a> also came up with a Markov process as a general model of fixation placement during the task of reading. <a name="bb0415" href="#b0415" class="workspace-trigger">Pieters, Rosbergen, and Wedel (1999)</a>
 observed a similar pattern in the scanpaths of the observers while 
looking at printed advertisements. There is more information in the 
time-series of eye positions than just the ordering of fixation 
locations. Evidence from studies done with viewers carrying out natural 
real-world tasks emphasizes the need to consider fixation duration as 
well as fixation location in understanding the mechanism of the visual 
system (<a name="bb0125" href="#b0125" class="workspace-trigger">Droll et al., 2005</a>; <a name="bb0225" href="#b0225" class="workspace-trigger">Hayhoe, Bensinger, &amp; Ballard, 1998</a>; <a name="bb0335" href="#b0335" class="workspace-trigger">Land, Mennie, &amp; Rusted, 1999</a>).</p><p id="p0430">The
 HMM approach that we propose in this paper conveniently incorporates 
the temporal aspects of attention through its Markov modeling. The 
temporal order of the fixations plays an important role in decoding the 
pattern of eye movements in the HMMs. The transition matrix of the HMMs (<em>A</em>)
 adjusts its elements according to the order of the fixations viewers 
make on targets during the training. This information is later used by 
the HMM to match the pattern of state transitions against that of a test
 trajectory. The better the transition pattern of the test trajectory 
accords with that of a task-dependent HMM, the more likely the 
trajectory is to be an observation of that task.</p><p id="p0435">It is possible to extract at least temporal order information from the eye trajectories in the original Yarbus experiment (<a name="bb0555" href="#b0555" class="workspace-trigger">Yarbus, 1967, Figs. 107–124</a>), as well as in the experiment by <a name="bb0190" href="#b0190" class="workspace-trigger">Greene, Liu, and Wolfe (2012), figure 3</a>.
 So, the machine learning method employed in the Greene et al. study 
could have used temporal features as well as the summary spatial 
statistics. It is possible that the human classifiers did 
(unconsciously) assume some sort of temporal order by tracing along the 
lines of the displayed eye tracks. The Greene et al. study therefore 
leaves open the question of whether temporal information can improve the
 task inference. To judge the influence of the temporal information on 
the ability to infer task, we created a constrained HMM method which 
lacked any temporal information. We removed the temporal information of 
the fixations from the trained HMMs by setting the transition matrix to 
equal values. In this way no knowledge of the temporal order of 
fixations that may be in the training set is incorporated into the HMM. 
Throwing away the temporal information in this manner resulted in a <span class="math"><math><mrow is="true"><mn is="true">15.51</mn><mo is="true">%</mo></mrow></math></span>
 average degradation on the diagonal elements of the confusion matrix. 
This is a significant drop in performance but it should be noted that 
the performance is still above chance, showing that the decoupling of 
the FOA and COG results in some improvement over the summary statistics.
 The HMMs fail completely in inferring the task when spatial information
 from the eye trajectories is also removed. Thus, we conclude that both 
spatial and temporal information is crucial in solving the inverse 
Yarbus problem, and the lack of such information may be one reason that 
the Greene et al. approach did not work.</p><p id="p0440">The HMM task 
inference method we proposed requires that the location of attention 
targets be known beforehand. In the past, salience was used to define 
targets for attention shifts, but in real-world viewing of complex 
scenes, with abstract tasks, defining salience is difficult. The 
specification of task-dependent salience measures is an open research 
problem, and our paper only scratches the surface of what is necessary. 
Well-performing task-dependent salient point localization schemes will 
involve high-level symbolic reasoning about the scene directed by task 
knowledge. Our approach is very simple, but was sufficient for the 
restricted problem posed by the <a name="bb0185" href="#b0185" class="workspace-trigger">Greene et al., 2011</a>, <a name="bb0190" href="#b0190" class="workspace-trigger">Greene et al., 2012</a>
 work – that of training a viewer-task classifier from a set of images 
and eye movement trajectories recorded while viewers examine these 
images under various task instructions. These training examples can be 
used to find a statistical model for the salient locations. We used the <em>K</em>-means
 clustering technique on the training set and used the centroids of 
these clusters as the salient points or potential targets. Due to the 
lack of knowledge about the task relevance of these potential targets, 
we cannot reject the possibility of next attending a given target given 
the currently fixated one. Thus, to model the temporal aspect of the eye
 movements we used an ergodic structure for a generic HMM that allows 
transitions from a state to any other one. The generic HMM undergoes a 
training phase to build attention models for each task-image 
combination, whereby we can calculate the likelihood term of Eq. <a name="be0025" href="#e0025" class="workspace-trigger">(5)</a>
 and make an inference about the task. While this approach to 
predefining attention targets worked well in this specific application, a
 more unconstrained and unsupervised problem would require a much more 
sophisticated approach to learning what the targets are. For example, if
 we applied our trained HMMs to inferring visual task for viewers 
looking at images that were not trained on, the method would fail 
miserably, since the targets will be in locations different than those 
in the training set. Some method for generalizing the location of the 
targets to different images would be needed.</p><p id="p0445">An 
interesting phenomenon seen in the training results is the variation of 
performance with different standard deviations of the observation pdf (<a name="bf0045" href="#f0045" class="workspace-trigger">Fig. 9</a>a).
 This figure shows a falloff in the task classification accuracy as the 
standard deviation moves away from a value of roughly 4° of visual 
angle. The optimal value is consistent with previous estimates of the 
size of the <em>operational fovea</em> as the central 3° of vision <a name="bb0270" href="#b0270" class="workspace-trigger">Johansson et al. (2001)</a>. <a name="bb0070" href="#b0070" class="workspace-trigger">Carpenter (1991)</a>
 shows that targets within 4° of central vision are still perceived at 
50% of maximal acuity. Based on the current evidence we cannot tell 
whether this merely a coincidence, but further experimentation could 
investigate this more deeply. Certainly, the degree of spatial 
decoupling of the FOA and COG is worth quantifying, whether this 
information is used to tune a statistical attention model such as ours, 
or to gauge the level of acuity needed for carrying out specific visual 
tasks.</p><p id="p0450">Task inference has many applications. Knowing 
what the user is seeking on a web page combined with a dynamic 
interactive design can lead to a smart web page that highlights the 
relevant information in a page according to the ongoing visual-task. The
 same idea applies to an intelligent signage that changes its contents 
to show relevant advertisements according to the task inferred from each
 viewer’s eye movements. We believe that in each of these applications 
an HMM-based model can be used as a reliable model to infer the 
visual-task. Indeed, by increasing the amount of training data and using
 prior task knowledge in the Bayesian formulation we can improve the 
accuracy of the results. Other examples of interesting applications can 
be found in the literature. <a name="bb0520" href="#b0520" class="workspace-trigger">Vidal et al. (2012)</a> implemented a pervasive healthcare system by using eye movements to infer the mental status of patients. <a name="bb0045" href="#b0045" class="workspace-trigger">Bulling, Roggen, and Troster (2011)</a>
 used eye movements to obtain information about a person’s context, and 
suggested a context-aware pervasive computing system based on the eye 
movements. As mentioned earlier, a by-product of our HMM model is that 
it can locate the focus of attention, whether it is overt or covert. 
This feature allows us to track the more informative attentional spot, 
rather than the simple motion of the gaze. Thus, in applications based 
on eye movements, performance gains might be obtained by using the 
attentional locus, which is task-oriented and robust, rather than the 
gaze information provided by standard eye trackers.</p></section><section id="s0055"><h2 id="st055" class="u-h3 u-margin-l-top u-margin-xs-bottom">6. Ethical considerations</h2><p id="p0455">All
 experiments conducted in this research were approved by the McGill 
Ethics Review Board. The Research Ethics Boards of McGill University 
adhere to and are required to follow the Canadian Institutes of Health 
Research, Natural Sciences and Engineering Research Council of Canada, 
and Social Sciences and Humanities Research Council of Canada- 
Tri-Council Policy Statement: Ethical Conduct for Research Involving 
Humans, December 2010. This policy espouses the core principles of 
Respect for Persons, Concern for Welfare and Justice, in keeping with 
leading international ethical norms, such as the Declaration of 
Helsinki.</p><section id="s0060"><h3 id="st060" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.1. Role of the funding source</h3><p id="p0460">The
 funding agencies had no involvement in the study design, collection, 
analysis and interpretation of data, writing of the report, nor in the 
decision to submit the article for publication.</p></section></section></div><section id="ak005"><h2 id="st085" class="u-h3 u-margin-l-top u-margin-xs-bottom">Acknowledgment</h2><p id="p0580">The
 authors would like to thank Jeremy Wolfe and Ben Tatler, the reviewers 
of the manuscript, for their valuable comments, which helped us 
significantly improve the paper. We would also like to thank <em>Fonds Québecois de la Recherche sur la Nature et les Technologies</em> (FQRNT) and <em>Natural Sciences and Engineering Research Council of Canada</em> (NSERC) for their support of this work.</p></section><div class="Appendices"><section id="s0070"><h2 id="st070" class="u-h3 u-margin-l-top u-margin-xs-bottom">Appendix A. Supplementary material</h2><p id="p0470"><span class="display"><span class="e-component e-component-mmc1" id="m0005"><span class="article-attachment"><a class="icon-link" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-mmc1.pdf" title="Download Acrobat PDF file (278KB)" target="_blank" rel="noreferrer noopener"><svg focusable="false" viewBox="0 0 95 128" width="17.8125" height="24" class="icon icon-pdf-download"><path d="m82 108h-7e1v-49c0-6.08 4.92-11 11-11h17v-2e1h-6c-2.2 0-4 1.8-4 4v6h-7c-3.32 0-6.44 0.78-9.22 2.16 2.46-5.62 7.28-11.86 13.5-17.1 2.34-1.98 5.3-3.06 8.32-3.06h46.4v4e1h1e1v-5e1h-56.4c-5.38 0-10.62 1.92-14.76 5.4-9.1 7.68-18.84 20.14-18.84 32.1v70.5h9e1v-18h-1e1v8zm-25.94-39.4c-0.84-0.38-1.84-0.56-2.98-0.56h-9.04v23.6h5.94v-9h2.18c2.48 0 4.36-0.62 5.66-1.84s1.92-3.06 1.92-5.5c0-1.04-0.12-2-0.4-2.88-0.26-0.88-0.66-1.64-1.22-2.3s-1.22-1.14-2.06-1.52zm-3.12 8.94c-0.4 0.46-0.98 0.7-1.74 0.7h-1.22v-5.76h1.22c1.56 0 2.34 0.96 2.34 2.88 0 0.98-0.2 1.72-0.6 2.18zm21.84-8.52c-0.96-0.66-2.32-0.98-4.06-0.98h-8.72v23.6h8.72c1.74 0 3.1-0.32 4.06-0.98s1.7-1.52 2.18-2.62 0.78-2.34 0.88-3.76 0.16-2.9 0.16-4.44-0.06-3.02-0.16-4.44-0.4-2.68-0.88-3.76c-0.48-1.1-1.2-1.96-2.18-2.62zm-2.78 14.66c-0.06 0.96-0.18 1.72-0.38 2.24s-0.48 0.88-0.84 1.04-0.86 0.24-1.48 0.24h-1.32v-14.74h1.32c0.62 0 1.12 0.08 1.48 0.24s0.64 0.52 0.84 1.04 0.32 1.28 0.38 2.24c0.06 0.98 0.08 2.24 0.08 3.84s-0.02 2.9-0.08 3.86zm13.98-6.58v-4.02h7.74v-5.04h-13.7v23.6h5.96v-9.72h7.26v-4.82z"></path></svg></a><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0042698914002004-mmc1.pdf" target="_blank" download="" title="Download Acrobat PDF file (278KB)"><span class="anchor-text">Download : <span class="download-link-title">Download Acrobat PDF file (278KB)</span></span></a></span></span></span></p></section></div></div><div class="related-content-links u-hide-from-md"><button class="button button-anchor" type="button"><span class="button-text">Recommended articles</span></button><button class="button button-anchor" type="button"><span class="button-text">Citing articles (29)</span></button></div><div class="Tail"></div><section class="bibliography u-font-serif text-s" id="bi005"><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="bs005"><dl class="references" id="reference-links-bs005"><dt class="label"><a href="#bb0005" id="ref-id-b0005" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ballard and Hayhoe, 2009</a></dt><dd class="reference" id="h0005"><div class="contribution">D. Ballard, M. Hayhoe<strong class="title">Modelling the role of task in the control of gaze</strong></div><div class="host">Visual Cognition, 17 (6-7) (2009), pp. 1185-1204</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/13506280902978477">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70449518576&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Modelling%20the%20role%20of%20task%20in%20the%20control%20of%20gaze&amp;publication_year=2009&amp;author=D.%20Ballard&amp;author=M.%20Hayhoe">Google Scholar</a></div></dd><dt class="label"><a href="#bb0010" id="ref-id-b0010" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ballard et al., 1995</a></dt><dd class="reference" id="h0010"><div class="contribution">D. Ballard, M. Hayhoe, J. Pelz<strong class="title">Memory representations in natural tasks</strong></div><div class="host">Journal of Cognitive Neuroscience, 7 (1) (1995), pp. 66-80</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1162/jocn.1995.7.1.66">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0028907957&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Memory%20representations%20in%20natural%20tasks&amp;publication_year=1995&amp;author=D.%20Ballard&amp;author=M.%20Hayhoe&amp;author=J.%20Pelz">Google Scholar</a></div></dd><dt class="label"><a href="#bb0015" id="ref-id-b0015" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Becker, 1972</a></dt><dd class="reference" id="h0015"><div class="contribution">W. Becker<strong class="title">The control of eye movements in the saccadic system</strong></div><div class="host">Cerebral Control Of Eye Movements And Motion Perception, 82 (1972), pp. 233-243</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0015449731&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20control%20of%20eye%20movements%20in%20the%20saccadic%20system&amp;publication_year=1972&amp;author=W.%20Becker">Google Scholar</a></div></dd><dt class="label"><a href="#bb0020" id="ref-id-b0020" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Bengio and Frasconi, 1995</a></dt><dd class="reference" id="h0020"><div class="contribution">Y. Bengio, P. Frasconi<strong class="title">An input output HMM architecture</strong></div><div class="host">Advances in neural information processing systems, Morgan Kaufmann Publisher (1995), pp. 427-434</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0000971250&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=An%20input%20output%20HMM%20architecture&amp;publication_year=1995&amp;author=Y.%20Bengio&amp;author=P.%20Frasconi">Google Scholar</a></div></dd><dt class="label"><a href="#bb0025" id="ref-id-b0025" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Benson et al., 2012</a></dt><dd class="reference" id="h0025"><div class="contribution">P. Benson, S. Beedie, E. Shephard, I. Giegling, D. Rujescu, D. St Clair<strong class="title">Simple
 viewing tests can detect eye movement abnormalities that distinguish 
schizophrenia cases from controls with exceptional accuracy</strong></div><div class="host">Biological Psychiatry, 72 (9) (2012), pp. 716-724</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84867088878&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Simple%20viewing%20tests%20can%20detect%20eye%20movement%20abnormalities%20that%20distinguish%20schizophrenia%20cases%20from%20controls%20with%20exceptional%20accuracy&amp;publication_year=2012&amp;author=P.%20Benson&amp;author=S.%20Beedie&amp;author=E.%20Shephard&amp;author=I.%20Giegling&amp;author=D.%20Rujescu&amp;author=D.%20St%20Clair">Google Scholar</a></div></dd><dt class="label"><a href="#bb0030" id="ref-id-b0030" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Borji and Itti, 2013</a></dt><dd class="reference" id="h0030"><div class="contribution">A. Borji, L. Itti<strong class="title">State-of-the-art in visual attention modeling</strong></div><div class="host">IEEE Transactions on Pattern Analysis and Machine Intelligence, 35 (1) (2013), pp. 185-207</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84870220894&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=State-of-the-art%20in%20visual%20attention%20modeling&amp;publication_year=2013&amp;author=A.%20Borji&amp;author=L.%20Itti">Google Scholar</a></div></dd><dt class="label"><a href="#bb0035" id="ref-id-b0035" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Borji and Itti, 2014</a></dt><dd class="reference" id="h0035"><div class="contribution">A. Borji, L. Itti<strong class="title">Defending Yarbus: Eye movements reveal observers’ task</strong></div><div class="host">Journal of Vision, 14 (3) (2014), p. 29</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1167/14.3.29">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Defending%20Yarbus:%20Eye%20movements%20reveal%20observers%20task">Google Scholar</a></div></dd><dt class="label"><a href="#bb0040" id="ref-id-b0040" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Bruce and Tsotsos, 2009</a></dt><dd class="reference" id="h0040"><div class="contribution">N. Bruce, J. Tsotsos<strong class="title">Saliency, attention, and visual search: An information theoretic approach</strong></div><div class="host">Journal of Vision, 9 (3) (2009), p. 5</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1167/9.3.5">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Saliency%2C%20attention%2C%20and%20visual%20search%3A%20An%20information%20theoretic%20approach&amp;publication_year=2009&amp;author=N.%20Bruce&amp;author=J.%20Tsotsos">Google Scholar</a></div></dd><dt class="label"><a href="#bb0045" id="ref-id-b0045" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Bulling et al., 2011</a></dt><dd class="reference" id="h0045"><div class="contribution">A. Bulling, D. Roggen, G. Troster<strong class="title">What’s in the eyes for context-awareness?</strong></div><div class="host">IEEE Pervasive Computing, 10 (2) (2011), pp. 48-57</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/MPRV.2010.49">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79954500663&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Whats%20in%20the%20eyes%20for%20context-awareness">Google Scholar</a></div></dd><dt class="label"><a href="#bb0050" id="ref-id-b0050" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Bulling et al., 2009</a></dt><dd class="reference" id="h0050"><div class="contribution">A. Bulling, J. Ward, H. Gellersen, G. Tröster<strong class="title">Eye movement analysis for activity recognition</strong></div><div class="host">Proceedings of the 11th international conference on Ubiquitous computing, ACM (2009), pp. 41-50</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/1620545.1620552">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20movement%20analysis%20for%20activity%20recognition&amp;publication_year=2009&amp;author=A.%20Bulling&amp;author=J.%20Ward&amp;author=H.%20Gellersen&amp;author=G.%20Tr%C3%B6ster">Google Scholar</a></div></dd><dt class="label"><a href="#bb0055" id="ref-id-b0055" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Bulling et al., 2011</a></dt><dd class="reference" id="h0055"><div class="contribution">A. Bulling, J. Ward, H. Gellersen, G. Tröster<strong class="title">Eye movement analysis for activity recognition using electrooculography</strong></div><div class="host">IEEE Transactions on Pattern Analysis and Machine Intelligence, 33 (4) (2011), pp. 741-753</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/TPAMI.2010.86">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79951941415&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20movement%20analysis%20for%20activity%20recognition%20using%20electrooculography&amp;publication_year=2011&amp;author=A.%20Bulling&amp;author=J.%20Ward&amp;author=H.%20Gellersen&amp;author=G.%20Tr%C3%B6ster">Google Scholar</a></div></dd><dt class="label"><a href="#bb0060" id="ref-id-b0060" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Buswell, 1920</a></dt><dd class="reference" id="h0060"><span>Buswell, G. (1920). In <em>An experimental study of the eye-voice span in reading</em> (Vol. 17). University of Chicago.</span><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Buswell,%20G.%20.%20In%20An%20experimental%20study%20of%20the%20eye-voice%20span%20in%20reading%20.%20University%20of%20Chicago.">Google Scholar</a></div></dd><dt class="label"><a href="#bb0065" id="ref-id-b0065" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Buswell, 1935</a></dt><dd class="reference" id="h0065"><div class="contribution">G. Buswell<strong class="title">How people look at pictures: A study of the psychology of perception in art</strong></div><div class="host">University of Chicago Press, Chicago (1935)</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=How%20people%20look%20at%20pictures%3A%20A%20study%20of%20the%20psychology%20of%20perception%20in%20art&amp;publication_year=1935&amp;author=G.%20Buswell">Google Scholar</a></div></dd><dt class="label"><a href="#bb0070" id="ref-id-b0070" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Carpenter, 1991</a></dt><dd class="reference" id="h0070"><div class="contribution">R. Carpenter<strong class="title">The visual origins of ocular motility</strong></div><div class="host">Vision And Visual Function, 8 (1991), pp. 1-10</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0002782651&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20visual%20origins%20of%20ocular%20motility&amp;publication_year=1991&amp;author=R.%20Carpenter">Google Scholar</a></div></dd><dt class="label"><a href="#bb0075" id="ref-id-b0075" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Castelhano and Henderson, 2008</a></dt><dd class="reference" id="h0075"><div class="contribution">M. Castelhano, J. Henderson<strong class="title">Stable individual differences across images in human saccadic eye movements</strong></div><div class="host">Canadian Journal of Experimental Psychology/Revue canadienne de psychologie expérimentale, 62 (1) (2008), pp. 1-14</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1037/1196-1961.62.1.1">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-46049089854&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Stable%20individual%20differences%20across%20images%20in%20human%20saccadic%20eye%20movements&amp;publication_year=2008&amp;author=M.%20Castelhano&amp;author=J.%20Henderson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0080" id="ref-id-b0080" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Castelhano et al., 2009</a></dt><dd class="reference" id="h0080"><div class="contribution">M. Castelhano, M. Mack, J. Henderson<strong class="title">Viewing task influences eye movement control during active scene perception</strong></div><div class="host">Journal of Vision, 9 (3) (2009), p. 6</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1167/9.3.6">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Viewing%20task%20influences%20eye%20movement%20control%20during%20active%20scene%20perception&amp;publication_year=2009&amp;author=M.%20Castelhano&amp;author=M.%20Mack&amp;author=J.%20Henderson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0085" id="ref-id-b0085" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Clark, 1999</a></dt><dd class="reference" id="h0085"><div class="contribution">J. Clark<strong class="title">Spatial attention and latencies of saccadic eye movements</strong></div><div class="host">Vision Research, 39 (3) (1999), pp. 585-602</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0344889218&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Spatial%20attention%20and%20latencies%20of%20saccadic%20eye%20movements&amp;publication_year=1999&amp;author=J.%20Clark">Google Scholar</a></div></dd><dt class="label"><a href="#bb0090" id="ref-id-b0090" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Clark and O’Regan, 1998</a></dt><dd class="reference" id="h0090"><div class="contribution">J. Clark, J. O’Regan<strong class="title">Word ambiguity and the optimal viewing position in reading</strong></div><div class="host">Vision Research, 39 (4) (1998), pp. 843-857</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Word%20ambiguity%20and%20the%20optimal%20viewing%20position%20in%20reading&amp;publication_year=1998&amp;author=J.%20Clark&amp;author=J.%20O%E2%80%99Regan">Google Scholar</a></div></dd><dt class="label"><a href="#bb0095" id="ref-id-b0095" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Coëffé and O’regan, 1987</a></dt><dd class="reference" id="h0095"><div class="contribution">C. Coëffé, J. O’regan<strong class="title">Reducing the influence of non-target stimuli on saccade accuracy: Predictability and latency effects</strong></div><div class="host">Vision Research, 27 (2) (1987), pp. 227-240</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0023252947&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Reducing%20the%20influence%20of%20non-target%20stimuli%20on%20saccade%20accuracy%3A%20Predictability%20and%20latency%20effects&amp;publication_year=1987&amp;author=C.%20Co%C3%ABff%C3%A9&amp;author=J.%20O%E2%80%99regan">Google Scholar</a></div></dd><dt class="label"><a href="#bb0100" id="ref-id-b0100" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Connor et al., 2004</a></dt><dd class="reference" id="h0100"><div class="contribution">C. Connor, H. Egeth, S. Yantis<strong class="title">Visual attention: Bottom-up versus top-down</strong></div><div class="host">Current Biology, 14 (19) (2004), pp. R850-R852</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-4644244389&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20attention%3A%20Bottom-up%20versus%20top-down&amp;publication_year=2004&amp;author=C.%20Connor&amp;author=H.%20Egeth&amp;author=S.%20Yantis">Google Scholar</a></div></dd><dt class="label"><a href="#bb0105" id="ref-id-b0105" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Desimone and Duncan, 1995</a></dt><dd class="reference" id="h0105"><div class="contribution">R. Desimone, J. Duncan<strong class="title">Neural mechanisms of selective visual attention</strong></div><div class="host">Annual Review Of Neuroscience, 18 (1) (1995), pp. 193-222</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1146/annurev.ne.18.030195.001205">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0028951934&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Neural%20mechanisms%20of%20selective%20visual%20attention&amp;publication_year=1995&amp;author=R.%20Desimone&amp;author=J.%20Duncan">Google Scholar</a></div></dd><dt class="label"><a href="#bb0110" id="ref-id-b0110" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Deubel and Schneider, 1996</a></dt><dd class="reference" id="h0110"><div class="contribution">H. Deubel, W. Schneider<strong class="title">Saccade target selection and object recognition: Evidence for a common attentional mechanism</strong></div><div class="host">Vision Research, 36 (12) (1996), pp. 1827-1837</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029884453&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Saccade%20target%20selection%20and%20object%20recognition%3A%20Evidence%20for%20a%20common%20attentional%20mechanism&amp;publication_year=1996&amp;author=H.%20Deubel&amp;author=W.%20Schneider">Google Scholar</a></div></dd><dt class="label"><a href="#bb0115" id="ref-id-b0115" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Di Stasi et al., 2012</a></dt><dd class="reference" id="h0115"><div class="contribution">L. Di Stasi, R. Renner, A. Catena, J. Cañas, B. Velichkovsky, S. Pannasch<strong class="title">Towards a driver fatigue test based on the saccadic main sequence: A partial validation by subjective report data</strong></div><div class="host">Transportation Research Part C: Emerging Technologies, 21 (1) (2012), pp. 122-133</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80155154086&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Towards%20a%20driver%20fatigue%20test%20based%20on%20the%20saccadic%20main%20sequence%3A%20A%20partial%20validation%20by%20subjective%20report%20data&amp;publication_year=2012&amp;author=L.%20Di%20Stasi&amp;author=R.%20Renner&amp;author=A.%20Catena&amp;author=J.%20Ca%C3%B1as&amp;author=B.%20Velichkovsky&amp;author=S.%20Pannasch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0120" id="ref-id-b0120" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Di Stasi et al., 2010</a></dt><dd class="reference" id="h0120"><div class="contribution">L. Di Stasi, R. Renner, P. Staehr, J. Helmert, B. Velichkovsky, J. Canas, <em> et al.</em><strong class="title">Saccadic peak velocity sensitivity to variations in mental workload</strong></div><div class="host">Aviation, Space, and Environmental Medicine, 81 (4) (2010), pp. 413-417</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.3357/ASEM.2579.2010">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77950632133&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Saccadic%20peak%20velocity%20sensitivity%20to%20variations%20in%20mental%20workload&amp;publication_year=2010&amp;author=L.%20Di%20Stasi&amp;author=R.%20Renner&amp;author=P.%20Staehr&amp;author=J.%20Helmert&amp;author=B.%20Velichkovsky&amp;author=J.%20Canas">Google Scholar</a></div></dd><dt class="label"><a href="#bb0125" id="ref-id-b0125" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Droll et al., 2005</a></dt><dd class="reference" id="h0125"><div class="contribution">J. Droll, M. Hayhoe, J. Triesch, B. Sullivan<strong class="title">Task demands control acquisition and storage of visual information</strong></div><div class="host">Journal of Experimental Psychology: Human Perception and Performance, 31 (6) (2005), p. 1416</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1037/0096-1523.31.6.1416">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-32644486053&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Task%20demands%20control%20acquisition%20and%20storage%20of%20visual%20information&amp;publication_year=2005&amp;author=J.%20Droll&amp;author=M.%20Hayhoe&amp;author=J.%20Triesch&amp;author=B.%20Sullivan">Google Scholar</a></div></dd><dt class="label"><a href="#bb0130" id="ref-id-b0130" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ehinger et al., 2009</a></dt><dd class="reference" id="h0130"><div class="contribution">K. Ehinger, B. Hidalgo-Sotelo, A. Torralba, A. Oliva<strong class="title">Modelling search for people in 900 scenes: A combined source model of eye guidance</strong></div><div class="host">Visual Cognition, 17 (6-7) (2009), pp. 945-978</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/13506280902834720">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70349919085&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Modelling%20search%20for%20people%20in%20900%20scenes%3A%20A%20combined%20source%20model%20of%20eye%20guidance&amp;publication_year=2009&amp;author=K.%20Ehinger&amp;author=B.%20Hidalgo-Sotelo&amp;author=A.%20Torralba&amp;author=A.%20Oliva">Google Scholar</a></div></dd><dt class="label"><a href="#bb0135" id="ref-id-b0135" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Einhäuser et al., 2008</a></dt><dd class="reference" id="h0135"><div class="contribution">W. Einhäuser, U. Rutishauser, C. Koch<strong class="title">Task-demands can immediately reverse the effects of sensory-driven saliency in complex visual stimuli</strong></div><div class="host">Journal of Vision, 8 (2) (2008), p. 2</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1167/8.2.2">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Task-demands%20can%20immediately%20reverse%20the%20effects%20of%20sensory-driven%20saliency%20in%20complex%20visual%20stimuli&amp;publication_year=2008&amp;author=W.%20Einh%C3%A4user&amp;author=U.%20Rutishauser&amp;author=C.%20Koch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0140" id="ref-id-b0140" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Elhelw et al., 2008</a></dt><dd class="reference" id="h0140"><div class="contribution">M. Elhelw, M. Nicolaou, A. Chung, G. Yang, M. Atkins<strong class="title">A gaze-based study for investigating the perception of visual realism in simulated scenes</strong></div><div class="host">ACM Transactions on Applied Perception (TAP), 5 (1) (2008), p. 3</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20gaze-based%20study%20for%20investigating%20the%20perception%20of%20visual%20realism%20in%20simulated%20scenes&amp;publication_year=2008&amp;author=M.%20Elhelw&amp;author=M.%20Nicolaou&amp;author=A.%20Chung&amp;author=G.%20Yang&amp;author=M.%20Atkins">Google Scholar</a></div></dd><dt class="label"><a href="#bb0145" id="ref-id-b0145" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Epelboim et al., 1995</a></dt><dd class="reference" id="h0145"><div class="contribution">J. Epelboim, R. Steinman, E. Kowler, M. Edwards, Z. Pizlo, C. Erkelens, <em> et al.</em><strong class="title">The function of visual search and memory in sequential looking tasks</strong></div><div class="host">Vision Research, 35 (23) (1995), pp. 3401-3422</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0028879724&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20function%20of%20visual%20search%20and%20memory%20in%20sequential%20looking%20tasks&amp;publication_year=1995&amp;author=J.%20Epelboim&amp;author=R.%20Steinman&amp;author=E.%20Kowler&amp;author=M.%20Edwards&amp;author=Z.%20Pizlo&amp;author=C.%20Erkelens">Google Scholar</a></div></dd><dt class="label"><a href="#bb0150" id="ref-id-b0150" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Eriksen and James, 1986</a></dt><dd class="reference" id="h0150"><div class="contribution">C. Eriksen, J. James<strong class="title">Visual attention within and around the field of focal attention: A zoom lens model</strong></div><div class="host">Perception &amp; Psychophysics, 40 (4) (1986), pp. 225-240</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0022803434&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20attention%20within%20and%20around%20the%20field%20of%20focal%20attention%3A%20A%20zoom%20lens%20model&amp;publication_year=1986&amp;author=C.%20Eriksen&amp;author=J.%20James">Google Scholar</a></div></dd><dt class="label"><a href="#bb0155" id="ref-id-b0155" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Findlay, 1981</a></dt><dd class="reference" id="h0155"><div class="contribution">J. Findlay<strong class="title">Local and global influences on saccadic eye movements</strong></div><div class="host">Eye Movements: Cognition And Visual Perception (1981), pp. 171-179</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0012541546&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Local%20and%20global%20influences%20on%20saccadic%20eye%20movements&amp;publication_year=1981&amp;author=J.%20Findlay">Google Scholar</a></div></dd><dt class="label"><a href="#bb0160" id="ref-id-b0160" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Flanagan and Johansson, 2003</a></dt><dd class="reference" id="h0160"><div class="contribution">J. Flanagan, R. Johansson<strong class="title">Action plans used in action observation</strong></div><div class="host">Nature, 424 (6950) (2003), pp. 769-771</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0042467641&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Action%20plans%20used%20in%20action%20observation&amp;publication_year=2003&amp;author=J.%20Flanagan&amp;author=R.%20Johansson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0165" id="ref-id-b0165" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Fuchs, 1971</a></dt><dd class="reference" id="h0165"><div class="contribution">A. Fuchs<strong class="title">The saccadic system</strong></div><div class="host">C. Collins, J. Hyde (Eds.), The control of eye movements, Academic Press, New York (1971), pp. 343-362</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20saccadic%20system&amp;publication_year=1971&amp;author=A.%20Fuchs">Google Scholar</a></div></dd><dt class="label"><a href="#bb0170" id="ref-id-b0170" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Furneaux and Land, 1999</a></dt><dd class="reference" id="h0170"><div class="contribution">S. Furneaux, M. Land<strong class="title">The effects of skill on the eye–hand span during musical sight–reading</strong></div><div class="host">Proceedings of the Royal Society of London. Series B: Biological Sciences, 266 (1436) (1999), pp. 2435-2440</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033534012&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=The%20effects%20of%20skill%20on%20the%20eyehand%20span%20during%20musical%20sightreading">Google Scholar</a></div></dd><dt class="label"><a href="#bb0175" id="ref-id-b0175" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Goferman et al., 2012</a></dt><dd class="reference" id="h0175"><div class="contribution">S. Goferman, L. Zelnik-Manor, A. Tal<strong class="title">Context-aware saliency detection</strong></div><div class="host">IEEE Transactions on Pattern Analysis and Machine Intelligence, 34 (10) (2012), pp. 1915-1926</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/TPAMI.2011.272">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84865331032&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Context-aware%20saliency%20detection&amp;publication_year=2012&amp;author=S.%20Goferman&amp;author=L.%20Zelnik-Manor&amp;author=A.%20Tal">Google Scholar</a></div></dd><dt class="label"><a href="#bb0180" id="ref-id-b0180" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Google and photo archive hosted by Google, 2013</a></dt><dd class="reference" id="h0180"><span>Google, (2013). LIFE photo archive hosted by Google. &lt;<a href="http://images.google.com/hosted/life" target="_blank" rel="noreferrer noopener">http://images.google.com/hosted/life</a>&gt;.</span><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Google,%20.%20LIFE%20photo%20archive%20hosted%20by%20Google.%20http:images.google.comhostedlife.">Google Scholar</a></div></dd><dt class="label"><a href="#bb0185" id="ref-id-b0185" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Greene et al., 2011</a></dt><dd class="reference" id="h0185"><div class="contribution">M. Greene, T. Liu, J. Wolfe<strong class="title">Reconsidering Yarbus: Pattern classification cannot predict observer’s task from scan paths</strong></div><div class="host">Journal of Vision, 11 (11) (2011), p. 498</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84908074974&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Reconsidering%20Yarbus:%20Pattern%20classification%20cannot%20predict%20observers%20task%20from%20scan%20paths">Google Scholar</a></div></dd><dt class="label"><a href="#bb0190" id="ref-id-b0190" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Greene et al., 2012</a></dt><dd class="reference" id="h0190"><div class="contribution">M. Greene, T. Liu, J. Wolfe<strong class="title">Reconsidering Yarbus: A failure to predict observers’ task from eye movement patterns</strong></div><div class="host">Vision Research, 62 (2012), pp. 1-8</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84859918194&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Reconsidering%20Yarbus:%20A%20failure%20to%20predict%20observers%20task%20from%20eye%20movement%20patterns">Google Scholar</a></div></dd><dt class="label"><a href="#bb0195" id="ref-id-b0195" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Gunn, 1998</a></dt><dd class="reference" id="h0195"><span>Gunn, S. R. (1998). Support vector machines for classification and regression. ISIS Technical Report 14.</span><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Gunn,%20S.%20R.%20.%20Support%20vector%20machines%20for%20classification%20and%20regression.%20ISIS%20Technical%20Report%2014.">Google Scholar</a></div></dd><dt class="label"><a href="#bb0200" id="ref-id-b0200" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hacisalihzade et al., 1992</a></dt><dd class="reference" id="h0200"><div class="contribution">S. Hacisalihzade, L. Stark, J. Allen<strong class="title">Visual perception and sequences of eye movement fixations: A stochastic modeling approach</strong></div><div class="host">IEEE Transactions on Systems Man and Cybernetics, 2 (3) (1992), pp. 474-481</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0026861345&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20perception%20and%20sequences%20of%20eye%20movement%20fixations%3A%20A%20stochastic%20modeling%20approach&amp;publication_year=1992&amp;author=S.%20Hacisalihzade&amp;author=L.%20Stark&amp;author=J.%20Allen">Google Scholar</a></div></dd><dt class="label"><a href="#bb0205" id="ref-id-b0205" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hafed and Clark, 2002</a></dt><dd class="reference" id="h0205"><div class="contribution">Z. Hafed, J. Clark<strong class="title">Microsaccades as an overt measure of covert attention shifts</strong></div><div class="host">Vision Research, 42 (22) (2002), pp. 2533-2545</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036810151&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Microsaccades%20as%20an%20overt%20measure%20of%20covert%20attention%20shifts&amp;publication_year=2002&amp;author=Z.%20Hafed&amp;author=J.%20Clark">Google Scholar</a></div></dd><dt class="label"><a href="#bb0210" id="ref-id-b0210" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Haji-Abolhassani and Clark, 2013</a></dt><dd class="reference" id="h0210"><div class="contribution">A. Haji-Abolhassani, J. Clark<strong class="title">A computational model for task inference in visual search</strong></div><div class="host">Journal of Vision, 13 (3) (2013), p. 29</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1167/13.3.29">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20computational%20model%20for%20task%20inference%20in%20visual%20search&amp;publication_year=2013&amp;author=A.%20Haji-Abolhassani&amp;author=J.%20Clark">Google Scholar</a></div></dd><dt class="label"><a href="#bb0215" id="ref-id-b0215" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Haxby et al., 2001</a></dt><dd class="reference" id="h0215"><div class="contribution">J. Haxby, M. Gobbini, M. Furey, A. Ishai, J. Schouten, P. Pietrini<strong class="title">Distributed and overlapping representations of faces and objects in ventral temporal cortex</strong></div><div class="host">Science, 293 (5539) (2001), pp. 2425-2430</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035964792&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Distributed%20and%20overlapping%20representations%20of%20faces%20and%20objects%20in%20ventral%20temporal%20cortex&amp;publication_year=2001&amp;author=J.%20Haxby&amp;author=M.%20Gobbini&amp;author=M.%20Furey&amp;author=A.%20Ishai&amp;author=J.%20Schouten&amp;author=P.%20Pietrini">Google Scholar</a></div></dd><dt class="label"><a href="#bb0220" id="ref-id-b0220" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hayhoe and Ballard, 2005</a></dt><dd class="reference" id="h0220"><div class="contribution">M. Hayhoe, D. Ballard<strong class="title">Eye movements in natural behavior</strong></div><div class="host">Trends in Cognitive Sciences, 9 (4) (2005), pp. 188-194</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-17444385937&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20movements%20in%20natural%20behavior&amp;publication_year=2005&amp;author=M.%20Hayhoe&amp;author=D.%20Ballard">Google Scholar</a></div></dd><dt class="label"><a href="#bb0225" id="ref-id-b0225" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hayhoe et al., 1998</a></dt><dd class="reference" id="h0225"><div class="contribution">M. Hayhoe, D. Bensinger, D. Ballard<strong class="title">Task constraints in visual working memory</strong></div><div class="host">Vision Research, 38 (1) (1998), pp. 125-137</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0031986480&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Task%20constraints%20in%20visual%20working%20memory&amp;publication_year=1998&amp;author=M.%20Hayhoe&amp;author=D.%20Bensinger&amp;author=D.%20Ballard">Google Scholar</a></div></dd><dt class="label"><a href="#bb0230" id="ref-id-b0230" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hayhoe et al., 2003</a></dt><dd class="reference" id="h0230"><div class="contribution">M. Hayhoe, A. Shrivastava, R. Mruczek, J. Pelz<strong class="title">Visual memory and motor planning in a natural task</strong></div><div class="host">Journal of Vision, 3 (1) (2003), p. 6</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1167/3.1.6">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20memory%20and%20motor%20planning%20in%20a%20natural%20task&amp;publication_year=2003&amp;author=M.%20Hayhoe&amp;author=A.%20Shrivastava&amp;author=R.%20Mruczek&amp;author=J.%20Pelz">Google Scholar</a></div></dd><dt class="label"><a href="#bb0235" id="ref-id-b0235" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hearst et al., 1998</a></dt><dd class="reference" id="h0235"><div class="contribution">M. Hearst, S. Dumais, E. Osman, J. Platt, B. Scholkopf<strong class="title">Support vector machines</strong></div><div class="host">IEEE Intelligent Systems and Their Applications, 13 (4) (1998), pp. 18-28</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/5254.708428">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Support%20vector%20machines&amp;publication_year=1998&amp;author=M.%20Hearst&amp;author=S.%20Dumais&amp;author=E.%20Osman&amp;author=J.%20Platt&amp;author=B.%20Scholkopf">Google Scholar</a></div></dd><dt class="label"><a href="#bb0240" id="ref-id-b0240" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">He and Kowler, 1989</a></dt><dd class="reference" id="h0240"><div class="contribution">P. He, E. Kowler<strong class="title">The role of location probability in the programming of saccades: Implications for center-of-gravity tendencies</strong></div><div class="host">Vision Research, 29 (9) (1989), pp. 1165-1181</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0024434632&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20role%20of%20location%20probability%20in%20the%20programming%20of%20saccades%3A%20Implications%20for%20center-of-gravity%20tendencies&amp;publication_year=1989&amp;author=P.%20He&amp;author=E.%20Kowler">Google Scholar</a></div></dd><dt class="label"><a href="#bb0245" id="ref-id-b0245" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Henderson, 1992</a></dt><dd class="reference" id="h0245"><div class="contribution">J. Henderson<strong class="title">Visual attention and eye movement control during reading and picture viewing</strong></div><div class="host">Eye movements and visual cognition, Springer (1992), pp. 260-283</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/978-1-4612-2852-3_15">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20attention%20and%20eye%20movement%20control%20during%20reading%20and%20picture%20viewing&amp;publication_year=1992&amp;author=J.%20Henderson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0250" id="ref-id-b0250" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hoffman and Subramaniam, 1995</a></dt><dd class="reference" id="h0250"><div class="contribution">J. Hoffman, B. Subramaniam<strong class="title">The role of visual attention in saccadic eye movements</strong></div><div class="host">Perception &amp; Psychophysics, 57 (6) (1995), pp. 787-795</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029356126&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20role%20of%20visual%20attention%20in%20saccadic%20eye%20movements&amp;publication_year=1995&amp;author=J.%20Hoffman&amp;author=B.%20Subramaniam">Google Scholar</a></div></dd><dt class="label"><a href="#bb0255" id="ref-id-b0255" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hu et al., 1996</a></dt><dd class="reference" id="h0255"><div class="contribution">J. Hu, M. Brown, W. Turin<strong class="title">HMM based on-line handwriting recognition</strong></div><div class="host">IEEE Transactions on Pattern Analysis and Machine Intelligence, 18 (10) (1996), pp. 1039-1045</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0030261112&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=HMM%20based%20on-line%20handwriting%20recognition&amp;publication_year=1996&amp;author=J.%20Hu&amp;author=M.%20Brown&amp;author=W.%20Turin">Google Scholar</a></div></dd><dt class="label"><a href="#bb0260" id="ref-id-b0260" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Itti and Koch, 2001</a></dt><dd class="reference" id="h0260"><div class="contribution">L. Itti, C. Koch<strong class="title">Computational modelling of visual attention</strong></div><div class="host">Nature Reviews Neuroscience, 2 (3) (2001), pp. 194-204</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1038/35058500">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035286497&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Computational%20modelling%20of%20visual%20attention&amp;publication_year=2001&amp;author=L.%20Itti&amp;author=C.%20Koch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0265" id="ref-id-b0265" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Itti and Koch, 2001</a></dt><dd class="reference" id="h0265"><div class="contribution">L. Itti, C. Koch<strong class="title">Feature combination strategies for saliency-based visual attention systems</strong></div><div class="host">Journal of Electronic Imaging, 10 (2001), pp. 161-169</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035056743&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Feature%20combination%20strategies%20for%20saliency-based%20visual%20attention%20systems&amp;publication_year=2001&amp;author=L.%20Itti&amp;author=C.%20Koch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0270" id="ref-id-b0270" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Johansson et al., 2001</a></dt><dd class="reference" id="h0270"><div class="contribution">R. Johansson, G. Westling, A. Bäckström, J. Flanagan<strong class="title">Eye–hand coordination in object manipulation</strong></div><div class="host">Journal of Neuroscience, 21 (17) (2001), pp. 6917-6932</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035448913&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Eyehand%20coordination%20in%20object%20manipulation">Google Scholar</a></div></dd><dt class="label"><a href="#bb0275" id="ref-id-b0275" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Judd et al., 2012</a></dt><dd class="reference" id="h0275"><span>Judd,
 T., Durand, F., Torralba, A. (2012). A benchmark of computational 
models of saliency to predict human fixations. Mit Tech Report URL &lt;<a href="http://hdl.handle.net/1721.1/68590" target="_blank" rel="noreferrer noopener">http://hdl.handle.net/1721.1/68590</a>&gt;.</span><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Judd,%20T.,%20Durand,%20F.,%20Torralba,%20A.%20.%20A%20benchmark%20of%20computational%20models%20of%20saliency%20to%20predict%20human%20fixations.%20Mit%20Tech%20Report%20URL%20http:hdl.handle.net1721.168590.">Google Scholar</a></div></dd><dt class="label"><a href="#bb0280" id="ref-id-b0280" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kanan et al., 2014</a></dt><dd class="reference" id="h0280"><div class="contribution">C. Kanan, N.A. Ray, D.N. Bseiso, J.H. Hsiao, G.W. Cottrell<strong class="title">Predicting an observer’s task using multi-fixation pattern analysis</strong></div><div class="host">Proceedings of the symposium on eye tracking research and applications, Springer (2014), pp. 287-290</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/2578153.2578208">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84899680594&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Predicting%20an%20observers%20task%20using%20multi-fixation%20pattern%20analysis">Google Scholar</a></div></dd><dt class="label"><a href="#bb0285" id="ref-id-b0285" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kanan et al., 2009</a></dt><dd class="reference" id="h0285"><div class="contribution">C. Kanan, M. Tong, L. Zhang, G. Cottrell<strong class="title">SUN: Top-down saliency using natural statistics</strong></div><div class="host">Visual Cognition, 17 (6-7) (2009), pp. 979-1003</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/13506280902771138">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70549092787&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=SUN%3A%20Top-down%20saliency%20using%20natural%20statistics&amp;publication_year=2009&amp;author=C.%20Kanan&amp;author=M.%20Tong&amp;author=L.%20Zhang&amp;author=G.%20Cottrell">Google Scholar</a></div></dd><dt class="label"><a href="#bb0290" id="ref-id-b0290" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kaufman and Rousseeuw, 2009</a></dt><dd class="reference" id="h0290"><div class="contribution">L. Kaufman, P. Rousseeuw</div><div class="host">Finding groups in data: An introduction to cluster analysis, Vol. 344, Wiley Interscience (2009)</div><div class="ReferenceLinks u-font-sans"></div></dd><dt class="label"><a href="#bb0295" id="ref-id-b0295" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Klein, 1980</a></dt><dd class="reference" id="h0295"><div class="contribution">R. Klein<strong class="title">Does oculomotor readiness mediate cognitive control of visual attention</strong></div><div class="host">Attention and Performance VIII, 8 (1980), pp. 259-276</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84985266066&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Does%20oculomotor%20readiness%20mediate%20cognitive%20control%20of%20visual%20attention&amp;publication_year=1980&amp;author=R.%20Klein">Google Scholar</a></div></dd><dt class="label"><a href="#bb0300" id="ref-id-b0300" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Klein, 2000</a></dt><dd class="reference" id="h0300"><div class="contribution">R. Klein<strong class="title">Inhibition of return</strong></div><div class="host">Trends in Cognitive Sciences, 4 (4) (2000), pp. 138-147</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0034177174&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Inhibition%20of%20return&amp;publication_year=2000&amp;author=R.%20Klein">Google Scholar</a></div></dd><dt class="label"><a href="#bb0305" id="ref-id-b0305" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Klein and MacInnes, 1999</a></dt><dd class="reference" id="h0305"><div class="contribution">R. Klein, W. MacInnes<strong class="title">Inhibition of return is a foraging facilitator in visual search</strong></div><div class="host">Psychological Science, 10 (4) (1999), pp. 346-352</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1111/1467-9280.00166">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0242454667&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Inhibition%20of%20return%20is%20a%20foraging%20facilitator%20in%20visual%20search&amp;publication_year=1999&amp;author=R.%20Klein&amp;author=W.%20MacInnes">Google Scholar</a></div></dd><dt class="label"><a href="#bb0310" id="ref-id-b0310" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Koch and Ullman, 1985</a></dt><dd class="reference" id="h0310"><div class="contribution">C. Koch, S. Ullman<strong class="title">Shifts in selective visual attention: towards the underlying neural circuitry</strong></div><div class="host">Human Neurobiology, 4 (4) (1985), pp. 219-227</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0022388528&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Shifts%20in%20selective%20visual%20attention%3A%20towards%20the%20underlying%20neural%20circuitry&amp;publication_year=1985&amp;author=C.%20Koch&amp;author=S.%20Ullman">Google Scholar</a></div></dd><dt class="label"><a href="#bb0315" id="ref-id-b0315" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kowler et al., 1995</a></dt><dd class="reference" id="h0315"><div class="contribution">E. Kowler, E. Anderson, B. Dosher, E. Blaser<strong class="title">The role of attention in the programming of saccades</strong></div><div class="host">Vision Research, 35 (13) (1995), pp. 1897-1916</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029037304&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20role%20of%20attention%20in%20the%20programming%20of%20saccades&amp;publication_year=1995&amp;author=E.%20Kowler&amp;author=E.%20Anderson&amp;author=B.%20Dosher&amp;author=E.%20Blaser">Google Scholar</a></div></dd><dt class="label"><a href="#bb0320" id="ref-id-b0320" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Land and Furneaux, 1997</a></dt><dd class="reference" id="h0320"><div class="contribution">M. Land, S. Furneaux<strong class="title">The knowledge base of the oculomotor system</strong></div><div class="host">Philosophical Transactions of the Royal Society of London, Series B: Biological Sciences, 352 (1358) (1997), pp. 1231-1239</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0031590136&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20knowledge%20base%20of%20the%20oculomotor%20system&amp;publication_year=1997&amp;author=M.%20Land&amp;author=S.%20Furneaux">Google Scholar</a></div></dd><dt class="label"><a href="#bb0325" id="ref-id-b0325" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Land and Lee, 1994</a></dt><dd class="reference" id="h0325"><div class="contribution">M. Land, D. Lee<strong class="title">Where do we look when we steer</strong></div><div class="host">Nature, 369 (1994), pp. 742-744</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0028225676&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Where%20do%20we%20look%20when%20we%20steer&amp;publication_year=1994&amp;author=M.%20Land&amp;author=D.%20Lee">Google Scholar</a></div></dd><dt class="label"><a href="#bb0330" id="ref-id-b0330" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Land and McLeod, 2000</a></dt><dd class="reference" id="h0330"><div class="contribution">M. Land, P. McLeod<strong class="title">From eye movements to actions: How batsmen hit the ball</strong></div><div class="host">Nature Neuroscience, 3 (12) (2000), pp. 1340-1345</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033681569&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=From%20eye%20movements%20to%20actions%3A%20How%20batsmen%20hit%20the%20ball&amp;publication_year=2000&amp;author=M.%20Land&amp;author=P.%20McLeod">Google Scholar</a></div></dd><dt class="label"><a href="#bb0335" id="ref-id-b0335" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Land et al., 1999</a></dt><dd class="reference" id="h0335"><div class="contribution">M. Land, N. Mennie, J. Rusted<strong class="title">The roles of vision and eye movements in the control of activities of daily living</strong></div><div class="host">PERCEPTION, 28 (11) (1999), pp. 1311-1328</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1068/p2935">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033252705&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20roles%20of%20vision%20and%20eye%20movements%20in%20the%20control%20of%20activities%20of%20daily%20living&amp;publication_year=1999&amp;author=M.%20Land&amp;author=N.%20Mennie&amp;author=J.%20Rusted">Google Scholar</a></div></dd><dt class="label"><a href="#bb0340" id="ref-id-b0340" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Land and Tatler, 2001</a></dt><dd class="reference" id="h0340"><div class="contribution">M. Land, B. Tatler<strong class="title">Steering with the head: The visual strategy of a racing driver</strong></div><div class="host">Current Biology, 11 (15) (2001), pp. 1215-1220</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035822592&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Steering%20with%20the%20head%3A%20The%20visual%20strategy%20of%20a%20racing%20driver&amp;publication_year=2001&amp;author=M.%20Land&amp;author=B.%20Tatler">Google Scholar</a></div></dd><dt class="label"><a href="#bb0345" id="ref-id-b0345" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">MacLeod et al., 1986</a></dt><dd class="reference" id="h0345"><div class="contribution">C. MacLeod, A. Mathews, P. Tata<strong class="title">Attentional bias in emotional disorders</strong></div><div class="host">Journal of Abnormal Psychology, 95 (1) (1986), p. 15</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0022898410&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Attentional%20bias%20in%20emotional%20disorders&amp;publication_year=1986&amp;author=C.%20MacLeod&amp;author=A.%20Mathews&amp;author=P.%20Tata">Google Scholar</a></div></dd><dt class="label"><a href="#bb0350" id="ref-id-b0350" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Mannan et al., 1997</a></dt><dd class="reference" id="h0350"><div class="contribution">S. Mannan, K. Ruddock, D. Wooding<strong class="title">Fixation sequences made during visual examination of briefly presented 2D images</strong></div><div class="host">Spatial Vision, 11 (2) (1997), pp. 157-178</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0031283011&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Fixation%20sequences%20made%20during%20visual%20examination%20of%20briefly%20presented%202D%20images&amp;publication_year=1997&amp;author=S.%20Mannan&amp;author=K.%20Ruddock&amp;author=D.%20Wooding">Google Scholar</a></div></dd><dt class="label"><a href="#bb0355" id="ref-id-b0355" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Mika et al., 1999</a></dt><dd class="reference" id="h0355"><span>Mika, S., Ratsch, G., Weston, J., Scholkopf, B., &amp; Mullers, K. (1999). Fisher discriminant analysis with kernels. In <em>Neural networks for signal processing IX, 1999. Proceedings of the 1999 IEEE signal processing society workshop</em> (pp. 41–48). IEEE.</span><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Mika,%20S.,%20Ratsch,%20G.,%20Weston,%20J.,%20Scholkopf,%20B.,%20%20Mullers,%20K.%20.%20Fisher%20discriminant%20analysis%20with%20kernels.%20In%20Neural%20networks%20for%20signal%20processing%20IX,%201999.%20Proceedings%20of%20the%201999%20IEEE%20signal%20processing%20society%20workshop%20.%20IEEE.">Google Scholar</a></div></dd><dt class="label"><a href="#bb0360" id="ref-id-b0360" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Mills et al., 2011</a></dt><dd class="reference" id="h0360"><div class="contribution">M. Mills, A. Hollingworth, S. Van der Stigchel, L. Hoffman, M.D. Dodd<strong class="title">Examining the influence of task set on eye movements and fixations</strong></div><div class="host">Journal of Vision, 11 (8) (2011)</div><div class="comment">(article 17)</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Examining%20the%20influence%20of%20task%20set%20on%20eye%20movements%20and%20fixations&amp;publication_year=2011&amp;author=M.%20Mills&amp;author=A.%20Hollingworth&amp;author=S.%20Van%20der%20Stigchel&amp;author=L.%20Hoffman&amp;author=M.D.%20Dodd">Google Scholar</a></div></dd><dt class="label"><a href="#bb0365" id="ref-id-b0365" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Nair et al., 2002</a></dt><dd class="reference" id="h0365"><span>Nair, V., &amp; Clark, J. (2002). Automated visual surveillance using hidden markov models. In <em>International conference on vision interface</em> (Vol. 93, pp. 88–93).</span><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Nair,%20V.,%20%20Clark,%20J.%20.%20Automated%20visual%20surveillance%20using%20hidden%20markov%20models.%20In%20International%20conference%20on%20vision%20interface%20.">Google Scholar</a></div></dd><dt class="label"><a href="#bb0370" id="ref-id-b0370" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Najemnik and Geisler, 2005</a></dt><dd class="reference" id="h0370"><div class="contribution">J. Najemnik, W. Geisler<strong class="title">Optimal eye movement strategies in visual search</strong></div><div class="host">Nature, 434 (7031) (2005), pp. 387-391</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1038/nature03390">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-15244352522&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Optimal%20eye%20movement%20strategies%20in%20visual%20search&amp;publication_year=2005&amp;author=J.%20Najemnik&amp;author=W.%20Geisler">Google Scholar</a></div></dd><dt class="label"><a href="#bb0375" id="ref-id-b0375" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Niebur and Koch, 1998</a></dt><dd class="reference" id="h0375"><div class="contribution">E. Niebur, C. Koch<strong class="title">Computational architectures for attention</strong></div><div class="host">R. Parasuraman (Ed.), The attentive brain, MIT Press, Cambridge, MA (1998), pp. 163-186</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Computational%20architectures%20for%20attention&amp;publication_year=1998&amp;author=E.%20Niebur&amp;author=C.%20Koch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0380" id="ref-id-b0380" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Noton and Stark, 1971</a></dt><dd class="reference" id="h0380"><div class="contribution">D. Noton, L. Stark<strong class="title">Scanpaths in eye movements during pattern perception</strong></div><div class="host">Science, 171 (968) (1971), pp. 308-311</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0015235750&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Scanpaths%20in%20eye%20movements%20during%20pattern%20perception&amp;publication_year=1971&amp;author=D.%20Noton&amp;author=L.%20Stark">Google Scholar</a></div></dd><dt class="label"><a href="#bb0385" id="ref-id-b0385" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Nuthmann and Henderson, 2010</a></dt><dd class="reference" id="h0385"><div class="contribution">A. Nuthmann, J. Henderson<strong class="title">Object-based attentional selection in scene viewing</strong></div><div class="host">Journal of Vision, 10 (8) (2010)</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Object-based%20attentional%20selection%20in%20scene%20viewing&amp;publication_year=2010&amp;author=A.%20Nuthmann&amp;author=J.%20Henderson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0390" id="ref-id-b0390" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">O’Regan et al., 2000</a></dt><dd class="reference" id="h0390"><div class="contribution">J. O’Regan, H. Deubel, J. Clark, R. Rensink<strong class="title">Picture changes during blinks: Looking without seeing and seeing without looking</strong></div><div class="host">Visual Cognition, 7 (1-3) (2000), pp. 191-211</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0034046279&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Picture%20changes%20during%20blinks%3A%20Looking%20without%20seeing%20and%20seeing%20without%20looking&amp;publication_year=2000&amp;author=J.%20O%E2%80%99Regan&amp;author=H.%20Deubel&amp;author=J.%20Clark&amp;author=R.%20Rensink">Google Scholar</a></div></dd><dt class="label"><a href="#bb0395" id="ref-id-b0395" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Parkhurst et al., 2002</a></dt><dd class="reference" id="h0395"><div class="contribution">D. Parkhurst, K. Law, E. Niebur<strong class="title">Modeling the role of salience in the allocation of overt visual attention</strong></div><div class="host">Vision Research, 42 (1) (2002), pp. 107-123</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036160528&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Modeling%20the%20role%20of%20salience%20in%20the%20allocation%20of%20overt%20visual%20attention&amp;publication_year=2002&amp;author=D.%20Parkhurst&amp;author=K.%20Law&amp;author=E.%20Niebur">Google Scholar</a></div></dd><dt class="label"><a href="#bb0400" id="ref-id-b0400" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Patla and Vickers, 1997</a></dt><dd class="reference" id="h0400"><div class="contribution">A. Patla, J. Vickers<strong class="title">Where and when do we look as we approach and step over an obstacle in the travel path?</strong></div><div class="host">Neuroreport, 8 (17) (1997), pp. 3661-3665</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0031465542&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Where%20and%20when%20do%20we%20look%20as%20we%20approach%20and%20step%20over%20an%20obstacle%20in%20the%20travel%20path">Google Scholar</a></div></dd><dt class="label"><a href="#bb0405" id="ref-id-b0405" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Patla and Vickers, 2003</a></dt><dd class="reference" id="h0405"><div class="contribution">A. Patla, J. Vickers<strong class="title">How far ahead do we look when required to step on specific locations in the travel path during locomotion?</strong></div><div class="host">Experimental Brain Research, 148 (1) (2003), pp. 133-138</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037213653&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=How%20far%20ahead%20do%20we%20look%20when%20required%20to%20step%20on%20specific%20locations%20in%20the%20travel%20path%20during%20locomotion">Google Scholar</a></div></dd><dt class="label"><a href="#bb0410" id="ref-id-b0410" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pelz and Canosa, 2001</a></dt><dd class="reference" id="h0410"><div class="contribution">J. Pelz, R. Canosa<strong class="title">Oculomotor behavior and perceptual strategies in complex tasks</strong></div><div class="host">Vision Research, 41 (25) (2001), pp. 3587-3596</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035179686&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Oculomotor%20behavior%20and%20perceptual%20strategies%20in%20complex%20tasks&amp;publication_year=2001&amp;author=J.%20Pelz&amp;author=R.%20Canosa">Google Scholar</a></div></dd><dt class="label"><a href="#bb0415" id="ref-id-b0415" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pieters et al., 1999</a></dt><dd class="reference" id="h0415"><div class="contribution">R. Pieters, E. Rosbergen, M. Wedel<strong class="title">Visual attention to repeated print advertising: A test of scanpath theory</strong></div><div class="host">Journal of Marketing Research, 36 (4) (1999), pp. 424-438</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033236253&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Visual%20attention%20to%20repeated%20print%20advertising%3A%20A%20test%20of%20scanpath%20theory&amp;publication_year=1999&amp;author=R.%20Pieters&amp;author=E.%20Rosbergen&amp;author=M.%20Wedel">Google Scholar</a></div></dd><dt class="label"><a href="#bb0420" id="ref-id-b0420" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Posner and Cohen, 1984</a></dt><dd class="reference" id="h0420"><div class="contribution">M. Posner, Y. Cohen<strong class="title">Components of visual orienting</strong></div><div class="host">Attention and Performance X: Control of Language Processes, 32 (1984), pp. 531-556</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0005613514&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Components%20of%20visual%20orienting&amp;publication_year=1984&amp;author=M.%20Posner&amp;author=Y.%20Cohen">Google Scholar</a></div></dd><dt class="label"><a href="#bb0425" id="ref-id-b0425" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Rabiner, 1990</a></dt><dd class="reference" id="h0425"><div class="contribution">L. Rabiner<strong class="title">A tutorial on hidden Markov models and selected applications in speech recognition</strong></div><div class="host">Readings in Speech Recognition, 53 (3) (1990), pp. 267-296</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052477053&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20tutorial%20on%20hidden%20Markov%20models%20and%20selected%20applications%20in%20speech%20recognition&amp;publication_year=1990&amp;author=L.%20Rabiner">Google Scholar</a></div></dd><dt class="label"><a href="#bb0430" id="ref-id-b0430" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Reinagel and Zador, 1999</a></dt><dd class="reference" id="h0430"><div class="contribution">P. Reinagel, A. Zador<strong class="title">Natural scene statistics at the centre of gaze</strong></div><div class="host">Network: Computation in Neural Systems, 10 (4) (1999), pp. 341-350</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0008183823&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Natural%20scene%20statistics%20at%20the%20centre%20of%20gaze&amp;publication_year=1999&amp;author=P.%20Reinagel&amp;author=A.%20Zador">Google Scholar</a></div></dd><dt class="label"><a href="#bb0435" id="ref-id-b0435" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Rothkopf et al., 2007</a></dt><dd class="reference" id="h0435"><div class="contribution">C. Rothkopf, D. Ballard, M. Hayhoe<strong class="title">Task and context determine where you look</strong></div><div class="host">Journal of Vision, 7 (14) (2007), p. 16</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Task%20and%20context%20determine%20where%20you%20look&amp;publication_year=2007&amp;author=C.%20Rothkopf&amp;author=D.%20Ballard&amp;author=M.%20Hayhoe">Google Scholar</a></div></dd><dt class="label"><a href="#bb0440" id="ref-id-b0440" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Rutishauser and Koch, 2007</a></dt><dd class="reference" id="h0440"><div class="contribution">U. Rutishauser, C. Koch<strong class="title">Probabilistic modeling of eye movement data during conjunction search via feature-based attention</strong></div><div class="host">Journal of Vision, 7 (6) (2007), p. 5</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1167/7.6.5">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Probabilistic%20modeling%20of%20eye%20movement%20data%20during%20conjunction%20search%20via%20feature-based%20attention&amp;publication_year=2007&amp;author=U.%20Rutishauser&amp;author=C.%20Koch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0445" id="ref-id-b0445" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Salvucci and Anderson, 2001</a></dt><dd class="reference" id="h0445"><div class="contribution">D. Salvucci, J. Anderson<strong class="title">Automated eye-movement protocol analysis</strong></div><div class="host">Human-Computer Interaction, 16 (1) (2001), pp. 39-86</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1207/S15327051HCI1601_2">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0034916517&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Automated%20eye-movement%20protocol%20analysis&amp;publication_year=2001&amp;author=D.%20Salvucci&amp;author=J.%20Anderson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0450" id="ref-id-b0450" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Salvucci and Goldberg, 2000</a></dt><dd class="reference" id="h0450"><div class="contribution">D. Salvucci, J. Goldberg<strong class="title">Identifying fixations and saccades in eye-tracking protocols</strong></div><div class="host">Proceedings of the 2000 symposium on Eye tracking research &amp; applications, ACM (2000), pp. 71-78</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/355017.355028">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0034592439&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Identifying%20fixations%20and%20saccades%20in%20eye-tracking%20protocols&amp;publication_year=2000&amp;author=D.%20Salvucci&amp;author=J.%20Goldberg">Google Scholar</a></div></dd><dt class="label"><a href="#bb0455" id="ref-id-b0455" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Schleicher et al., 2008</a></dt><dd class="reference" id="h0455"><div class="contribution">R. Schleicher, N. Galley, S. Briest, L. Galley<strong class="title">Blinks and saccades as indicators of fatigue in sleepiness warnings: Looking tired?</strong></div><div class="host">Ergonomics, 51 (7) (2008), pp. 982-1010</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/00140130701817062">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-46249112789&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Blinks%20and%20saccades%20as%20indicators%20of%20fatigue%20in%20sleepiness%20warnings:%20Looking%20tired">Google Scholar</a></div></dd><dt class="label"><a href="#bb0460" id="ref-id-b0460" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Schneider, 1995</a></dt><dd class="reference" id="h0460"><div class="contribution">W. Schneider<strong class="title">VAM: A neuro-cognitive model for visual attention control of segmentation, object recognition, and space-based motor action</strong></div><div class="host">Visual Cognition, 2 (2-3) (1995), pp. 331-376</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/13506289508401737">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0000761119&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=VAM%3A%20A%20neuro-cognitive%20model%20for%20visual%20attention%20control%20of%20segmentation%2C%20object%20recognition%2C%20and%20space-based%20motor%20action&amp;publication_year=1995&amp;author=W.%20Schneider">Google Scholar</a></div></dd><dt class="label"><a href="#bb0465" id="ref-id-b0465" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Schneider and Deubel, 2002</a></dt><dd class="reference" id="h0465"><div class="contribution">W. Schneider, H. Deubel<strong class="title">Selection-for-perception
 and selection-for-spatial-motor-action are coupled by visual attention:
 A review of recent findings and new evidence from stimulus-driven 
saccade control</strong></div><div class="host">Attention and Performance Xix: Common Mechanisms in Perception and Action (19) (2002), pp. 609-627</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036425189&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Selection-for-perception%20and%20selection-for-spatial-motor-action%20are%20coupled%20by%20visual%20attention%3A%20A%20review%20of%20recent%20findings%20and%20new%20evidence%20from%20stimulus-driven%20saccade%20control&amp;publication_year=2002&amp;author=W.%20Schneider&amp;author=H.%20Deubel">Google Scholar</a></div></dd><dt class="label"><a href="#bb0470" id="ref-id-b0470" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Simola et al., 2008</a></dt><dd class="reference" id="h0470"><div class="contribution">J. Simola, J. Salojärvi, I. Kojo<strong class="title">Using hidden Markov model to uncover processing states from eye movements in information search tasks</strong></div><div class="host">Cognitive Systems Research, 9 (4) (2008), pp. 237-251</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-50849109169&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Using%20hidden%20Markov%20model%20to%20uncover%20processing%20states%20from%20eye%20movements%20in%20information%20search%20tasks&amp;publication_year=2008&amp;author=J.%20Simola&amp;author=J.%20Saloj%C3%A4rvi&amp;author=I.%20Kojo">Google Scholar</a></div></dd><dt class="label"><a href="#bb0475" id="ref-id-b0475" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Smith and Henderson, 2009</a></dt><dd class="reference" id="h0475"><div class="contribution">T. Smith, J. Henderson<strong class="title">Facilitation of return during scene viewing</strong></div><div class="host">Visual Cognition, 17 (6-7) (2009), pp. 1083-1108</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1080/13506280802678557">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70549114772&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Facilitation%20of%20return%20during%20scene%20viewing&amp;publication_year=2009&amp;author=T.%20Smith&amp;author=J.%20Henderson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0480" id="ref-id-b0480" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Stark and Ellis, 1981</a></dt><dd class="reference" id="h0480"><div class="contribution">L.W. Stark, S. Ellis<strong class="title">Scanpaths revisited: Cognitive models direct active looking</strong></div><div class="host">D. Fisher, R. Monty, J. Senders (Eds.), Eye movements and psychological processes, Lawrence Erlbaum Associates, Hillsdale, NJ (1981), pp. 192-226</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Scanpaths%20revisited%3A%20Cognitive%20models%20direct%20active%20looking&amp;publication_year=1981&amp;author=L.W.%20Stark&amp;author=S.%20Ellis">Google Scholar</a></div></dd><dt class="label"><a href="#bb0485" id="ref-id-b0485" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tatler et al., 2006</a></dt><dd class="reference" id="h0485"><div class="contribution">B. Tatler, R. Baddeley, B. Vincent<strong class="title">The long and the short of it: Spatial statistics at fixation vary with saccade amplitude and task</strong></div><div class="host">Vision Research, 46 (12) (2006), pp. 1857-1862</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33344464967&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=The%20long%20and%20the%20short%20of%20it%3A%20Spatial%20statistics%20at%20fixation%20vary%20with%20saccade%20amplitude%20and%20task&amp;publication_year=2006&amp;author=B.%20Tatler&amp;author=R.%20Baddeley&amp;author=B.%20Vincent">Google Scholar</a></div></dd><dt class="label"><a href="#bb0490" id="ref-id-b0490" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tatler et al., 2011</a></dt><dd class="reference" id="h0490"><div class="contribution">B. Tatler, M. Hayhoe, M. Land, D. Ballard<strong class="title">Eye guidance in natural vision: Reinterpreting salience</strong></div><div class="host">Journal of Vision, 11 (5) (2011)</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20guidance%20in%20natural%20vision%3A%20Reinterpreting%20salience&amp;publication_year=2011&amp;author=B.%20Tatler&amp;author=M.%20Hayhoe&amp;author=M.%20Land&amp;author=D.%20Ballard">Google Scholar</a></div></dd><dt class="label"><a href="#bb0495" id="ref-id-b0495" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tatler and Vincent, 2008</a></dt><dd class="reference" id="h0495"><div class="contribution">B. Tatler, B. Vincent<strong class="title">Systematic tendencies in scene viewing</strong></div><div class="host">Journal of Eye Movement Research, 2 (2) (2008), pp. 1-18</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70449527968&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Systematic%20tendencies%20in%20scene%20viewing&amp;publication_year=2008&amp;author=B.%20Tatler&amp;author=B.%20Vincent">Google Scholar</a></div></dd><dt class="label"><a href="#bb0500" id="ref-id-b0500" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tatler et al., 2010</a></dt><dd class="reference" id="h0500"><div class="contribution">B. Tatler, N. Wade, H. Kwan, J. Findlay, B. Velichkovsky, <em> et al.</em><strong class="title">Yarbus, eye movements, and vision</strong></div><div class="host">I-Perception, 1 (1) (2010), p. 7</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1068/i0382">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052606486&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Yarbus%2C%20eye%20movements%2C%20and%20vision&amp;publication_year=2010&amp;author=B.%20Tatler&amp;author=N.%20Wade&amp;author=H.%20Kwan&amp;author=J.%20Findlay&amp;author=B.%20Velichkovsky">Google Scholar</a></div></dd><dt class="label"><a href="#bb0505" id="ref-id-b0505" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Torralba et al., 2006</a></dt><dd class="reference" id="h0505"><div class="contribution">A. Torralba, A. Oliva, M. Castelhano, J. Henderson<strong class="title">Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search</strong></div><div class="host">Psychological Review, 113 (4) (2006), pp. 766-786</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33750341577&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Contextual%20guidance%20of%20eye%20movements%20and%20attention%20in%20real-world%20scenes%3A%20The%20role%20of%20global%20features%20in%20object%20search&amp;publication_year=2006&amp;author=A.%20Torralba&amp;author=A.%20Oliva&amp;author=M.%20Castelhano&amp;author=J.%20Henderson">Google Scholar</a></div></dd><dt class="label"><a href="#bb0510" id="ref-id-b0510" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Treisman and Gelade, 1980</a></dt><dd class="reference" id="h0510"><div class="contribution">A. Treisman, G. Gelade<strong class="title">A feature-integration theory of attention</strong></div><div class="host">Cognitive Psychology, 12 (1) (1980), pp. 97-136</div><div class="comment">ISSN 0010-028</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0018878142&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=A%20feature-integration%20theory%20of%20attention&amp;publication_year=1980&amp;author=A.%20Treisman&amp;author=G.%20Gelade">Google Scholar</a></div></dd><dt class="label"><a href="#bb0515" id="ref-id-b0515" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Van Der Lans et al., 2008</a></dt><dd class="reference" id="h0515"><div class="contribution">R. Van Der Lans, R. Pieters, M. Wedel<strong class="title">Eye-movement analysis of search effectiveness</strong></div><div class="host">Journal of the American Statistical Association, 103 (482) (2008), pp. 452-461</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1198/016214507000000437">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-49549111808&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye-movement%20analysis%20of%20search%20effectiveness&amp;publication_year=2008&amp;author=R.%20Van%20Der%20Lans&amp;author=R.%20Pieters&amp;author=M.%20Wedel">Google Scholar</a></div></dd><dt class="label"><a href="#bb0520" id="ref-id-b0520" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Vidal et al., 2012</a></dt><dd class="reference" id="h0520"><div class="contribution">M. Vidal, J. Turner, A. Bulling, H. Gellersen<strong class="title">Wearable eye tracking for mental health monitoring</strong></div><div class="host">Computer Communications, 35 (11) (2012), pp. 1306-1311</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84861998404&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Wearable%20eye%20tracking%20for%20mental%20health%20monitoring&amp;publication_year=2012&amp;author=M.%20Vidal&amp;author=J.%20Turner&amp;author=A.%20Bulling&amp;author=H.%20Gellersen">Google Scholar</a></div></dd><dt class="label"><a href="#bb0525" id="ref-id-b0525" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Walther and Koch, 2006</a></dt><dd class="reference" id="h0525"><div class="contribution">D. Walther, C. Koch<strong class="title">Modeling attention to salient proto-objects</strong></div><div class="host">Neural Networks, 19 (9) (2006), pp. 1395-1407</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33750684017&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Modeling%20attention%20to%20salient%20proto-objects&amp;publication_year=2006&amp;author=D.%20Walther&amp;author=C.%20Koch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0530" id="ref-id-b0530" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Wischnewski et al., 2010</a></dt><dd class="reference" id="h0530"><div class="contribution">M. Wischnewski, A. Belardinelli, W. Schneider, J. Steil<strong class="title">Where to look next? Combining static and dynamic proto-objects in a TVA-based model of visual attention</strong></div><div class="host">Cognitive Computation, 2 (4) (2010), pp. 326-343</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/s12559-010-9080-1">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78649916078&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar?q=Where%20to%20look%20next%20Combining%20static%20and%20dynamic%20proto-objects%20in%20a%20TVA-based%20model%20of%20visual%20attention">Google Scholar</a></div></dd><dt class="label"><a href="#bb0535" id="ref-id-b0535" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Wischnewski et al., 2009</a></dt><dd class="reference" id="h0535"><div class="contribution">M. Wischnewski, J. Steil, L. Kehrer, W. Schneider<strong class="title">Integrating inhomogeneous processing and proto-object formation in a computational model of visual attention</strong></div><div class="host">H. Ritter (Ed.), Human centered robot systems, Vol. 6, Springer (2009), pp. 93-102</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1007/978-3-642-10403-9_10">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78649917653&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Integrating%20inhomogeneous%20processing%20and%20proto-object%20formation%20in%20a%20computational%20model%20of%20visual%20attention&amp;publication_year=2009&amp;author=M.%20Wischnewski&amp;author=J.%20Steil&amp;author=L.%20Kehrer&amp;author=W.%20Schneider">Google Scholar</a></div></dd><dt class="label"><a href="#bb0540" id="ref-id-b0540" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Wojciulik et al., 1998</a></dt><dd class="reference" id="h0540"><div class="contribution">E. Wojciulik, N. Kanwisher, J. Driver<strong class="title">Covert visual attention modulates face-specific activity in the human fusiform gyrus: fMRI study</strong></div><div class="host">Journal of Neurophysiology, 79 (3) (1998), pp. 1574-1578</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1152/jn.1998.79.3.1574">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0031893479&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Covert%20visual%20attention%20modulates%20face-specific%20activity%20in%20the%20human%20fusiform%20gyrus%3A%20fMRI%20study&amp;publication_year=1998&amp;author=E.%20Wojciulik&amp;author=N.%20Kanwisher&amp;author=J.%20Driver">Google Scholar</a></div></dd><dt class="label"><a href="#bb0545" id="ref-id-b0545" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Wolfe et al., 1989</a></dt><dd class="reference" id="h0545"><div class="contribution">J. Wolfe, K. Cave, S. Franzel<strong class="title">Guided search: An alternative to the feature integration model for visual search</strong></div><div class="host">Journal of Experimental Psychology: Human perception and performance, 15 (3) (1989), pp. 419-433</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0024712254&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Guided%20search%3A%20An%20alternative%20to%20the%20feature%20integration%20model%20for%20visual%20search&amp;publication_year=1989&amp;author=J.%20Wolfe&amp;author=K.%20Cave&amp;author=S.%20Franzel">Google Scholar</a></div></dd><dt class="label"><a href="#bb0550" id="ref-id-b0550" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Xu and Wunsch, 2005</a></dt><dd class="reference" id="h0550"><div class="contribution">R. Xu, D. Wunsch<strong class="title">Survey of clustering algorithms</strong></div><div class="host">IEEE Transactions on Neural Networks, 16 (3) (2005), pp. 645-678</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/TNN.2005.845141">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-16444383160&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Survey%20of%20clustering%20algorithms&amp;publication_year=2005&amp;author=R.%20Xu&amp;author=D.%20Wunsch">Google Scholar</a></div></dd><dt class="label"><a href="#bb0555" id="ref-id-b0555" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Yarbus, 1967</a></dt><dd class="reference" id="h0555"><div class="contribution">A. Yarbus</div><div class="host">Eye movements and vision, Plenum Press, New York (1967)</div><div class="comment">(Translated from the Russian edition by Haigh, B)</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20movements%20and%20vision&amp;publication_year=1967&amp;author=A.%20Yarbus">Google Scholar</a></div></dd><dt class="label"><a href="#bb0560" id="ref-id-b0560" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zelinsky et al., 1997</a></dt><dd class="reference" id="h0560"><div class="contribution">G. Zelinsky, R. Rao, M. Hayhoe, D. Ballard<strong class="title">Eye movements reveal the spatiotemporal dynamics of visual search</strong></div><div class="host">Psychological Science, 8 (6) (1997), pp. 448-453</div><div class="ReferenceLinks u-font-sans"><a class="link" target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1111/j.1467-9280.1997.tb00459.x">CrossRef</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0009399972&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class="link" target="_blank" rel="noopener noreferrer" href="https://scholar.google.com/scholar_lookup?title=Eye%20movements%20reveal%20the%20spatiotemporal%20dynamics%20of%20visual%20search&amp;publication_year=1997&amp;author=G.%20Zelinsky&amp;author=R.%20Rao&amp;author=M.%20Hayhoe&amp;author=D.%20Ballard">Google Scholar</a></div></dd></dl></section></section><div class="Footnotes"><dl class="footnote"><dt class="footnote-label"><sup><a href="#bfn1">1</a></sup></dt><dd class="u-margin-xxl-left"><p id="np005">Repetitive and idiosyncratic eye trajectories during a recognition task is called scanpath (<a name="bb0380" href="#b0380" class="workspace-trigger">Noton &amp; Stark, 1971</a>).</p></dd></dl><dl class="footnote"><dt class="footnote-label"><sup><a href="#bfn2">2</a></sup></dt><dd class="u-margin-xxl-left"><p id="np010">“Strictly
 speaking, an ergodic model has the property that every state can be 
reached from every other state in a finite number of steps.” <a name="bb0425" href="#b0425" class="workspace-trigger">Rabiner (1990)</a>.</p></dd></dl></div><div class="Copyright"><span class="copyright-line">Copyright © 2014 Elsevier Ltd. All rights reserved.</span></div></article><div class="u-show-from-md col-lg-6 col-md-8 pad-right"><aside class="RelatedContent" aria-label="Related content"><section class="SidePanel u-margin-s-bottom"><header id="recommended-articles-header" class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text"><h2 class="section-title u-h4">Recommended articles</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles"><ul><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0042698914000704"><h3 class="article-title ellipsis text-s" id="recommended-articles-article0-title" title="Asymmetrical control of fixation durations in scene viewing"><span>Asymmetrical control of fixation durations in scene viewing</span></h3></a><div class="article-source ellipsis"><div class="source">Vision Research, Volume 100, 2014, pp. 38-46</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S0042698914000704/pdfft?md5=01520db0c6921377c97fa440a8da96c0&amp;pid=1-s2.0-S0042698914000704-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby="recommended-articles-article0-title" aria-controls="recommended-articles-article0" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article0" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0042698914003071"><h3 class="article-title ellipsis text-s" id="recommended-articles-article1-title" title="On the possible roles of microsaccades and drifts in visual perception"><span>On the possible roles of microsaccades and drifts in visual perception</span></h3></a><div class="article-source ellipsis"><div class="source">Vision Research, Volume 118, 2016, pp. 25-30</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S0042698914003071/pdfft?md5=dfc6ea1894c0e900f610cfcf9b32be41&amp;pid=1-s2.0-S0042698914003071-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby="recommended-articles-article1-title" aria-controls="recommended-articles-article1" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article1" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0042698914002818"><h3 class="article-title ellipsis text-s" id="recommended-articles-article2-title" title="Goal-oriented gaze strategies afforded by object interaction"><span>Goal-oriented gaze strategies afforded by object interaction</span></h3></a><div class="article-source ellipsis"><div class="source">Vision Research, Volume 106, 2015, pp. 47-57</div></div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S0042698914002818/pdfft?md5=67f92361e0a9a920f1f2013af3c32303&amp;pid=1-s2.0-S0042698914002818-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby="recommended-articles-article2-title" aria-controls="recommended-articles-article2" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="recommended-articles-article2" aria-hidden="true"></div></li></ul></div><div class="pagination u-position-relative u-padding-s-bottom"><span class="u-position-absolute"></span><span class="pagination-pages-label"><span class="pagination-nav u-margin-xs-hor pagination-current underline-page-number">1</span><span class="pagination-nav u-margin-xs-hor">2</span></span><span class="u-position-absolute"><button class="button-link button-link-secondary next-button" data-aa-button="sd:product:journal:article:location=recommended-articles:type=Next" type="button"><span class="button-link-text">Next</span><svg focusable="false" viewBox="0 0 54 128" width="10.125" height="24" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></button></span></div></div></section><section class="SidePanel u-margin-s-bottom"><header id="citing-articles-header" class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle button-link-primary" aria-expanded="false" data-aa-button="sd:product:journal:article:location=citing-articles:type=open" type="button"><span class="button-link-text"><h2 class="section-title u-h4">Citing articles (29)</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="u-display-none" aria-hidden="true" aria-describedby="citing-articles-header"><div id="citing-articles"><ul><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0018506X19300534" target="_self"><h3 class="article-title ellipsis text-s" id="citing-articles-article0-title" title="Visual cues to fertility are in the eye (movements) of the beholder">Visual cues to fertility are in the eye (movements) of the beholder</h3></a><div class="article-source ellipsis">2019, Hormones and Behavior</div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S0018506X19300534/pdfft?md5=533f8c3486242049d6da292f9df70995&amp;pid=1-s2.0-S0018506X19300534-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article0-title" aria-controls="citing-articles-article0" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="citing-articles-article0" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S2352154616301255" target="_self"><h3 class="article-title ellipsis text-s" id="citing-articles-article1-title" title="Theoretical perspectives on active sensing">Theoretical perspectives on active sensing</h3></a><div class="article-source ellipsis">2016, Current Opinion in Behavioral Sciences</div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S2352154616301255/pdfft?md5=31247c39debe82081b09d1e60241d2bf&amp;pid=1-s2.0-S2352154616301255-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article1-title" aria-controls="citing-articles-article1" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="citing-articles-article1" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0925231216304131" target="_self"><h3 class="article-title ellipsis text-s" id="citing-articles-article2-title" title="Predicting task from eye movements: On the importance of spatial distribution, dynamics, and image features">Predicting task from eye movements: On the importance of spatial distribution, dynamics, and image features</h3></a><div class="article-source ellipsis">2016, Neurocomputing</div></div><div class="buttons"><a class="anchor side-panel-pdf-link" href="https://www.sciencedirect.com/science/article/pii/S0925231216304131/pdfft?md5=0d4f859451851e3efa4537abf731ac07&amp;pid=1-s2.0-S0925231216304131-main.pdf" target="_blank" rel="nofollow"><svg focusable="false" viewBox="0 0 32 32" width="24" height="24" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" stroke="#aaa" stroke-width=".315" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text">Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article2-title" aria-controls="citing-articles-article2" aria-expanded="false" type="button"><span class="button-link-text">View details</span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id="citing-articles-article2" aria-hidden="true"></div></li></ul><a class="anchor side-panel-view-more" href="http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-84908060981&amp;md5=8331683aa0ffb5b950359f2a2cb923eb" target="_blank"><span class="anchor-text">View more articles</span><svg focusable="false" viewBox="0 0 54 128" width="10.125" height="24" class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a></div></div></section><section class="SidePanel u-margin-s-bottom hidden"><header id="metrics-header" class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded="true" type="button"><span class="button-link-text"><h2 class="section-title u-h4">Article Metrics</h2></span><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><a href="https://plu.mx/plum/a/?doi=10.1016/j.visres.2014.08.014" class="plumx-summary plum-sciencedirect-theme" data-pass-hidden-categories="true" data-hide-usage="true" data-orientation="vertical" data-hide-print="true" data-site="plum" data-on-success="onMetricsWidgetSuccess">View article metrics</a></div></section></aside></div></div><div></div></div><div id="footer" role="contentinfo"><div class="hor-line-top u-padding-s-top u-bg-white u-clr-grey7" style="border-color:#e9711c"></div><a class="anchor u-padding-s-left move-left els-footer-elsevier" href="https://www.elsevier.com/" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text"><svg viewBox="-3345 3440.027 140.01 24.333" style="width:104px;height:30px"><title>Elsevier</title><path id="E" style="fill:#E9711C" d="M-3343.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3344L-3343.999,3461.698"></path><path style="fill:#E9711C" d="M-3325.448,3461.698c2.074-0.118,2.83-0.502,2.83-2.832v-13.511c0-2.33-0.757-2.715-2.83-2.832v-0.591h8.884 v0.591c-2.243,0-3.027,0.472-3.027,2.891v13.009c0,1.652,0.056,2.625,1.71,2.625h4.008c3,0,4.4-2,5.576-4.1l0.673,0.118 l-1.71,5.222h-16.114V3461.698"></path><path style="fill:#E9711C" d="M-3307.122,3456.27h0.561c1.12,2.596,2.886,5.4,5.94,5.4c2,0,3.672-1.3,3.672-3.334 c0-1.652-1.626-3.1-4.176-4.927c-3.28-2.36-5.41-3.746-5.41-6.43c0-3.422,2.55-5.517,5.633-5.517c2.214,0,3,0.944,4.204,0.944 c0.476,0,0.56-0.266,0.476-0.737h0.561l0.645,6.076h-0.561c-0.785-2.625-2.523-5.19-5.354-5.19c-1.737,0-3.138,1.356-3.138,3.185 c0,1.918,1.933,3.157,5.016,5.016c2.523,1.504,5,3.362,5,6.254c0,3.245-2.608,5.752-5.97,5.752c-2.02,0-4.148-0.855-4.68-0.855 c-0.336,0-0.7,0.207-0.756,0.649h-0.56l-1.094-6.282"></path><path style="fill:#E9711C" d="M-3293.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3294L-3293.999,3461.698"></path><path style="fill:#E9711C" d="M-3265.839,3462.524h-0.42l-5.41-12.538c-0.896-2.065-1.71-4.16-2.83-6.136 c-0.478-0.826-1.346-1.327-2.3-1.327v-0.591h8.323v0.591c-0.785,0-2.354,0-2.354,1.15c0,0.384,0.87,2.45,1.653,4.308l4.063,9.676 l4.877-11.359c0.588-1.356,0.757-2.094,0.757-2.713c0-0.618-0.673-0.974-2.13-1.062v-0.59h5.941v0.591 c-0.337,0.06-0.7,0.117-1.037,0.295c-1.066,0.56-2.13,3.57-2.635,4.749l-6.5,14.957"></path><path style="fill:#E9711C" d="M-3255.472,3461.698c2.24,0,3.025-0.473,3.025-2.892v-13.393c0-2.42-0.784-2.89-3.025-2.891v-0.59h9.078v0.591 c-2.24,0-3.025,0.472-3.025,2.891v13.393c0,2.42,0.784,2.892,3.025,2.892v0.59h-9.08v-0.59"></path><path id="E_2_" style="fill:#E9711C" d="M-3244.999,3461.698c2.24,0,3.026-0.473,3.026-2.892v-13.393c0-2.42-0.785-2.89-3.026-2.891v-0.59 h16.787l0.252,4.455h-0.56c-0.308-2.48-1.513-3.216-3.84-3.216h-5.325c-1.26,0-1.26,0.355-1.26,2.066v5.693h5.913 c1.934,0,2.522-0.826,2.718-2.803h0.56v6.844h-0.56c-0.168-1.946-0.813-2.802-2.718-2.802h-5.913v6.401 c0,1.858,0.11,2.476,1.4,2.476h5.914c2.522,0,4.092-1.62,4.82-3.952l0.532,0.207l-1.513,4.985H-3245L-3244.999,3461.698"></path><path style="fill:#E9711C" d="M-3206,3461.698c-1.26-0.354-1.71-0.68-2.466-1.623l-6.166-7.609c3.027-0.65,5.13-2.185,5.13-5.547 c0-4.75-4.26-4.986-7.764-4.986h-9.191v0.591c2.24,0,3.026,0.472,3.026,2.891v13.393c0,2.42-0.785,2.892-3.026,2.892v0.59h9.08 v-0.59c-2.242,0-3.027-0.473-3.027-2.892v-5.604h2.551l7.314,9.086h4.54L-3206,3461.698 M-3220.399,3444.499 c0-1.387,0.337-1.476,2.186-1.476c2.774,0,5.3,0.974,5.3,4.308c0,3.6-2.887,4.63-5.914,4.631h-1.569v-7.463H-3220.399z"></path></svg></span></a><div class="panel-s u-bg-white u-padding-s-hor-from-md u-padding-xs-ver text-xs u-clear-both-from-xs u-clear-none-from-md"><ul class="u-margin-xs-bottom" style="list-style:none"><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/solutions/sciencedirect" id="els-footer-about-science-direct" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">About ScienceDirect</span></a><wbr></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.sciencedirect.com/customer/authenticate/manra" id="els-footer-remote-access" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Remote access</span></a><wbr></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://sd-cart.elsevier.com/?" id="els-footer-shopping-cart" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Shopping cart</span></a></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="http://elsmediakits.com/" id="els-footer-advertise" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Advertise</span></a><wbr></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" id="els-footer-contact-support" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Contact and support</span></a><wbr></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" id="els-footer-terms-condition" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Terms and conditions</span></a><wbr></li><li class="u-display-inline"><a class="anchor u-margin-xs-right u-margin-s-right-from-sm u-margin-l-right-from-md anchor-has-inherit-color" href="https://www.elsevier.com/legal/privacy-policy" id="els-footer-privacy-policy" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text">Privacy policy</span></a></li></ul><p id="els-footer-cookie-message">We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the <a class="anchor u-margin-0-right" href="https://www.sciencedirect.com/legal/use-of-cookies" target="_blank" rel="nofollow" style="white-space: nowrap;"><span class="anchor-text">use of cookies</span></a>.</p><p id="els-footer-copyright">Copyright © 2019 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.</p></div><a class="anchor u-padding-l-bottom u-padding-s-hor u-position-relative move-bottom u-float-left-from-xs u-float-right-from-md move-right u-padding-0-hor u-margin-top-xs els-footer-relx" href="https://www.relx.com/" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx-group" target="_blank" rel="nofollow" style="white-space:nowrap"><span class="anchor-text"><svg xmlns="http://www.w3.org/2000/svg" width="78" height="30" version="1" viewBox="0 0 280 65" alt="RELX group home page"><g transform="matrix(.13333 0 0 -.13333 0 65)"><path fill="#f4741e" d="M207 251c110 0 242 26 242 131 0 75-77 103-146 103C196 485 0 425 0 215 0 110 80 49 208 49c135 0 232 87 262 205C391 118 291 75 209 75 107 75 68 151 68 216c0 185 144 248 235 248 61 0 97-40 97-83 0-119-172-116-250-116h-38l-16-17c36-5 85-15 139-42C347 149 453 0 592 0c51 0 64 15 74 29-69-44-183 32-245 90-51 48-96 103-214 132"></path><path fill="#666666" d="M886 310c0 33-22 53-55 53h-77c-2 0-3-1-3-3v-99c0-2 1-3 3-3h77c33 0 55 20 55 52m7-261c-5 0-7 2-8 6l-69 145h-62c-2 0-3-1-3-4V55c0-4-2-6-6-6h-51c-4 0-6 2-6 6v361c0 3 2 5 6 5h139c66 0 115-45 115-111 0-49-27-86-69-102l76-152c2-4 0-7-4-7h-58M1010 416c0 3 3 5 6 5h218c4 0 6-2 6-5v-47c0-3-2-6-6-6h-158c-2 0-3-1-3-3v-91c0-2 1-3 3-3h127c4 0 6-2 6-6v-47c0-3-2-5-6-5h-127c-2 0-3-1-3-3v-95c0-2 1-3 3-3h158c4 0 6-2 6-5V55c0-4-2-6-6-6h-218c-3 0-6 2-6 6v361M1298 416c0 3 2 5 5 5h52c3 0 6-2 6-5V110c0-2 1-3 3-3h147c3 0 5-2 5-5V55c0-4-2-6-5-6h-208c-3 0-5 2-5 6v361M1775 49c-4 0-6 1-8 5l-73 127h-1l-73-127c-2-4-4-5-8-5h-58c-3 0-5 3-3 7l108 185-100 173c-2 4 0 7 3 7h58c4 0 6-2 8-6l65-112h1l65 112c2 4 5 6 9 6h57c4 0 5-3 3-7l-99-173 107-185c2-4 1-7-3-7h-58M1912 295l-2 1v104l-1 1h-32l-2 2v16l2 2h87l2-2v-16l-2-2h-32l-1-1V296l-2-1h-17M1988 419l2 2h16c1 0 2 0 3-2l33-78h1l33 78c1 2 2 2 3 2h16l2-2V296l-2-1h-16l-1 1v78h-1l-26-60-3-2h-11l-3 2-26 60h-1v-78l-2-1h-15l-2 1v123"></path></g></svg></span></a></div></section></div></div></div>
<script type="application/json" data-iso-key="_0">{"userAgent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36","abstracts":{"content":[{"#name":"abstract","$":{"xmlns:ce":true,"class":"author-highlights","lang":"en","id":"ab005","view":"all"},"$$":[{"#name":"section-title","$":{"id":"st075"},"_":"Highlights"},{"#name":"abstract-sec","$":{"id":"as005","view":"all"},"$$":[{"#name":"simple-para","$":{"id":"sp005","view":"all"},"$$":[{"#name":"list","$":{"id":"l0005"},"$$":[{"#name":"list-item","$":{"id":"u0005"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0475","view":"all"},"_":"A computational model is developed for visual-task inference given eye trajectories."}]},{"#name":"list-item","$":{"id":"u0010"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0480","view":"all"},"_":"Hidden Markov Models are used to predict fixation coordinates contingent on task."}]},{"#name":"list-item","$":{"id":"u0015"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0485","view":"all"},"_":"The model allows for both overt and covert shifts of attention."}]},{"#name":"list-item","$":{"id":"u0020"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0490","view":"all"},"_":"The model locates attended targets and identifies the task."}]},{"#name":"list-item","$":{"id":"u0025"},"$$":[{"#name":"label","_":"•"},{"#name":"para","$":{"id":"p0495","view":"all"},"_":"The results support the findings of Yarbus on task-dependent eye movements."}]}]}]}]}]},{"#name":"abstract","$":{"xmlns:ce":true,"class":"author","lang":"en","id":"ab010","view":"all"},"$$":[{"#name":"section-title","$":{"id":"st080"},"_":"Abstract"},{"#name":"abstract-sec","$":{"id":"as010","view":"all"},"$$":[{"#name":"simple-para","$":{"id":"sp010","view":"all"},"_":"In this paper we develop a probabilistic method to infer the visual-task of a viewer given measured eye movement trajectories. This method is based on the theory of hidden Markov models (HMM) that employs a first order Markov process to predict the coordinates of fixations given the task. The prediction confidence level of each task-dependent model is used in a Bayesian inference formulation, whereby the task with the maximum a posteriori (MAP) probability is selected. We applied this technique to a challenging dataset consisting of eye movement trajectories obtained from subjects viewing monochrome images of real scenes tasked with answering questions regarding the scenes. The results show that the HMM approach, combined with a clustering technique, can be a reliable way to infer visual-task from eye movements data."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"biographies":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"kg005","class":"keyword","view":"all"},"$$":[{"#name":"section-title","$":{"id":"st090"},"_":"Keywords"},{"#name":"keyword","$":{"id":"k0005"},"$$":[{"#name":"text","_":"Visual-task inference"}]},{"#name":"keyword","$":{"id":"k0010"},"$$":[{"#name":"text","_":"Attention cognitive model"}]},{"#name":"keyword","$":{"id":"k0015"},"$$":[{"#name":"text","$$":[{"#name":"italic","_":"K"},{"#name":"__text__","_":"-means clustering"}]}]},{"#name":"keyword","$":{"id":"k0020"},"$$":[{"#name":"text","_":"Visual search"}]},{"#name":"keyword","$":{"id":"k0025"},"$$":[{"#name":"text","_":"Eye movement"}]},{"#name":"keyword","$":{"id":"k0030"},"$$":[{"#name":"text","_":"Hidden Markov model"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"rawtext":"","authors":{"content":[{"#name":"author-group","$":{"xmlns:ce":true,"id":"ag005"},"$$":[{"#name":"author","$":{"id":"au005"},"$$":[{"#name":"given-name","_":"Amin"},{"#name":"surname","_":"Haji-Abolhassani"},{"#name":"cross-ref","$":{"id":"ar005","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"e-address","$":{"id":"em005","type":"email"},"_":"amin@cim.mcgill.ca"}]},{"#name":"author","$":{"id":"au010"},"$$":[{"#name":"given-name","_":"James J."},{"#name":"surname","_":"Clark"},{"#name":"e-address","$":{"id":"em010","type":"email"},"_":"clark@cim.mcgill.ca"}]},{"#name":"affiliation","$":{"id":"af005"},"$$":[{"#name":"textfn","_":"Centre for Intelligent Machines, Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec H3A 0E9, Canada"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Centre for Intelligent Machines"},{"#name":"organization","_":"Department of Electrical and Computer Engineering"},{"#name":"organization","_":"McGill University"},{"#name":"city","_":"Montreal"},{"#name":"state","_":"Quebec"},{"#name":"postal-code","_":"H3A 0E9"},{"#name":"country","_":"Canada"}]}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"affiliations":{"af005":{"#name":"affiliation","$":{"id":"af005"},"$$":[{"#name":"textfn","_":"Centre for Intelligent Machines, Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec H3A 0E9, Canada"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Centre for Intelligent Machines"},{"#name":"organization","_":"Department of Electrical and Computer Engineering"},{"#name":"organization","_":"McGill University"},{"#name":"city","_":"Montreal"},{"#name":"state","_":"Quebec"},{"#name":"postal-code","_":"H3A 0E9"},{"#name":"country","_":"Canada"}]}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"body":{},"exam":{},"article":{"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Pergamon","id":"67"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"Partial","oaArticleCount":7753,"openArchiveStatus":true,"openArchiveArticleCount":7638,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S0042698914X0009X-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698914X0009X/cover/DOWNSAMPLED200/image/gif/ef8fc69072d89d1f5d9593e28e54e492/cov200h.gif","https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698914X0009X/cover/DOWNSAMPLED200/image/gif/ef8fc69072d89d1f5d9593e28e54e492/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"11305","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S0042698914X0009X-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0042698914X0009X/cover/DOWNSAMPLED/image/gif/a827e4bd21d8e8c8ccd23c597631344d/cov150h.gif","https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0042698914X0009X/cover/DOWNSAMPLED/image/gif/a827e4bd21d8e8c8ccd23c597631344d/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"7239","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S00426989.gif","title":"vision-research","contentTypeCode":"JL","sourceOpenAccess":false,"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S0042698914X0009X-cov150h.gif"},"pii":"S0042698914002004","dates":{"Available online":"28 August 2014","Received":"25 August 2013","Revised":["30 July 2014"],"Publication date":"1 October 2014"},"access":{"openArchive":true,"openAccess":true,"sponsorType":"ElsevierBranded","license":"http://www.elsevier.com/open-access/userlicense/1.0/"},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"document-references":112,"analyticsMetadata":{"accountId":"20840","accountName":"University of Nebraska-Lincoln Love Library","loginStatus":"anonymous","userId":"437158","isLoggedIn":false},"cid":"271122","content-family":"serial","copyright-line":"Copyright © 2014 Elsevier Ltd. All rights reserved.","cover-date-years":["2014"],"cover-date-start":"2014-10-01","cover-date-text":"October 2014","document-subtype":"fla","document-type":"article","entitledToken":"4CF45A2B508BBA9FB737C15D050404995DC017486B8F92717D3CFF903356731D90A8BE5A2A97AFCC","eid":"1-s2.0-S0042698914002004","doi":"10.1016/j.visres.2014.08.014","first-fp":"127","hub-eid":"1-s2.0-S0042698914X0009X","issuePii":"S0042698914X0009X","item-weight":"FULL-TEXT","language":"en","last-lp":"142","last-author":{"#name":"last-author","$":{"xmlns:dm":true},"$$":[{"#name":"author","$":{"xmlns:ce":true,"id":"au010"},"$$":[{"#name":"given-name","_":"James J."},{"#name":"surname","_":"Clark"},{"#name":"e-address","$":{"id":"em010","type":"email"},"_":"clark@cim.mcgill.ca"}]}]},"normalized-first-auth-initial":"A","normalized-first-auth-surname":"HAJIABOLHASSANI","pages":[{"last-page":"142","first-page":"127"}],"srctitle":"Vision Research","suppl":"C","timestamp":"2016-03-31T21:22:41.104776-04:00","title":{"content":[{"#name":"title","$":{"xmlns:ce":true,"id":"tm005"},"_":"An inverse Yarbus process: Predicting observers’ task from eye movement patterns"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"103","vol-iss-suppl-text":"Volume 103","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":true,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"issn":"00426989","issn-primary-formatted":"0042-6989","useEnhancedReader":true,"isCorpReq":false,"pdfDownload":{"linkType":"DOWNLOAD","linkToPdf":"/science/article/pii/S0042698914002004/pdfft?md5=559279836fc0333adb4ec89bc86fcda4&pid=1-s2.0-S0042698914002004-main.pdf","isPdfFullText":false,"fileName":"1-s2.0-S0042698914002004-main.pdf"},"pdfUrlForCrawlers":"https://www.sciencedirect.com/science/article/pii/S0042698914002004/pdfft?md5=559279836fc0333adb4ec89bc86fcda4&pid=1-s2.0-S0042698914002004-main.pdf","indexTag":true,"volRange":"103","issRange":"","freeHtmlGiven":false,"userProfile":{"departmentName":"Lincoln Library","accessType":"IPRANGE","accountId":"20840","webUserId":"437158","libraryBanner":{"text":"University of Nebraska-Lincoln","position":"RIGHT","url":"http://iris.unl.edu"},"accountName":"University of Nebraska-Lincoln Love Library","departmentId":"32570","userType":"NORMAL","hasMultipleOrganizations":false},"entitlementReason":"subscription","articleEntitlement":{"entitled":true,"usageInfo":"(437158,U|32570,D|20840,A|24513,S|34,P|2,PL)(SDFE,CON|bccd630f9dfc664da2396397f6f8fa6e0e4agxrqa,SSO|ANON_IP,ACCESS_TYPE)"},"aipType":"none","hasChorus":false,"downloadFullIssue":true,"headerConfig":{"helpUrl":"https://service.elsevier.com/app/home/supporthub/sciencedirect/","contactUrl":"https://service.elsevier.com/app/contact/supporthub/sciencedirect/","userName":"","userEmail":"","orgName":"University of Nebraska-Lincoln Love Library","webUserId":"437158","libraryBanner":{"libraryBannerText":"University of Nebraska-Lincoln","libraryBannerUrl":"http://iris.unl.edu","position":"RIGHT"},"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasMultiOrg":false,"userType":"IPRANGE","allowCart":false,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-us-east-1.sciencedirectassets.com"},"self-archiving":{},"titleString":"An inverse Yarbus process: Predicting observers’ task from eye movement patterns","onAbstractWhitelist":false,"isAbstract":false,"isContentVisible":false,"ajaxLinks":{"citingArticles":true,"references":true,"referredToBy":true,"toc":true,"body":true,"recommendations":true}},"specialIssueArticles":{},"recommendations":{},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"citingArticles":{},"workspace":{"isOpen":false},"crossMark":{"isOpen":false},"userIdentity":{},"refersTo":{},"referredToBy":{},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[]},"references":{},"referenceLinks":{"internal":{},"external":{}},"glossary":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenRecommendations":true,"isOpenCitingArticles":false,"citingArticles":[false,false,false],"recommendations":[false,false,false,false,false,false],"specialIssueArticles":[false,false,false]},"banner":{"expanded":false},"transientError":{"isOpen":false},"tableOfContents":{"showEntitledTocLinks":true},"chapters":{"toc":[],"isLoading":false},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"metrics":{"isOpen":true},"signOut":{"isOpen":false},"issueNavigation":{"previous":{},"next":{}},"tail":{},"linkingHubLinks":[],"signInFromEmail":{"isOpen":false},"supplementaryFilesData":[],"clinicalKey":{},"accessOptions":{},"changeViewLinks":{"showFullTextLink":false,"showAbstractLink":false}}</script>
<script src="satelliteLib-b7cfe8df39a4e5eec5536bba80e13f4b6fa0dd7c.js" type="text/javascript"></script><script src="satellite-565e008964746d4385002642.js"></script>
<script src="https://sdfestaticassets-us-east-1.sciencedirectassets.com/shared-assets/10/js/babel-polyfill/6.26.0/babel-polyfill.min.js" type="text/javascript"></script>
<script src="https://sdfestaticassets-us-east-1.sciencedirectassets.com/shared-assets/18/js/react/16.4.2/react.production.min.js" type="text/javascript"></script>
<script src="https://sdfestaticassets-us-east-1.sciencedirectassets.com/shared-assets/18/js/react-dom/16.4.2/react-dom.production.min.js" type="text/javascript"></script>
<script async="" src="https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/72b3c75e9c9c2dbf390ead8825e79d3a0cba3150/arp.js" type="text/javascript"></script>

<script type="text/javascript">
        window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
        window.usabilla_live = lightningjs.require("usabilla_live", "https://w.usabilla.com/eb1c14a91932.js");
        var customData = {};

        if(window.pageData && pageData.content && pageData.content[0]) {
          customData.entitlementType = pageData.content[0].entitlementType;
        }
        if(window.pageData && pageData.visitor) {
          customData.accessType = pageData.visitor.accessType;
          customData.accountId = pageData.visitor.accountId;
          customData.loginStatus = pageData.visitor.loginStatus;
        }
        usabilla_live("data", {"custom": customData });
      </script>

<script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="text/javascript"></script>
<script async="" src="https://www.googletagservices.com/tag/js/gpt.js" type="text/javascript"></script>

<div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><script src="https://cdn.plu.mx/widget-summary.js" async=""></script><iframe sandbox="allow-scripts allow-same-origin" title="Adobe ID Syncing iFrame" id="destination_publishing_iframe_elsevier_0" name="destination_publishing_iframe_elsevier_0_name" style="display: none; width: 0px; height: 0px;" src="https://elsevier.demdex.net/dest5.html?d_nsid=0#https%3A%2F%2Fwww.sciencedirect.com"></iframe></body></html>