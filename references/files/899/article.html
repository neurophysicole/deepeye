<!DOCTYPE html>
<!--[if lte IE 8]>
    <html id="doc" class="lt-ie9" lang="en">
<![endif]-->
<!--[if gt IE 8]>
    <html id="doc" lang="en">
<![endif]-->
<!--[if !IE]> -->
<html id="doc" style="" class=" js flexbox flexboxlegacy canvas canvastext webgl no-touch geolocation postmessage no-websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients no-cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths" lang="en"><!-- <![endif]--><head id="Head1">
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script type="text/javascript" src="300lo.json"></script><script type="text/javascript" src="_ate.config_resp"></script><script type="text/javascript" src="moatframe.js"></script><meta charset="utf-8"><title>
	Classification of visual and linguistic tasks using eye-movement features | JOV | ARVO Journals
</title><link rel="canonical" href="http://jov.arvojournals.org/article.aspx?articleid=2121494"><link id="lnkFavicon" rel="icon" type="image/png" href="http://jov.arvojournals.org/UI/app/images/JOV_favicon.png">
        <link href="http://jov.arvojournals.org/UI/app/images/60x60_jov.png" rel="apple-touch-icon" sizes="60x60">
        <link href="http://jov.arvojournals.org/UI/app/images/76x76_jov.png" rel="apple-touch-icon" sizes="76x76">
        <link href="http://jov.arvojournals.org/UI/app/images/120x120_jov.png" rel="apple-touch-icon" sizes="120x120">
        <link href="http://jov.arvojournals.org/UI/app/images/152x152_jov.png" rel="apple-touch-icon" sizes="152x152">

        <link href="css.css" rel="stylesheet" type="text/css">
    
    
    <meta name="citation_author" content="Moreno I. Coco"><meta name="citation_author_institution" content="Faculdade de Psicologia, Universidade de Lisboa, Lisboa, Portugal"><meta name="citation_author_email" content="micoco@fp.ul.pt"><meta name="citation_author" content="Frank Keller"><meta name="citation_author_institution" content="School of Informatics, University of Edinburgh, Edinburgh, UK"><meta name="citation_author_email" content="keller@inf.ed.ac.uk"><meta name="citation_title" content="Classification of visual and linguistic tasks using eye-movement features"><meta name="citation_firstpage" content="11"><meta name="citation_lastpage" content="11"><meta name="citation_doi" content="10.1167/14.3.11"><meta name="citation_keyword" content="eye movement"><meta name="citation_keyword" content="linguistics"><meta name="citation_keyword" content="naming function"><meta name="citation_keyword" content="visual search"><meta name="citation_journal_title" content="Journal of Vision"><meta name="citation_journal_abbrev" content="Journal of Vision"><meta name="citation_volume" content="14"><meta name="citation_issue" content="3"><meta name="citation_publication_date" content="2014/03/01"><meta name="citation_issn" content="1534-7362"><meta name="citation_publisher" content="The Association for Research in Vision and Ophthalmology"><meta name="citation_reference" content="citation_title=Incremental interpretation at verbs: Restricting the domain of subsequent reference; citation_author=Altmann  G.; citation_author=Kamide  Y.;  citation_journal_title=Cognition;  citation_year=(1999);  citation_volume=73;  citation_pages=247-264; "><meta name="citation_reference" content="citation_title=Mixed-effects modeling with crossed random effects for subjects and items; citation_author=Baayen  R.; citation_author=Davidson  D.; citation_author=Bates  D.;  citation_journal_title=Journal of Memory and Language;  citation_year=(2008;  citation_volume=59;  citation_pages=390-412; "><meta name="citation_reference" content="citation_title=Modeling the role of task in the control of gaze; citation_author=Ballard  D.; citation_author=Hayhoe  M.;  citation_journal_title=Visual Cognition;  citation_year=(2009;  citation_volume=17;  citation_pages=1185-1204; "><meta name="citation_reference" content="citation_title=Memory representations in natural tasks; citation_author=Ballard  D.; citation_author=Hayhoe  M.; citation_author=Pelz  J.;  citation_journal_title=Journal of Cognitive Neuroscience;  citation_year=(1995;  citation_volume=7;  citation_issue=(1);  citation_pages=66-80; "><meta name="citation_reference" content="citation_title=Random effects structure for confirmatory hypothesis testing: Keep it maximal; citation_author=Barr  D.; citation_author=Levy  R.; citation_author=Scheepers  C.; citation_author=Tily  H.;  citation_journal_title=Journal of Memory and Language;  citation_year=(2013;  citation_volume=68;  citation_issue=(3);  citation_pages=255-278; "><meta name="citation_reference" content="citation_author=Bates  D.; citation_author=Maechler  M.; citation_author=Bolker  B.;  citation_journal_title=lme4: Linear mixed-effects models using s4 classes;  citation_year=(2011); "><meta name="citation_reference" content="citation_title=How people look at pictures: A study of the psychology and perception of art; citation_author=Buswell  G. T.; citation_publisher=University of Chicago Press, Chicago;  citation_year=(1935); "><meta name="citation_reference" content="citation_title=The relative contribution of scene context and target features to visual search in real-world scenes; citation_author=Castelhano  M.; citation_author=Heaven  C.;  citation_journal_title=Attention, Perception and Psychophysics;  citation_year=(2010;  citation_volume=72;  citation_pages=1283-1297; "><meta name="citation_reference" content="citation_title=Viewing task influences eye movement control during active scene perception; citation_author=Castelhano  M.; citation_author=Mack  M.; citation_author=Henderson  J.;  citation_journal_title=Journal of Vision;  citation_year=(2009);  citation_volume=9;  citation_issue=(3);  citation_pages=1-15; "><meta name="citation_reference" content="citation_title=LIBSVM: A library for support vector machines; citation_author=Chang  C.; citation_author=Lin  C.;  citation_journal_title=ACM Transactions on Intelligent Systems and Technology, 2;  citation_year=(2011);  citation_volume=27;  citation_pages=1-27; "><meta name="citation_reference" content="citation_title=The impact of visual information on referent assignment in sentence production; citation_author=Coco  M.; citation_author=Keller  F.; citation_author=Taatgen  N. A.; citation_author=Van Rijn  H.; citation_publisher=Cognitive Science Society, ;  citation_year=(2009); "><meta name="citation_reference" content="citation_title=Scan patterns predict sentence production in the cross-modal processing of visual scenes; citation_author=Coco  M.; citation_author=Keller  F.;  citation_journal_title=Cognitive Science;  citation_year=(2012;  citation_volume=36;  citation_issue=(7);  citation_pages=1204-1223; "><meta name="citation_reference" content="citation_title=The interplay of bottom-up and top-down mechanisms in visual guidance during object naming; citation_author=Coco  M.; citation_author=Malcolm  G.; citation_author=Keller  F.;  citation_journal_title=Quarterly Journal of Experimental Psychology;  citation_year=(2013); "><meta name="citation_reference" content="citation_title=The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing; citation_author=Cooper  R.;  citation_journal_title=Cognitive Psychology;  citation_year=(1974;  citation_volume=6;  citation_issue=(1);  citation_pages=84-107; "><meta name="citation_reference" content="citation_title=Top-down control of eye movements: Yarbus revisited; citation_author=DeAngelus  M.; citation_author=Pelz  J.;  citation_journal_title=Visual Cognition;  citation_year=(2009;  citation_volume=17;  citation_issue=(6–7);  citation_pages=790-811; "><meta name="citation_reference" content="citation_title=Incremental learning of target locations in visual search; citation_author=Dziemianko  M.; citation_author=Keller  F.; citation_author=Coco  M.; citation_author=Carlson  I.; citation_author=Hoelscher  C.; citation_author=Shipley  T. F.; citation_publisher=Cognitive Science Society, Boston;  citation_year=(2009); "><meta name="citation_reference" content="citation_title=Objects predict fixations better than early saliency; citation_author=Einhäuser  W.; citation_author=Spain  M.; citation_author=Perona  P.;  citation_journal_title=Journal of Vision;  citation_year=(2008);  citation_volume=8;  citation_issue=(14);  citation_pages=1-26; "><meta name="citation_reference" content="citation_title=Rapid detection of person information in a naturalistic scene; citation_author=Fletcher-Watson  S.; citation_author=Findlay  J.; citation_author=Leekam  S.; citation_author=Benson  V.;  citation_journal_title=Perception;  citation_year=(2008;  citation_volume=37;  citation_issue=(4);  citation_pages=571-583; "><meta name="citation_reference" content="citation_title=Regularization paths for generalized linear models via coordinate descent; citation_author=Friedman  J.; citation_author=Hastie  T.; citation_author=Tibshirani  R.;  citation_journal_title=Journal of Statistical Software;  citation_year=(2010;  citation_volume=33;  citation_issue=(1);  citation_pages=1-22; "><meta name="citation_reference" content="citation_title=Direct control of fixation times in scene viewing: Evidence from analysis of the distribution of first fixation duration; citation_author=Glaholt  M.; citation_author=Reingold  E.;  citation_journal_title=Visual Cognition;  citation_year=(2012;  citation_volume=20;  citation_issue=(6);  citation_pages=605-626; "><meta name="citation_reference" content="citation_title=On the give and take between event apprehension and utterance formulation; citation_author=Gleitman  L.; citation_author=January  D.; citation_author=Nappa  R.; citation_author=Trueswell  J.;  citation_journal_title=Journal of Memory and Language;  citation_year=(2007;  citation_volume=57;  citation_pages=544-569; "><meta name="citation_reference" content="citation_title=Reconsidering Yarbus: A failure to predict observers' task from eye movement patterns; citation_author=Greene  M.; citation_author=Liu  T.; citation_author=Wolfe  J.;  citation_journal_title=Vision Research;  citation_year=(2012);  citation_volume=62;  citation_pages=1-8; "><meta name="citation_reference" content="citation_title=What the eyes say about speaking; citation_author=Griffin  Z.; citation_author=Bock  K.;  citation_journal_title=Psychological Science;  citation_year=(2000);  citation_volume=11;  citation_pages=274-279; "><meta name="citation_reference" content="citation_title=Visual perception in fencing: Do the eye movements of fencers represent their information pickup?; citation_author=Hagemann  N.; citation_author=Schorer  J.; citation_author=Cañal-Bruland  R.; citation_author=Lotz  S.; citation_author=Strauss  B.;  citation_journal_title=Attention, Perception, and Psychophysics;  citation_year=(2010);  citation_volume=72;  citation_issue=(8);  citation_pages=2204-2214; "><meta name="citation_reference" content="citation_title=Human gaze control during real-world scene perception; citation_author=Henderson  J.;  citation_journal_title=Trends in Cognitive Sciences;  citation_year=(2003;  citation_volume=7;  citation_pages=498-504; "><meta name="citation_reference" content="citation_title=Predicting cognitive state from eye movements; citation_author=Henderson  J.; citation_author=Shinkareva  S.; citation_author=Wang  S.; citation_author=Luke  S. G.; citation_author=Olejarczyk  J.;  citation_journal_title=PloS one;  citation_year=(2013);  citation_volume=8;  citation_issue=(5);  citation_pages=e64937"><meta name="citation_reference" content="citation_title=Parafoveal word processing during eye fixations in reading: Effects of word frequency; citation_author=Inhoff  A.; citation_author=Rayner  K.;  citation_journal_title=Perception &amp; Psychophysics;  citation_year=(1986;  citation_volume=40;  citation_issue=(6);  citation_pages=431-139; "><meta name="citation_reference" content="citation_title=The knowledge base of the oculomotor system. Philosophical Transactions of the Royal Society of London; citation_author=Land  M.; citation_author=Furneaux  S.;  citation_journal_title=Series B: Biological Sciences;  citation_year=(1997;  citation_volume=352;  citation_issue=(1358);  citation_pages=1231-1239; "><meta name="citation_reference" content="citation_title=In what ways do eye movements contribute to everyday activities?; citation_author=Land  M.; citation_author=Hayhoe  M.;  citation_journal_title=Vision Research;  citation_year=(2001);  citation_volume=41;  citation_pages=3559-3565; "><meta name="citation_reference" content="citation_title=From eye movements to actions: How batsmen hit the ball; citation_author=Land  M.; citation_author=McLeod  P.;  citation_journal_title=Nature Neuroscience;  citation_year=(2000);  citation_volume=3;  citation_pages=1340-1345; "><meta name="citation_reference" content="citation_title=The roles of vision and eye movements in the control of activities of daily living; citation_author=Land  M.; citation_author=Mennie  N.; citation_author=Rusted  J.;  citation_journal_title=Perception;  citation_year=(1999;  citation_volume=28;  citation_pages=1311-1328; "><meta name="citation_reference" content="citation_title=Combining top-down processes to guide eye movements during real-world scene search; citation_author=Malcolm  G.; citation_author=Henderson  J.;  citation_journal_title=Journal of Vision;  citation_year=(2010);  citation_volume=10;  citation_issue=(2);  citation_pages=1-11; "><meta name="citation_reference" content="citation_title=The effects of target template specificity on visual search in real-world scenes; citation_author=Malcolm  G.; citation_author=Henderson  J.;  citation_journal_title=Journal of Vision;  citation_year=(2009);  citation_volume=9;  citation_issue=(11);  citation_pages=1-13; "><meta name="citation_reference" content="citation_title=Examining the influence of task set on eye movements and fixations; citation_author=Mills  M.; citation_author=Hollingworth  A.; citation_author=Van der Stigchel  S.; citation_author=Hoffman  L.; citation_author=Dodd  M.;  citation_journal_title=Journal of Vision;  citation_year=(2011);  citation_volume=11;  citation_issue=(8);  citation_pages=1-15; "><meta name="citation_reference" content="citation_title=Scanpaths in saccadic eye movements while viewing and recognizing patterns; citation_author=Noton  D.; citation_author=Stark  L.;  citation_journal_title=Vision Research;  citation_year=(1971;  citation_volume=11;  citation_pages=919-942; "><meta name="citation_reference" content="citation_title=Oculomotor behavior and perceptual strategies in complex tasks; citation_author=Pelz  J.; citation_author=Canosa  R.;  citation_journal_title=Vision Research;  citation_year=(2001);  citation_volume=41;  citation_issue=(25);  citation_pages=3587-3596; "><meta name="citation_reference" content="citation_title=Disambiguating complex visual information: Towards communication of personal views of a scene; citation_author=Pomplun  M.; citation_author=Ritter  H.; citation_author=Velichkvosky  B.;  citation_journal_title=Perception;  citation_year=(1996;  citation_volume=25;  citation_pages=931-948; "><meta name="citation_reference" content="citation_title=Eye movements and attention in reading, scene perception, and visual search; citation_author=Rayner  K.;  citation_journal_title=The Quarterly Journal of Experimental Psychology;  citation_year=(2009);  citation_volume=62;  citation_issue=(8);  citation_pages=1457-1506; "><meta name="citation_reference" content="citation_title=Measuring visual clutter; citation_author=Rosenholtz  R.; citation_author=Li  Y.; citation_author=Nakano  L.;  citation_journal_title=Journal of Vision, 7;  citation_year=(2007);  citation_volume=(2);  citation_issue=17; "><meta name="citation_reference" content="citation_title=Task and context determine where you look; citation_author=Rothkopf  C.; citation_author=Ballard  D.; citation_author=Hayhoe  M.;  citation_journal_title=Journal of Vision, 7;  citation_year=(2007);  citation_volume=(14);  citation_issue=16; "><meta name="citation_reference" content="citation_title=Labelme: A database and web-based tool for image annotation; citation_author=Russell  B.; citation_author=Torralba  A.; citation_author=Murphy  K.; citation_author=Freeman  W.;  citation_journal_title=International Journal of Computer Vision;  citation_year=(2008);  citation_volume=77;  citation_issue=(1–3);  citation_pages=151-173; "><meta name="citation_reference" content="citation_title=Attentional capture of objects referred to by spoken language; citation_author=Salverda  A.; citation_author=Altmann  G.;  citation_journal_title=Journal of Experimental Psychology: Human Perception and Performance;  citation_year=(2011;  citation_volume=37;  citation_issue=(4);  citation_pages=1122"><meta name="citation_reference" content="citation_title=Eye movements and spoken language comprehension: Effects of syntactic context on syntactic ambiguity resolution; citation_author=Spivey-Knowlton  M.; citation_author=Tanenhaus  M.; citation_author=Eberhard  K.; citation_author=Sedivy  J.;  citation_journal_title=Cognitive Psychology;  citation_year=(2002;  citation_issue=(45);  citation_pages=447-181; "><meta name="citation_reference" content="citation_title=Integration of visual and linguistic information in spoken language comprehension; citation_author=Tanenhaus  M. K.; citation_author=Eberhard  K.; citation_author=Sedivy  J.;  citation_journal_title=Science;  citation_year=(1995);  citation_volume=268;  citation_pages=632-634; "><meta name="citation_reference" content="citation_title=The long and the short of it: Spatial statistics at fixation vary with saccade amplitude and task; citation_author=Tatler  B. W.; citation_author=Baddeley  R. J.; citation_author=Vincent  B. T.;  citation_journal_title=Vision Research;  citation_year=(2006);  citation_volume=46;  citation_issue=(12);  citation_pages=1857-1862; "><meta name="citation_reference" content="citation_title=Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search; citation_author=Torralba  A.; citation_author=Oliva  A.; citation_author=Castelhano  M.; citation_author=Henderson  J.;  citation_journal_title=Psychological Review;  citation_year=(2006);  citation_volume=4;  citation_issue=(113);  citation_pages=766-786; "><meta name="citation_reference" content="citation_title=What you see is what you need; citation_author=Triesch  J.; citation_author=Ballard  D.; citation_author=Hayhoe  M.; citation_author=Sullivan  B.;  citation_journal_title=Journal of Vision;  citation_year=(2003);  citation_volume=3;  citation_issue=(1);  citation_pages=86-94; "><meta name="citation_reference" content="citation_author=Venables  W. N.; citation_author=Ripley  B. D.; citation_publisher=Springer, Berlin;  citation_journal_title=Modern applied statistics with s (4th ed.);  citation_year=(2002); "><meta name="citation_reference" content="citation_author=Yarbus  A.; citation_publisher=Plenum, New York;  citation_journal_title=Eye movements and vision;  citation_year=(1967); "><meta name="citation_fulltext_world_readable" content=""><meta name="citation_pdf_url" content="https://jov.arvojournals.org/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11.pdf">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=Edge">
<link href="shared-css.css" type="text/css" rel="stylesheet">
<link href="content-pages.css" type="text/css" rel="stylesheet">
<link href="chartist.css" type="text/css" rel="stylesheet">
<link rel="stylesheet" href="franklingothic.css"><link rel="stylesheet" href="background.css">
<script id="twitter-wjs" src="widgets.js"></script><script async="" src="gtm"></script><script async="" type="text/javascript" src="gpt.js"></script><script src="Core.js" type="text/javascript"></script>
<script src="globalJS.js" type="text/javascript"></script>

        <!--[if (gte IE 6)&(lte IE 8)]>
            <script type="text/javascript" src="/UI/app/scripts/polyfills/respond.min.js"></script>
            <link rel="stylesheet" href="UI/app/styles/global/iefix.css" />
        <![endif]-->
        

                
        <script type="text/javascript">
            var googletag = googletag || {};
            googletag.cmd = googletag.cmd || [];
            (function () {
                var gads = document.createElement('script');
                gads.async = true;
                gads.type = 'text/javascript';
                var useSSL = 'https:' == document.location.protocol;
                gads.src = (useSSL ? 'https:' : 'http:') +
                    '//www.googletagservices.com/tag/js/gpt.js';
                var node = document.getElementsByTagName('script')[0];
                node.parentNode.insertBefore(gads, node);
            })();
        </script>

        <script type="text/javascript">
            googletag.cmd.push(function () {
                // responsive mappings
                var mapping_leaderboard = googletag.sizeMapping()
                  .addSize([750, 480], [728, 90]) // desktop 
                  .addSize([320, 400], [320, 100]) // Mobile 
                  .addSize([0, 0], [])
                  .build();
                var mapping_tower = googletag.sizeMapping()                
                  .addSize([750, 480], [160, 600]) // desktop 
                  .addSize([320, 400], [300, 250]) // Mobile 
                  .addSize([0, 0], [])
                  .build();

                // ARVO ad codes
                var w = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
                if (w > 750) {
                    // desktop
                    googletag.defineSlot('/130817127/JOV_ROS_leaderboard', [728, 90], 'div-gpt-ad-mast-leaderboard').defineSizeMapping(mapping_leaderboard).addService(googletag.pubads());
                    googletag.defineSlot('/130817127/JOV_ROS_tower', [160, 600], 'div-gpt-ad-content-tower').defineSizeMapping(mapping_tower).addService(googletag.pubads());
                } else {
                    // mobile
                    googletag.defineSlot('/130817127/JOV_mobile_leaderboard', [320, 100], 'div-gpt-ad-mast-leaderboard').defineSizeMapping(mapping_leaderboard).addService(googletag.pubads());
                    googletag.defineSlot('/130817127/JOV_mobile_pillow', [300, 250], 'div-gpt-ad-content-tower').defineSizeMapping(mapping_tower).addService(googletag.pubads());
                }
                
                googletag.pubads().enableSingleRequest();
                googletag.enableServices();
            });
        </script>  
             
           
    <script id="altmetric-embed-js" src="altmetric_badges-e99c269c07ff8ac628ce199348c2901d63f7ce6052a1.js"></script><meta class="foundation-data-attribute-namespace"><meta class="foundation-mq-xxlarge"><meta class="foundation-mq-xlarge"><meta class="foundation-mq-large"><meta class="foundation-mq-medium"><meta class="foundation-mq-small"><style></style><style type="text/css">.at-icon{fill:#fff;border:0}.at-icon-wrapper{display:inline-block;overflow:hidden}a .at-icon-wrapper{cursor:pointer}.at-rounded,.at-rounded-element .at-icon-wrapper{border-radius:12%}.at-circular,.at-circular-element .at-icon-wrapper{border-radius:50%}.addthis_32x32_style .at-icon{width:2pc;height:2pc}.addthis_24x24_style .at-icon{width:24px;height:24px}.addthis_20x20_style .at-icon{width:20px;height:20px}.addthis_16x16_style .at-icon{width:1pc;height:1pc}#at16lb{display:none;position:absolute;top:0;left:0;width:100%;height:100%;z-index:1001;background-color:#000;opacity:.001}#at_complete,#at_error,#at_share,#at_success{position:static!important}.at15dn{display:none}#at15s,#at16p,#at16p form input,#at16p label,#at16p textarea,#at_share .at_item{font-family:arial,helvetica,tahoma,verdana,sans-serif!important;font-size:9pt!important;outline-style:none;outline-width:0;line-height:1em}* html #at15s.mmborder{position:absolute!important}#at15s.mmborder{position:fixed!important;width:250px!important}#at15s{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABtJREFUeNpiZGBgaGAgAjAxEAlGFVJHIUCAAQDcngCUgqGMqwAAAABJRU5ErkJggg==);float:none;line-height:1em;margin:0;overflow:visible;padding:5px;text-align:left;position:absolute}#at15s a,#at15s span{outline:0;direction:ltr;text-transform:none}#at15s .at-label{margin-left:5px}#at15s .at-icon-wrapper{width:1pc;height:1pc;vertical-align:middle}#at15s .at-icon{width:1pc;height:1pc}.at4-icon{display:inline-block;background-repeat:no-repeat;background-position:top left;margin:0;overflow:hidden;cursor:pointer}.addthis_16x16_style .at4-icon,.addthis_default_style .at4-icon,.at4-icon,.at-16x16{width:1pc;height:1pc;line-height:1pc;background-size:1pc!important}.addthis_32x32_style .at4-icon,.at-32x32{width:2pc;height:2pc;line-height:2pc;background-size:2pc!important}.addthis_24x24_style .at4-icon,.at-24x24{width:24px;height:24px;line-height:24px;background-size:24px!important}.addthis_20x20_style .at4-icon,.at-20x20{width:20px;height:20px;line-height:20px;background-size:20px!important}.at4-icon.circular,.circular .at4-icon,.circular.aticon{border-radius:50%}.at4-icon.rounded,.rounded .at4-icon{border-radius:4px}.at4-icon-left{float:left}#at15s .at4-icon{text-indent:20px;padding:0;overflow:visible;white-space:nowrap;background-size:1pc;width:1pc;height:1pc;background-position:top left;display:inline-block;line-height:1pc}.addthis_vertical_style .at4-icon,.at4-follow-container .at4-icon{margin-right:5px}html>body #at15s{width:250px!important}#at15s.atm{background:none!important;padding:0!important;width:10pc!important}#at15s_inner{background:#fff;border:1px solid #fff;margin:0}#at15s_head{position:relative;background:#f2f2f2;padding:4px;cursor:default;border-bottom:1px solid #e5e5e5}.at15s_head_success{background:#cafd99!important;border-bottom:1px solid #a9d582!important}.at15s_head_success a,.at15s_head_success span{color:#000!important;text-decoration:none}#at15s_brand,#at15sptx,#at16_brand{position:absolute}#at15s_brand{top:4px;right:4px}.at15s_brandx{right:20px!important}a#at15sptx{top:4px;right:4px;text-decoration:none;color:#4c4c4c;font-weight:700}#at15sptx:hover{text-decoration:underline}#at16_brand{top:5px;right:30px;cursor:default}#at_hover{padding:4px}#at_hover .at_item,#at_share .at_item{background:#fff!important;float:left!important;color:#4c4c4c!important}#at_share .at_item .at-icon-wrapper{margin-right:5px}#at_hover .at_bold{font-weight:700;color:#000!important}#at_hover .at_item{width:7pc!important;padding:2px 3px!important;margin:1px;text-decoration:none!important}#at_hover .at_item.athov,#at_hover .at_item:focus,#at_hover .at_item:hover{margin:0!important}#at_hover .at_item.athov,#at_hover .at_item:focus,#at_hover .at_item:hover,#at_share .at_item.athov,#at_share .at_item:hover{background:#f2f2f2!important;border:1px solid #e5e5e5;color:#000!important;text-decoration:none}.ipad #at_hover .at_item:focus{background:#fff!important;border:1px solid #fff}.at15t{display:block!important;height:1pc!important;line-height:1pc!important;padding-left:20px!important;background-position:0 0;text-align:left}.addthis_button,.at15t{cursor:pointer}.addthis_toolbox a.at300b,.addthis_toolbox a.at300m{width:auto}.addthis_toolbox a{margin-bottom:5px;line-height:initial}.addthis_toolbox.addthis_vertical_style{width:200px}.addthis_button_facebook_like .fb_iframe_widget{line-height:100%}.addthis_button_facebook_like iframe.fb_iframe_widget_lift{max-width:none}.addthis_toolbox a.addthis_button_counter,.addthis_toolbox a.addthis_button_facebook_like,.addthis_toolbox a.addthis_button_facebook_send,.addthis_toolbox a.addthis_button_facebook_share,.addthis_toolbox a.addthis_button_foursquare,.addthis_toolbox a.addthis_button_linkedin_counter,.addthis_toolbox a.addthis_button_pinterest_pinit,.addthis_toolbox a.addthis_button_tweet{display:inline-block}.addthis_toolbox span.addthis_follow_label{display:none}.addthis_toolbox.addthis_vertical_style span.addthis_follow_label{display:block;white-space:nowrap}.addthis_toolbox.addthis_vertical_style a{display:block}.addthis_toolbox.addthis_vertical_style.addthis_32x32_style a{line-height:2pc;height:2pc}.addthis_toolbox.addthis_vertical_style .at300bs{margin-right:4px;float:left}.addthis_toolbox.addthis_20x20_style span{line-height:20px}.addthis_toolbox.addthis_32x32_style span{line-height:2pc}.addthis_toolbox.addthis_pill_combo_style .addthis_button_compact .at15t_compact,.addthis_toolbox.addthis_pill_combo_style a{float:left}.addthis_toolbox.addthis_pill_combo_style a.addthis_button_tweet{margin-top:-2px}.addthis_toolbox.addthis_pill_combo_style .addthis_button_compact .at15t_compact{margin-right:4px}.addthis_default_style .addthis_separator{margin:0 5px;display:inline}div.atclear{clear:both}.addthis_default_style .addthis_separator,.addthis_default_style .at4-icon,.addthis_default_style .at300b,.addthis_default_style .at300bo,.addthis_default_style .at300bs,.addthis_default_style .at300m{float:left}.at300b img,.at300bo img{border:0}a.at300b .at4-icon,a.at300m .at4-icon{display:block}.addthis_default_style .at300b,.addthis_default_style .at300bo,.addthis_default_style .at300m{padding:0 2px}.at300b,.at300bo,.at300bs,.at300m{cursor:pointer}.addthis_button_facebook_like.at300b:hover,.addthis_button_facebook_like.at300bs:hover,.addthis_button_facebook_send.at300b:hover,.addthis_button_facebook_send.at300bs:hover{opacity:1}.addthis_20x20_style .at15t,.addthis_20x20_style .at300bs{overflow:hidden;display:block;height:20px!important;width:20px!important;line-height:20px!important}.addthis_32x32_style .at15t,.addthis_32x32_style .at300bs{overflow:hidden;display:block;height:2pc!important;width:2pc!important;line-height:2pc!important}.at300bs{overflow:hidden;display:block;background-position:0 0;height:1pc;width:1pc;line-height:1pc!important}.addthis_default_style .at15t_compact,.addthis_default_style .at15t_expanded{margin-right:4px}#at_share .at_item{width:123px!important;padding:4px;margin-right:2px;border:1px solid #fff}#at16p{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABtJREFUeNpiZGBgaGAgAjAxEAlGFVJHIUCAAQDcngCUgqGMqwAAAABJRU5ErkJggg==);z-index:10000001;position:absolute;top:50%;left:50%;width:300px;padding:10px;margin:0 auto;margin-top:-185px;margin-left:-155px;font-family:arial,helvetica,tahoma,verdana,sans-serif;font-size:9pt;color:#5e5e5e}#at_share{margin:0;padding:0}#at16pt{position:relative;background:#f2f2f2;height:13px;padding:5px 10px}#at16pt a,#at16pt h4{font-weight:700}#at16pt h4{display:inline;margin:0;padding:0;font-size:9pt;color:#4c4c4c;cursor:default}#at16pt a{position:absolute;top:5px;right:10px;color:#4c4c4c;text-decoration:none;padding:2px}#at15sptx:focus,#at16pt a:focus{outline:thin dotted}#at15s #at16pf a{top:1px}#_atssh{width:1px!important;height:1px!important;border:0!important}.atm{width:10pc!important;padding:0;margin:0;line-height:9pt;letter-spacing:normal;font-family:arial,helvetica,tahoma,verdana,sans-serif;font-size:9pt;color:#444;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABtJREFUeNpiZGBgaGAgAjAxEAlGFVJHIUCAAQDcngCUgqGMqwAAAABJRU5ErkJggg==);padding:4px}.atm-f{text-align:right;border-top:1px solid #ddd;padding:5px 8px}.atm-i{background:#fff;border:1px solid #d5d6d6;padding:0;margin:0;box-shadow:1px 1px 5px rgba(0,0,0,.15)}.atm-s{margin:0!important;padding:0!important}.atm-s a:focus{border:transparent;outline:0;transition:none}#at_hover.atm-s a,.atm-s a{display:block;text-decoration:none;padding:4px 10px;color:#235dab!important;font-weight:400;font-style:normal;transition:none}#at_hover.atm-s .at_bold{color:#235dab!important}#at_hover.atm-s a:hover,.atm-s a:hover{background:#2095f0;text-decoration:none;color:#fff!important}#at_hover.atm-s .at_bold{font-weight:700}#at_hover.atm-s a:hover .at_bold{color:#fff!important}.atm-s a .at-label{vertical-align:middle;margin-left:5px;direction:ltr}.at_PinItButton{display:block;width:40px;height:20px;padding:0;margin:0;background-image:url(//s7.addthis.com/static/t00/pinit00.png);background-repeat:no-repeat}.at_PinItButton:hover{background-position:0 -20px}.addthis_toolbox .addthis_button_pinterest_pinit{position:relative}.at-share-tbx-element .fb_iframe_widget span{vertical-align:baseline!important}#at16pf{height:auto;text-align:right;padding:4px 8px}.at-privacy-info{position:absolute;left:7px;bottom:7px;cursor:pointer;text-decoration:none;font-family:helvetica,arial,sans-serif;font-size:10px;line-height:9pt;letter-spacing:.2px;color:#666}.at-privacy-info:hover{color:#000}.body .wsb-social-share .wsb-social-share-button-vert{padding-top:0;padding-bottom:0}.body .wsb-social-share.addthis_counter_style .addthis_button_tweet.wsb-social-share-button{padding-top:40px}.body .wsb-social-share.addthis_counter_style .addthis_button_facebook_like.wsb-social-share-button{padding-top:21px}@media print{#at4-follow,#at4-share,#at4-thankyou,#at4-whatsnext,#at4m-mobile,#at15s,.at4,.at4-recommended{display:none!important}}@media screen and (max-width:400px){.at4win{width:100%}}@media screen and (max-height:700px) and (max-width:400px){.at4-thankyou-inner .at4-recommended-container{height:122px;overflow:hidden}.at4-thankyou-inner .at4-recommended .at4-recommended-item:first-child{border-bottom:1px solid #c5c5c5}}</style><style type="text/css">.at-branding-logo{font-family:helvetica,arial,sans-serif;text-decoration:none;font-size:10px;display:inline-block;margin:2px 0;letter-spacing:.2px}.at-branding-logo .at-branding-icon{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAMAAAC67D+PAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRF////+GlNUkcc1QAAAB1JREFUeNpiYIQDBjQmAwMmkwEM0JnY1WIxFyDAABGeAFEudiZsAAAAAElFTkSuQmCC")}.at-branding-logo .at-branding-icon,.at-branding-logo .at-privacy-icon{display:inline-block;height:10px;width:10px;margin-left:4px;margin-right:3px;margin-bottom:-1px;background-repeat:no-repeat}.at-branding-logo .at-privacy-icon{background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAkAAAAKCAMAAABR24SMAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAABhQTFRF8fr9ot/xXcfn2/P5AKva////////AKTWodjhjAAAAAd0Uk5T////////ABpLA0YAAAA6SURBVHjaJMzBDQAwCAJAQaj7b9xifV0kUKJ9ciWxlzWEWI5gMF65KUTv0VKkjVeTerqE/x7+9BVgAEXbAWI8QDcfAAAAAElFTkSuQmCC")}.at-branding-logo span{text-decoration:none}.at-branding-logo .at-branding-addthis,.at-branding-logo .at-branding-powered-by{color:#666}.at-branding-logo .at-branding-addthis:hover{color:#333}.at-cv-with-image .at-branding-addthis,.at-cv-with-image .at-branding-addthis:hover{color:#fff}a.at-branding-logo:visited{color:initial}.at-branding-info{display:inline-block;padding:0 5px;color:#666;border:1px solid #666;border-radius:50%;font-size:10px;line-height:9pt;opacity:.7;transition:all .3s ease;text-decoration:none}.at-branding-info span{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.at-branding-info:before{content:'i';font-family:Times New Roman}.at-branding-info:hover{color:#0780df;border-color:#0780df}</style><meta class="foundation-mq-topbar"><link rel="preload" href="integrator.js" as="script"><script type="text/javascript" src="integrator.js"></script><script src="pubads_impl_2020013001.js" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script charset="utf-8" src="https://platform.twitter.com/js/button.a657e8de41cd5e7b38cde1f36c9ab9c2.js"></script></head>
<body class="off-canvas arvo pg_article" data-sitename="jov" theme-jov=""><div id="MathJax_Message" style="">Loading [Contrib]/a11y/accessibility-menu.js</div>
    <!-- Google Tag Manager -->
    <noscript>
        <iframe src="//www.googletagmanager.com/ns.html?id=GTM-MG7R8N"
            height="0" width="0" style="display: none; visibility: hidden"></iframe>
    </noscript>
    <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
        'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
    j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
    '//www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-MG7R8N');</script>
    <!-- End Google Tag Manager -->
    <form method="post" action="./article.aspx?articleid=2121494" id="webform" novalidate="novalidate">
<div class="aspNetHidden">
<input name="__VIEWSTATE" id="__VIEWSTATE" value="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" type="hidden">
</div>

<div class="aspNetHidden">

	<input name="__VIEWSTATEGENERATOR" id="__VIEWSTATEGENERATOR" value="2173C2F0" type="hidden">
	<input name="__EVENTTARGET" id="__EVENTTARGET" value="" type="hidden">
	<input name="__EVENTARGUMENT" id="__EVENTARGUMENT" value="" type="hidden">
	<input name="__EVENTVALIDATION" id="__EVENTVALIDATION" value="/wEdAAp8SRSVxqY4KfmvXE3Kyqf5ud3klMxrHMtT1QhALa51hLRdgq07h3Ia1yoLUiQl7e+GDR/9H3H16vWfuk+ll/JrS4g2jmPfdcDjZLHdifFLXgYKvj1+ye81504o04JafrL8SBdQnEYKcdwa8FmgsnU2KTkiU5F5yVreXqTg8CmPEtraQ4OoTRHTMQwQE4Y2EF/By42jglXfXUG4T5SSlkzcKBhcpVniVIoqCbaDjsZEV7HA5IA=" type="hidden">
</div>
        
        <input id="hdnSiteID" value="170" type="hidden">
        
        
    <a id="Top"></a>
    <div class="master-container row collapse">
        <div class="large-12 columns">
            <section class="master-masthead">
                



<div class="text-center header-ad" id="div-gpt-ad-mast-leaderboard">
    <script type="text/javascript">
        googletag.cmd.push(function () { googletag.display('div-gpt-ad-mast-leaderboard'); });        
        
    </script>
</div>
<header class="header-wrap">
    <div id="HeaderTop" class="header-top row collapse">
        <div class="left-tri-wrap tri-wrap">
            <div class="left-tri tri"></div>
        </div>
        <div class="right-tri-wrap tri-wrap">
            <div class="right-tri tri"></div>    
        </div>
        
        <div id="UmbDropdownWrap" class="large-2 small-6 columns">
            <div class="umb-dropdown-area" data-toggle-target="#UmbNavDropdown">
                <div class="umb-drop-trigger">
                    <img class="umb-logo" src="arvo_journals_logo-white.png">
                    <i class="icon-caret-down"></i>
                    <div id="BodyContent_masthead_JournalShortName" class="journal-shortname">jov</div>
                </div>
            
                <div id="UmbNavDropdown">
                    
                            <ul id="MicroLinks" class="micro-nav">
                        
                            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_0">
                                <a href="http://jov.arvojournals.org/issues.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_0">
                                    Issues
                                    
                                </a>
                                
                            </li>
                        
                            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_1">
                                <a href="http://jov.arvojournals.org/topics.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_1">
                                    Topics
                                    
                                </a>
                                
                            </li>
                        
                            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_2">
                                <a href="http://jov.arvojournals.org/ss/forauthors.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_2">
                                    For Authors
                                    
                                </a>
                                
                            </li>
                        
                            <li id="BodyContent_masthead_rptNavBarUmbDropdown_ParentNavPiece_3" class="has-menu-dropdown">
                                <a href="http://jov.arvojournals.org/ss/about.aspx" id="BodyContent_masthead_rptNavBarUmbDropdown_lnkNavigationPiece_3">
                                    About
                                    <i id="BodyContent_masthead_rptNavBarUmbDropdown_IconCarrot_3" class="icon-angle-down"></i>
                                </a>
                                
                                        <ul class="site-submenu">
                                    
                                        <li>
                                            <a href="http://jov.arvojournals.org/ss/editorial_board.aspx">
                                            Editorial Board</a>
                                        </li>
                                    
                                        </ul>
                                    
                            </li>
                        
                            </ul>
                        
                    <ul class="all-journals">
                        <li><a href="http://jov.arvojournals.org/index.aspx"><i class="icon-home"></i>Journals Home</a></li>
                        
                                <li>
                                    <a href="https://iovs.arvojournals.org/">
                                        <img class="lazy" data-original="/UI/app/images/arvo_iovs_logo.png">    
                                    </a>
                                </li>
                            
                                <li>
                                    <a href="https://jov.arvojournals.org/">
                                        <img class="lazy" data-original="/UI/app/images/arvo_jov_logo.png">    
                                    </a>
                                </li>
                            
                                <li>
                                    <a href="https://tvst.arvojournals.org/">
                                        <img class="lazy" data-original="/UI/app/images/arvo_tvst_logo.png">    
                                    </a>
                                </li>
                            
                    </ul>
                </div>
            </div>
        </div>

        <div id="SignInWrap" class="large-3 small-6 columns large-push-7">
            <div class="sign-in-area">
                <div class="user-info">
                    
                    
                    <span id="MobileSignInLink"><i class="icon-user"></i></span>
                    <span id="SignInLink" class="sign-in">SIGN IN</span>
                </div>
            </div>
            <div id="SigninUserInfoDropdown" class="si-module">
                <div id="AccessSignIn" class="signin-container">
    
    <input name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$hfARVOStoreUrl" id="hfARVOStoreUrl" value="https://store.arvojournals.org" type="hidden">
    
            <div id="pnlGlobalSignin" class="si-panel" onkeypress="javascript:return WebForm_FireDefaultButton(event, 'BodyContent_masthead_ucGlobalSignInDropDown_ibSignIn')">
	
                <div class="si-form clearfix">
                    <div class="message-error" id="wrong-username-password"></div>
                    
                    <input name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$txtEmail" maxlength="50" id="txtEmail" class="requiredtxtusername" placeholder="Username" type="text">
                    <div class="message-error" id="reqEmailError"></div>

		            <input name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$txtPassword" maxlength="50" id="txtPassword" class="requiredtxtpassword" placeholder="Password" type="password">

                    <div class="message-error" id="reqPasswordError"></div>
                </div>
                <div class="si-forgot-pass clearfix text-center">
                    <a href="http://jov.arvojournals.org/account/forgotpassword.aspx" class="forgot-password">Forgot password?</a>
                </div>
                <div class="btn-group text-center"><input name="ctl00$ctl00$BodyContent$masthead$ucGlobalSignInDropDown$ibSignIn" value="Sign In" id="BodyContent_masthead_ucGlobalSignInDropDown_ibSignIn" class="dark-button signInBtn" type="submit"></div>
                <div id="BodyContent_masthead_ucGlobalSignInDropDown_divSignInCreateAccount" class="si-create-acct text-center clearfix">
                    <a href="http://jov.arvojournals.org/account/createaccount.aspx">Create an Account</a>
                </div>
                 <div id="signin-loading-spinner" class="signin-loading inner-circle hide"></div>
            
</div> 
                    
            <div id="SubscribeBox" data-hideattribute="true" class="subscribe-box box" style="display: none;">
                <h2>To View More...</h2>
                <p id="BodyContent_masthead_ucGlobalSignInDropDown_pPurchaseSubInstruction">Purchase this article with an account.</p>
                <div class="subscribe-btns">
                    
                    <p style="text-align: center;">or</p>
                    <a href="http://jov.arvojournals.org/ss/subscriptions.aspx"><span class="dark-button">Subscribe Now</span></a>    
                </div>
                
            </div>
        
</div>


            </div>
            <div class="toggle-icon search-toggle" data-target="#Search"><i class="icon-search"></i></div>
        </div>
        
        <div id="SearchWrap" class="large-7 columns large-pull-3">
            <div id="Search" class="search">
                <div class="search-tools">
                    <div id="SearchButton" class="search-button point"><i class="icon-search"></i></div>
                    <div class="adv-search">
                        <a href="http://jov.arvojournals.org/solr/advancedsearch.aspx">
                            <span class="adv-text">Advanced</span>Search
                        </a>
                    </div>
                </div>

                <div class="search-input">
                    <input id="SearchTerm" class="search-term" placeholder="Search..." maxlength="255" type="text">
                    <select id="searchResource" class="search-resource" sb="60589248" style="display: none;">
                        <option value="" selected="selected">All Journals</option>
                        
                                <option value="177" data-displayname="Investigative Ophthalmology &amp; Visual Science">IOVS</option>
                            
                                <option value="178" data-displayname="Journal of Vision">JOV</option>
                            
                                <option value="179" data-displayname="Translational Vision Science &amp; Technology">TVST</option>
                            

                    </select><div id="sbHolder_60589248" rel="" class="sbHolder"><a id="sbToggle_60589248" href="#" class="sbToggle"></a><a id="sbSelector_60589248" rel="" href="#" class="sbSelector">All Journals</a><ul id="sbOptions_60589248" class="sbOptions" style="display: none;"><li><a href="#" rel="" class="sbFocus">All Journals</a></li><li><a href="#177" rel="177">IOVS</a></li><li><a href="#178" rel="178">JOV</a></li><li><a href="#179" rel="179">TVST</a></li></ul></div> 
                    <input name="ctl00$ctl00$BodyContent$masthead$hfGlobalSearchSiteURL" id="hfGlobalSearchSiteURL" value="https://arvojournals.org" type="hidden">
                </div>
            </div>    
        </div>
    </div>

    <div id="BodyContent_masthead_HeaderBottom" class="header-bottom clearfix">
        <div class="micro-logo">
            <a href="http://jov.arvojournals.org/">
                <img src="arvo_jov_logo.png" id="BodyContent_masthead_ImgSiteLogo">
            </a>
        </div>
        
        <div class="toggle-icon nav-toggle" data-target="#SiteNavMenu"><i class="icon-nav"></i></div>    
            
        <nav class="nav-wrap">
            
                    <ul id="SiteNavMenu" class="site-nav">
                
                    <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_0">
                        <a href="http://jov.arvojournals.org/issues.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_0">
                            Issues
                            
                        </a>
                        
                    </li>
                
                    <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_1">
                        <a href="http://jov.arvojournals.org/topics.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_1">
                            Topics
                            
                        </a>
                        
                    </li>
                
                    <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_2">
                        <a href="http://jov.arvojournals.org/ss/forauthors.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_2">
                            For Authors
                            
                        </a>
                        
                    </li>
                
                    <li id="BodyContent_masthead_rptNavigationBar_ParentNavItem_3" class="has-menu-dropdown">
                        <a href="http://jov.arvojournals.org/ss/about.aspx" id="BodyContent_masthead_rptNavigationBar_lnkNavigationItem_3">
                            About
                            <i id="BodyContent_masthead_rptNavigationBar_IconCarrot_3" class="icon-angle-down"></i>
                        </a>
                        
                                <ul class="site-submenu">
                            
                                <li>
                                    <a href="http://jov.arvojournals.org/ss/editorial_board.aspx">
                                    Editorial Board</a>
                                </li>
                            
                                </ul>
                            
                    </li>
                
                    </ul>
                
        </nav>
    </div>
</header>
<script type="text/javascript">
	var App = App || {};
        App.LoginUserInfo = {
            isInstLoggedIn: [0],
            currentSessionId: 'qce01rnapqbdj0revk2sxspz'
        }; 
</script>
<script type="text/javascript">
    $('body').on('click', '.sign-in-area', function () {
        $('#SigninUserInfoDropdown').toggleClass('expanded');
        return false;
    });

    $(document).click(function (e) {
        var container = $("#SigninUserInfoDropdown");
        var clickelemnt = $(".sign-in-area");
        if (!container.is(e.target) // if the target of the click isn't the container...
            && container.has(e.target).length === 0 // ... nor a descendant of the container...
            && clickelemnt.has(e.target).length === 0) // ... nor a signin Click...
        {
            container.removeClass('expanded');
        } else {
            clickelemnt.addClass('dropdown-active');
        }
    });
</script>

            </section>
            <section class="master-main">
                
    <div class="row main-row">
        <div id="InfoColumn" class="large-2 medium-3 columns info-column">
            <header id="header" class="article-nav">
                       <div class="widget-IssueInfo widget-instance-ARVO_IssueInfo_Article">
        

<div id="issueInfo-ARVO_IssueInfo_Article">
        <a href="http://jov.arvojournals.org/issues.aspx?issueid=932817"><img id="issueFallbackImage" class="fb-featured-image" src="170.jpg"></a>
    
        <div class="ii-pub-date">
            March 2014
        </div>
        <div class="ii-vol-issue">
            Volume 14, Issue 3
        </div>
</div>

 
    </div>
    <div class="widget-ArticleNavLinks widget-instance-ARVO_ArticleNavLinks_Widget">
        <ul class="inline-list">
    <li class="first prev">
            <a href="http://jov.arvojournals.org/article.aspx?articleid=2121470">‹</a>
    </li>
    <li class="middle tableofcontents bdlr">
            <a href="http://jov.arvojournals.org/issue.aspx?issueid=932817&amp;journalid=178">Issue</a>
   </li> 
    <li class="last next">
            <a href="http://jov.arvojournals.org/article.aspx?articleid=2121512">›</a>
     </li>
</ul>
	   
               
    </div>

	        </header>
            <section id="LeftNavSticker">
                    <div class="widget-ArticleJumpLinks widget-instance-ARVO_ArticleJumpLinks_Widget">
            <h6 class="jumplinks-heading"><i class="icon-jumpto"></i>Jump To...</h6>    
    <ul data-magellan-expedition="fixed" style="">
        <li class="section-jump-link head-1" data-magellan-arrival="87793181"><a class="scrollTo" href="#87793181">Abstract</a></li>
        <li class="section-jump-link head-1" data-magellan-arrival="87793182"><a class="scrollTo" href="#87793182">Introduction</a></li>
        <li class="section-jump-link head-1" data-magellan-arrival="87793197"><a class="scrollTo" href="#87793197">The present study</a></li>
        <li class="section-jump-link head-1" data-magellan-arrival="87793233"><a class="scrollTo" href="#87793233">Results and discussion</a></li>
        <li class="section-jump-link head-1" data-magellan-arrival="87793269"><a class="scrollTo" href="#87793269">General discussion</a></li>
        <li class="section-jump-link head-1" data-magellan-arrival="87793285"><a class="scrollTo" href="#87793285">Acknowledgments</a></li>
        <li class="section-jump-link head-1" data-magellan-arrival="87793291"><a class="scrollTo" href="#87793291">References</a></li>
    </ul>
 
    </div>

	        </section>
        </div>
        
        <div class="link-to-top tab-up" style="left: 52.5px;">
            <div class="link-top-circle">
                <i class="icon-angle-up"></i>
            </div>
        </div>
        
        <div id="ContentColumn" class="large-7 medium-9 columns content-column">
            <div class="article-top-info">
                    <div class="widget-ArticleTopInfo widget-instance-ARVO_ArticleTopInfo_Widget">
        <div class="module-widget article-top-widget">

    
        <div class="access-state-logos all-viewports">
    

            <span class="article-accessType all-viewports left-flag FreeAccess">Free</span>
        </div>

    <div class="widget-items">
<span class="wi-clientType large-view-only">Article</span><span class="wi-pub-date large-view-only">&nbsp;&nbsp;|&nbsp;&nbsp; March 2014</span><div class="wi-article-title article-title-main">
            Classification of visual and linguistic tasks using eye-movement features
        </div>
        <div class="wi-authors">
            <div class="al-authors-list">
                <span class="wi-fullname brand-fg"><a href="http://jov.arvojournals.org/solr/searchresults.aspx?author=Moreno+I.+Coco">Moreno I. Coco</a></span><span class="al-author-delim">; </span><span class="wi-fullname brand-fg"><a href="http://jov.arvojournals.org/solr/searchresults.aspx?author=Frank+Keller">Frank Keller</a></span>
            </div>
        </div>
            <div class="ww-authorAffiliations">

                <div class="toggle-affiliation ati-toggle-trigger">
                    <i id="toggleAffiliationIcon" class="icon-plus brand-fg"></i>
                        <span class="wi-title">Author Affiliations</span>
                </div>
                    <ul class="wi-affiliationList ati-toggle-content hide">
                                <li>
                                        <div class="wi-fullname">Moreno I. Coco</div>
                                                                            <div class="wi-affiliations">Faculdade de Psicologia, Universidade de Lisboa, Lisboa, Portugal<br><a href="mailto:micoco@fp.ul.pt">micoco@fp.ul.pt</a><a href="http://homepages.inf.ed.ac.uk/mcoco/" target="_blank" class="italicLink">http://homepages.inf.ed.ac.uk/mcoco/</a></div>
                                                                    </li>
                                <li>
                                        <div class="wi-fullname">Frank Keller</div>
                                                                            <div class="wi-affiliations">School of Informatics, University of Edinburgh, Edinburgh, UK<br><a href="mailto:keller@inf.ed.ac.uk">keller@inf.ed.ac.uk</a><a href="http://homepages.inf.ed.ac.uk/keller/" target="_blank" class="italicLink">http://homepages.inf.ed.ac.uk/keller/</a></div>
                                                                    </li>
                    </ul>
                                            </div>
    <div class="widget-ArticleLinks widget-instance-ArticleTopInfo_ArticleLinks">
        
 
    </div>
        <div class="ww-citation large-view-only">
            <span class="journal-name">Journal of Vision</span><span> March 2014, Vol.14, 11. doi:<a href="https://doi.org/10.1167/14.3.11">https://doi.org/10.1167/14.3.11</a></span>
        </div>

    </div>
    <div class="widget-ArticleTopLinks widget-instance-ArticleTopInfo_ArticleTopLinks">
         
    </div>

</div>

<script>
    $(document).ready(function () {
        $('.article-top-widget').on('click', '.ati-toggle-trigger', function () {
            $(this).find('.icon-plus, .icon-minus').toggleClass('icon-minus icon-plus');
            $(this).siblings('.ati-toggle-content').toggleClass('hide');
        });

        // In Chrome, an anchor tag with target="_blank" and a "mailto:" href opens a new tab/window as well as the email client
        // I suspect this behavior will be corrected in the future
        // Remove the target="_blank"
        $('ul.wi-affiliationList').find('a[href^="mailto:"]').each(function () {
            $(this).removeAttr('target');
        });
    });
</script>

 
    </div>

            </div>
            
            <ul id="Toolbar" style="display: table;">
                <li class="toolbar-item item-with-dropdown">
                    <a class="toolbox-dropdown" data-dropdown="ViewsDrop">
                        <i class="icon-views"></i><span>Views</span><i class="icon-caret-down"></i>
                    </a>
                    <ul id="ViewsDrop" class="f-dropdown" data-dropdown-content="">
                        <li class="article-tab" style="display: none;"><a class="tab-item" data-tab="#ContentTab"><i class="icon-fulltext"></i><span>Full Article</span></a></li>
                        <li class="figure-tab"><a class="tab-item" data-tab="#FigureTab"><i class="icon-photo"></i><span>Figures</span></a></li>
                        <li class="table-tab"><a class="tab-item" data-tab="#TableTab"><i class="icon-table"></i><span>Tables</span></a></li>
                    </ul>
                </li>
                <li class="toolbar-item toolbar-pdf">
                            <a id="pdfLink" class="al-link pdf article-pdfLink pdfaccess" data-article-id="2121494" data-article-url="/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11.pdf" data-ajax-url="/Content/CheckPdfAccess">
            <i class="icon-file-pdf"></i><span>PDF</span>
        </a>

                </li>
                <li class="toolbar-item">
                    <a class="toolbox-dropdown" id="share-icon" data-dropdown="ShareDrop">
                        <i class="icon-share"></i><span>Share</span><i class="icon-caret-down"></i>
                    </a>
                    <ul id="ShareDrop" class="addthis_toolbox addthis_default_style addthis_20x20_style f-dropdown" data-dropdown-content="">
                        <li><a class="addthis_button_email" target="_blank" title="Email" href="#"><i class="icon-email"></i><span>E-mail</span></a></li>
                        <li><a class="addthis_button_facebook" title="Facebook" href="#"><i class="icon-facebook"></i><span>Facebook</span></a></li>
                        <li><a class="addthis_button_twitter" title="Twitter" href="#"><i class="icon-twitter"></i><span>Twitter</span></a></li>
                        <li><a class="addthis_button_google" target="_blank" title="Google Bookmark" href="#"><i class="icon-google"></i><span>Google</span></a></li>
                        <li><a class="addthis_button_digg" target="_blank" title="Digg" href="#"><i class="icon-digg"></i><span>Digg</span></a></li>
                        <li><a class="addthis_button_delicious"><i class="icon-delicious"></i><span>Delicious</span></a></li>
                        <li><a class="addthis_button_tumblr" target="_blank" title="Tumblr" href="#"><i class="icon-tumblr"></i><span>Tumblr</span></a></li>
                        <li><a class="addthis_button_stumbleupon" target="_blank" title="MIX" href="#"><i class="icon-stumbleupon"></i><span>StumbleUpon</span></a></li>

                    <div class="atclear"></div></ul>
                </li>
                <li class="toolbar-item last">
                    <a class="toolbox-dropdown" id="settings-icon" data-dropdown="otherToolsDrop">
                        <i class="icon-tools"></i><span>Tools</span><i class="icon-caret-down"></i>
                    </a>
                    <ul id="otherToolsDrop" class="f-dropdown" data-dropdown-content="">
                        <li><div id="toolboxGetAlertsWidget">

    <a id="revealGetAlertsARVO_Get_Alerts" class="getAlertsLink" onclick="openModal(this)" data-reveal="ARVO_Get_Alerts"><i class="icon-envelope-alt"></i><span>Alerts</span></a>

    
    <div id="getAlertsARVO_Get_Alerts" class="reveal-modal getAlertsModal" data-reveal="ARVO_Get_Alerts">

        <div class="m-alerts-title modal-title">User Alerts</div>


        

        <div class="m-alerts-alert-for modal-p">You are adding an alert for:</div>

        <div class="modal-resource-title modal-p">Classification of 
visual and linguistic tasks using eye-movement features                 
                                                                 </div>

        <div class="m-alerts-prompt modal-p">
            
            You will receive an email whenever this article is 
corrected, updated, or cited in the literature. You can manage this and 
all other alerts in <a href="https://jov.arvojournals.org/Account/myaccount.aspx">My Account</a>
        </div>


            <div class="m-alerts-email-address modal-p">The alert will be sent to: <a href="mailto:"></a></div>
<input id="UserEmailARVO_Get_Alerts" name="UserEmail" value="" type="hidden">
        <div>

            <input id="CurrentUserEmailARVO_Get_Alerts" name="CurrentUserEmail" value="" type="hidden">
            <input id="IsUserAlreadySubscribedARVO_Get_Alerts" name="IsUserAlreadySubscribed" value="False" type="hidden">
            <input id="SuccessfulUpdateARVO_Get_Alerts" name="SuccessfulUpdate" value="" type="hidden">
            <input id="ResourceIdARVO_Get_Alerts" name="ResourceId" value="2121494" type="hidden">
            <input id="ResourceTypeARVO_Get_Alerts" name="ResourceType" value="Article" type="hidden">
            <input id="ResourceTitleARVO_Get_Alerts" name="ResourceTitle" value="Classification of visual and linguistic tasks using eye-movement features" type="hidden">
            <input id="AlertTypeARVO_Get_Alerts" name="AlertType" value="Article" type="hidden">
            <input id="IsAccessPermittedARVO_Get_Alerts" name="IsAccessPermitted" value="False" type="hidden">
            <input id="ShowSaveSearchToUnauthenticatedUserARVO_Get_Alerts" name="ShowSaveSearchToUnauthenticatedUser" value="False" type="hidden">
            <input id="UserIsLoggedInARVO_Get_Alerts" name="UserIsLoggedIn" value="False" type="hidden">
            <input id="CurrentAlertContextSiteIdARVO_Get_Alerts" name="CurrentAlertContextSiteId" value="170" type="hidden">
            <input id="ModalDivIDARVO_Get_Alerts" name="InstanceNameForModalDivID" value="WidgetNoAccessModalSmallARVO_Get_Alerts" type="hidden">

            
            
            <div id="getAlertsConfirmationARVO_Get_Alerts">
                <div id="m-alerts-email-spinner-ARVO_Get_Alerts" class="css3preloader loaderWrap hide">
                    <div class="loading alertsSpinner"></div>
                </div>
                    <a class="m-alerts-submit" id="confirmSaveChangesARVO_Get_Alerts" onclick="confirm(this)" data-reveal="ARVO_Get_Alerts">Confirm</a>


            </div>

        </div>

        <a class="close-reveal-modal">×</a>
    </div>
    
    
    <div id="WidgetNoAccessModalSmallARVO_Get_Alerts" class="reveal-modal " data-reveal="">
            <h6>This feature is available to authenticated users only.</h6>
        
<a href="#" data-reveal-id="SignInModal" data-reveal="">Sign In</a>
             or 
<a id="aCreateAccount" class="brand-fg" href="http://jov.arvojournals.org/account/createaccount.aspx?ReturnUrl=http%3a%2f%2fjov.arvojournals.org%2farticle.aspx%3farticleid%3d2121494">Create an Account</a>            

        <a class="close-reveal-modal">×</a>
    </div>


</div>


<script type="text/javascript">

    function openModal(el)
    {
        var instanceName = $(el).attr("data-reveal");
        if ($("#IsAccessPermitted" + instanceName).length && !isTrue($("#IsAccessPermitted" + instanceName).val())) { // when user is not logged in, show no-access modal for non-free articles
            var divId = $("#ModalDivID" + instanceName).val();
            var alertsModal = $('#' + divId);
            if (alertsModal != undefined)
            {
                alertsModal.foundation('reveal', 'open');
            }
        } else {
            var alertsModal = $("#getAlerts" + instanceName);
            if (alertsModal != undefined)
            {
                alertsModal.foundation('reveal', 'open');
            }
        }
    }

    function confirm(el) {
        var instanceName = $(el).attr("data-reveal");
        $("#m-alerts-email-spinner-" + instanceName).removeClass("hide");
        $("#confirmSaveChanges" + instanceName).off('click');
        submitForm(instanceName);
    }

    function submitForm(instanceName)
    {
        var modelValues = {
            UserEmail: $('#UserEmail' + instanceName).val(),
            ResourceTitle: $('#ResourceTitle' + instanceName).val(),
            AlertType: $('#AlertType' + instanceName).val(),
            CurrentUserEmail: $('#CurrentUserEmail' + instanceName).val(),
            IsUserAlreadySubscribed: isTrue($('#IsUserAlreadySubscribed' + instanceName).val()),
            SuccessfulUpdate: $('#SuccessfulUpdate' + instanceName).val(),
            ResourceId: $('#ResourceId' + instanceName).val(),
            ResourceType: $('#ResourceType' + instanceName).val(),
            ShowSaveSearchToUnauthenticatedUser: isTrue($('#ShowSaveSearchToUnauthenticatedUser' + instanceName).val()),
            UserIsLoggedIn: isTrue($('#UserIsLoggedIn' + instanceName).val()),
            CurrentAlertContextSiteId: $('#CurrentAlertContextSiteId' + instanceName).val()
        };

        $.post("/Toolbox/ToolboxGetAlertsUpdateMessage", modelValues, function (data) {
            $('#getAlertsConfirmation' + instanceName).html(data);
        }).done(function () {
            $("#m-alerts-email-spinner" + instanceName).addClass("hide");
        });
    }

    function isTrue(input) {
        if (typeof input == 'string') {
            return input.toLowerCase() == 'true';
        }
        //use false as default if not string
        return false;
    }

</script>
</li>
                        <li>    <div class="widget-ToolboxGetCitation widget-instance-ARVO_Get_Citation">
        <a href="#" data-reveal-id="getCitation" data-reveal=""><i class="icon-read-more"></i><span>Get Citation</span></a>

<div id="getCitation" class="reveal-modal" data-reveal="">
    <div class="modal-title">Citation</div>
    <p>Moreno I. Coco, Frank Keller; Classification of visual and linguistic tasks using eye-movement features. <em>Journal of Vision</em> 2014;14(3):11. doi: <a href="https://doi.org/10.1167/14.3.11">https://doi.org/10.1167/14.3.11</a>.</p>
    <p class="citation-label">Download citation file:</p>  
    <ul>
        <li><a href="http://jov.arvojournals.org/Citation/Download?resourceId=2121494&amp;resourceType=3&amp;citationFormat=0">Ris (Zotero)</a></li>
        <li><a href="http://jov.arvojournals.org/Citation/Download?resourceId=2121494&amp;resourceType=3&amp;citationFormat=1">EndNote</a></li>
        <li><a href="http://jov.arvojournals.org/Citation/Download?resourceId=2121494&amp;resourceType=3&amp;citationFormat=2">BibTex</a></li>
        <li><a href="http://jov.arvojournals.org/Citation/Download?resourceId=2121494&amp;resourceType=3&amp;citationFormat=3">Medlars</a></li>
        <li><a href="http://jov.arvojournals.org/Citation/Download?resourceId=2121494&amp;resourceType=3&amp;citationFormat=0">ProCite</a></li>
        <li><a href="http://jov.arvojournals.org/Citation/Download?resourceId=2121494&amp;resourceType=3&amp;citationFormat=3">RefWorks</a></li>
        <li><a href="http://jov.arvojournals.org/Citation/Download?resourceId=2121494&amp;resourceType=3&amp;citationFormat=0">Reference Manager</a></li>
    </ul>
    <hr>
    
    <p>© ARVO (1962-2015); The Authors (2016-present)</p>
    <a class="close-reveal-modal">×</a>
</div> 
    </div>
</li>
                        <li>    <div class="widget-ToolboxPermissions widget-instance-">
            <div class="module-widget">
            <a href="http://www.copyright.com/openurl.do?issn=15347362" id="PermissionsLink" class="" target="_blank"><i class="icon-permissions"></i><span>Get Permissions</span></a>
    </div>
 
    </div>
</li>
                    </ul>
                </li>
                
                <li class="toolbar-item toolbar-search" data-target="#Search">
                    <a><i class="icon-search"></i></a>
                </li>
            </ul>
            
            <section class="tabs-content">
                <div id="ContentTab" class="content active">
                        <div class="widget-ArticleFulltext widget-instance-ARVO_ArticleFullText_Widget">
        <div class="module-widget">
    <div class="widget-items" data-widgetname="ArticleFulltext">

                    <a id="87793181"></a>
                    <div class="h6" data-magellan-destination="87793181">Abstract</div>
            <div>
                <div class="content-section clearfix ">
                    
                    <section class="abstract"><div class="h6 abstract-title">Abstract</div> <strong>Abstract</strong>:
     <p><strong>Abstract</strong>&nbsp;
                    <strong>The role of the task has received special 
attention in visual-cognition research because it can provide causal 
explanations of goal-directed eye-movement responses. The dependency 
between visual attention and task suggests that eye movements can be 
used to classify the task being performed. A recent study by Greene, 
Liu, and Wolfe <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>,
 however, fails to achieve accurate classification of visual tasks based
 on eye-movement features. In the present study, we hypothesize that 
tasks can be successfully classified when they differ with respect to 
the involvement of other cognitive domains, such as language processing.
 We extract the eye-movement features used by Greene et al. as well as 
additional features from the data of three different tasks: visual 
search, object naming, and scene description. First, we demonstrated 
that eye-movement responses make it possible to characterize the goals 
of these tasks. Then, we trained three different types of classifiers 
and predicted the task participants performed with an accuracy well 
above chance (a maximum of 88% for visual search). An analysis of the 
relative importance of features for classification accuracy reveals that
 just one feature, i.e., initiation time, is sufficient for above-chance
 performance (a maximum of 79% accuracy in object naming). Crucially, 
this feature is independent of task duration, which differs 
systematically across the three tasks we investigated. Overall, the best
 task classification performance was obtained with a set of seven 
features that included both spatial information (e.g., entropy of 
attention allocation) and temporal components (e.g., total fixation on 
objects) of the eye-movement record. This result confirms the 
task-dependent allocation of visual attention and extends previous work 
by showing that task classification is possible when tasks differ in the
 cognitive processes involved (purely visual tasks such as search vs. 
communicative tasks such as scene description).</strong></p> </section>
                </div>
                <a id="s1"></a>
                </div>
            <a id="87793182"></a>
                    <div class="h6" data-magellan-destination="87793182">Introduction</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Visual attention actively serves 
the cognitive system in a wide range of different tasks and everyday 
activities. Each task entails a well-defined sequence of steps to be 
accomplished. In order to inform this process, specific task-related 
information has to be extracted by the visual system from the visual 
percept.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The role of the task in visual attention has attracted the interest of vision researchers from very early on. Buswell <a reveal-id="i1534-7362-14-3-11-buswell1" class="revealLink refLink">(1935)</a>
 was the first one to investigate eye movements with complex scenes and 
to show that expertise in a certain task (being an artist or not) 
influences the associated eye-movement responses. A few decades later, 
Yarbus <a reveal-id="i1534-7362-14-3-11-yarbus1" class="revealLink refLink">(1967)</a>
 confirmed that indeed task plays a key role in the eye-movement 
patterns observed. Different task instructions, such as “estimate the 
material circumstances of the family shown in the picture” versus “give 
the ages of the people shown in the picture,” resulted in qualitatively 
different eye-movement trajectories, often referred to as <em>scan paths</em> (Noton &amp; Stark, <a reveal-id="i1534-7362-14-3-11-noton1" class="revealLink refLink">1971</a>) or scan patterns (Henderson, <a reveal-id="i1534-7362-14-3-11-henderson1" class="revealLink refLink">2003</a>).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The key message of this seminal 
work was that eye-movement patterns provide evidence about a possible 
causal model of the task being performed. Thus, it should be possible to
 infer the underlying attentional mechanisms by comparing eye-movement 
patterns across tasks.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">More recent work in visual 
cognition is motivated by the aim of understanding the visual system in 
ecologically valid real-world tasks, such as making tea or washing 
hands; sport activities, such as playing table tennis or driving; as 
well as in computer-simulated, virtual scenarios (Ballard &amp; Hayhoe, <a reveal-id="i1534-7362-14-3-11-ballard1" class="revealLink refLink">2009</a>; Ballard, Hayhoe, &amp; Pelz, <a reveal-id="i1534-7362-14-3-11-ballard2" class="revealLink refLink">1995</a>; Hagemann, Schorer, Cañal-Bruland, Lotz, &amp; Strauss, <a reveal-id="i1534-7362-14-3-11-hagemann1" class="revealLink refLink">2010</a>; Land &amp; Furneaux, <a reveal-id="i1534-7362-14-3-11-land1" class="revealLink refLink">1997</a>; Land &amp; Hayhoe, <a reveal-id="i1534-7362-14-3-11-land2" class="revealLink refLink">2001</a>; Land &amp; McLeod, <a reveal-id="i1534-7362-14-3-11-land3" class="revealLink refLink">2000</a>; Land, Mennie, &amp; Rusted, <a reveal-id="i1534-7362-14-3-11-land4" class="revealLink refLink">1999</a>; Pelz &amp; Canosa, <a reveal-id="i1534-7362-14-3-11-pelz1" class="revealLink refLink">2001</a>; Rothkopf, Ballard, &amp; Hayhoe, <a reveal-id="i1534-7362-14-3-11-rothkopf1" class="revealLink refLink">2007</a>).
 This research has demonstrated that eye-movement responses are launched
 preferentially to task-relevant objects during precisely time-locked 
stages of the task, e.g., looking at the spout of a kettle when pouring 
during a tea-making task (Land et al., <a reveal-id="i1534-7362-14-3-11-land4" class="revealLink refLink">1999</a>).
 The memorability of attended information has also been shown to depend 
on its task-relevance. Triesch, Ballard, Hayhoe, and Sullivan <a reveal-id="i1534-7362-14-3-11-triesch1" class="revealLink refLink">(2003)</a>,
 for example, showed that participants became aware of changes occurring
 to an attended object only when such object was relevant, at that 
particular moment, for the task.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Effects of task have also been 
observed in other visual activities, such as search or memorization 
(Castelhano, Mack, &amp; Henderson, <a reveal-id="i1534-7362-14-3-11-castelhano2" class="revealLink refLink">2009</a>; Mills, Hollingworth, Van der Stigchel, Hoffman, &amp; Dodd, <a reveal-id="i1534-7362-14-3-11-mills1" class="revealLink refLink">2011</a>; Tatler, Baddeley, &amp; Vincent, <a reveal-id="i1534-7362-14-3-11-tatler1" class="revealLink refLink">2006</a>), in which photo-realistic 2-D scenes were mainly used as contexts. Castelhano et al. <a reveal-id="i1534-7362-14-3-11-castelhano2" class="revealLink refLink">(2009)</a>,
 for example, compared several measures, such as the area of the scene 
inspected, for eye-movement data collected during a visual search task 
(find a MUG in a kitchen scene) and during a memorization task (memorize
 the scene in preparation for a later recall). They found significant 
differences between the two tasks, e.g., more regions of the scene were 
inspected during memorization than during search. In a memorization 
task, participants attempt to inspect as many objects as possible within
 the preview time with the aim of maximizing the number of items that 
could be recalled whereas, in a search task, participants focus on 
contextually relevant regions of the scene to maximize the likelihood of
 finding the target object (see also Castelhano &amp; Heaven, <a reveal-id="i1534-7362-14-3-11-castelhano1" class="revealLink refLink">2010</a> and Malcolm &amp; Henderson, <a reveal-id="i1534-7362-14-3-11-malcolm2" class="revealLink refLink">2010</a>
 for evidence of top-down contextual-target guidance in visual search). 
Moreover, also in purely visual tasks, the task-relevance of scene 
information exerts direct control on eye-movement responses, for 
example, on the duration of the first fixation (Glaholt &amp; Reingold, <a reveal-id="i1534-7362-14-3-11-glaholt1" class="revealLink refLink">2012</a>; see, however, Salverda &amp; Altmann, <a reveal-id="i1534-7362-14-3-11-salverda1" class="revealLink refLink">2011</a>,
 in which task-irrelevant information is also shown to impact fixation 
duration). The influence of direct cognitive control on visual attention
 is a strong indicator that task differences should be observed in the 
associated eye-movement pattern.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The causal dependence of task and 
eye-movement responses extends also to other cognitive activities, such 
as reading. Research in this area has clearly shown that eye-movement 
responses are strongly modulated by linguistic properties of the text, 
such as word frequency (Inhoff &amp; Rayner, <a reveal-id="i1534-7362-14-3-11-inhoff1" class="revealLink refLink">1986</a>),
 and more general task demands, such as silent reading versus reading 
aloud. Moreover, eye-movement patterns in reading significantly differ 
from those observed during scene perception. The average length of a 
saccade is, for example, longer in scene perception than in reading (see
 Rayner, <a reveal-id="i1534-7362-14-3-11-rayner1" class="revealLink refLink">2009</a>, for a review on the topic).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Task is therefore a major factor 
that needs to be considered when interpreting eye-movement responses. In
 fact, by understanding the properties of a task and its underlying 
goals, we should be able to accurately estimate which objects (or words)
 are attended and when this should happen during the task (Ballard &amp;
 Hayhoe, <a reveal-id="i1534-7362-14-3-11-ballard1" class="revealLink refLink">2009</a>).
 If this is correct, then the inverse inference should also be possible:
 Eye-movement responses should be informative of the task being 
performed. In particular, the statistics of eye-movement responses 
should be predictive of the task.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">A recent study by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 addressed this question by explicitly testing whether the task 
performed by the participants could be accurately determined from the 
associated eye-movement information (see also Henderson, Shinkareva, 
Wang, Luke, &amp; Olejarczyk, <a reveal-id="i1534-7362-14-3-11-henderson2" class="revealLink refLink">2013</a> for another example of eye movement–based task classification). This study followed up on work by <a reveal-id="i1534-7362-14-3-11-deangelus1" class="revealLink refLink">DeAngelus and Pelz (2009)</a>, in which <a reveal-id="i1534-7362-14-3-11-yarbus1" class="revealLink refLink">Yarbus's (1967)</a>
 qualitative findings were successfully replicated and more stringently 
quantified, using modern eye-tracking technology, a larger set of 
participants (25 instead of just one) by comparing task differences with
 and without self-termination (3 min of fixed viewing per scene).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Participants in Greene et al.'s <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 study were asked to perform four types of visual tasks (memorize the 
picture, determine the decade the picture was taken in, determine 
people's wealth and social context); the study used black-and-white 
historical scenes selected from the Time Life archive on Google (<a href="http://images.google.com/hosted/life" target="_blank">http://images.google.com/hosted/life</a>).
 From the eye-movement data collected during the different tasks for 
each individual trial, Greene et al. extracted seven distinct features 
(e.g., area of the scene fixated), reported also in previous studies 
(Castelhano et al., <a reveal-id="i1534-7362-14-3-11-castelhano2" class="revealLink refLink">2009</a>; Einhäuser, Spain, &amp; Perona, <a reveal-id="i1534-7362-14-3-11-einhauser1" class="revealLink refLink">2008</a>; Mills et al., <a reveal-id="i1534-7362-14-3-11-mills1" class="revealLink refLink">2011</a>).
 This set of features was used to train different regression-based 
classifiers (e.g., support vector machines) in order to automatically 
determine which of the four tasks was performed in a given trial.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Surprisingly, Greene et al.'s <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 results show that none of the classifiers they utilized was able to 
detect the task performed using eye-movement features with an accuracy 
above chance. This result seems at odds with DeAngelus and Pelz <a reveal-id="i1534-7362-14-3-11-deangelus1" class="revealLink refLink">(2009)</a>,
 in which task differences were clearly observed (e.g., the distance of 
scan-path trajectories between tasks was significantly larger than 
between observers). More generally, the result of Greene et al. 
undermines previous claims about task-dependent allocation of visual 
attention and challenges the widespread assumption that each task gives 
rise to a distinct pattern of eye-movement responses.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">However, the fact that Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 did not observe task-specific patterns of eye movement might not mean 
that such task differences do not exist but rather that the tasks 
performed by Greene et al.'s participants were not distinct enough to 
produce separable patterns of visual attention. One, mostly technical, 
explanation for the null result of Greene et al.'s study might be that 
all tasks had a fixed termination time (10 s per trial). The fixed 
viewing time might have flattened any implicit temporal variability 
between tasks. This hypothesis can be deduced from the study of 
DeAngelus and Pelz <a reveal-id="i1534-7362-14-3-11-deangelus1" class="revealLink refLink">(2009)</a>,
 in which it is clear that different tasks trigger different 
self-termination times. However, if this is the only explanation of the 
null result, then it should be impossible to accurately classify tasks 
using eye-movement features that are insensitive to self-termination, 
such as initiation time (i.e., the time it takes to launch the first eye
 movement) or the average length of a saccade.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Another, perhaps stronger, alternative hypothesis for the null result is that all tasks performed in Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 demanded only visual processing and did not require the involvement of 
other cognitive modalities (e.g., language processing). Thus, it seems 
likely that similar strategies of attention allocation are adopted when 
participants perform similar visual tasks. Greene et al.'s results 
therefore leave open the possibility that eye-movement patterns become 
distinctive when tasks differ substantially with respect to the 
cognitive processes they involve. This hypothesis would also better 
align with the literature on natural gaze control during real-world 
tasks, during which visual attention always occurs jointly with motor 
actions and attention allocation strongly depends on the task goals 
(e.g., Pelz &amp; Canosa, <a reveal-id="i1534-7362-14-3-11-pelz1" class="revealLink refLink">2001</a>).
 Visual attention does not only co-occur with motor actions, but it also
 often co-occurs with language in tasks such as describing the function 
of a device or giving directions on a map. Thus, we can hypothesize that
 reliable task differences can be observed between eye-movement patterns
 for purely visual tasks (e.g., visual search) and communicative tasks 
(scene description), which require the concurrent processing of visual 
and linguistic information.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Psycholinguistic research on 
linguistic tasks situated in visual scenes has, in fact, convincingly 
demonstrated that the allocation of visual attention and the processing 
of linguistic information are mutually interdependent: The mention of a 
visual referent occurs time-locked with fixations on the relevant object
 in a scene in both language comprehension and language production 
(e.g., Gleitman, January, Nappa, &amp; Trueswell, <a reveal-id="i1534-7362-14-3-11-gleitman1" class="revealLink refLink">2007</a>; Griffin &amp; Bock, <a reveal-id="i1534-7362-14-3-11-griffin1" class="revealLink refLink">2000</a>; Tanenhaus, Eberhard, &amp; Sedivy, <a reveal-id="i1534-7362-14-3-11-tanenhaus1" class="revealLink refLink">1995</a>).
 In scene-description tasks, for instance, the time-locking of speech 
and eye movements means that sentence similarity correlates with the 
similarity of the associated scan patterns (Coco &amp; Keller, <a reveal-id="i1534-7362-14-3-11-coco2" class="revealLink refLink">2012</a>).
 This suggests that eye-movement responses carry detailed information 
about the task that is performed; in fact, Coco and Keller <a reveal-id="i1534-7362-14-3-11-coco2" class="revealLink refLink">(2012)</a>
 show that it is possible, with accuracy above chance, to determine 
which sentence was spoken based on the scan pattern that was followed 
when speaking it.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The aim of the present study is to
 demonstrate that different tasks are characterized by distinct patterns
 of eye-movement responses and that the accurate classification of tasks
 is possible provided that the tasks differ substantially with respect 
to the cognitive processes involved in accomplishing them. In 
particular, we compare a visual search task with object-naming and 
scene-description tasks, both of which require the concurrent processing
 of visual and linguistic information. The three tasks considered vary 
by the amount of cross-modal processing involved: (a) Search is expected
 to mostly require visual processing, (b) naming also demands the 
activation of linguistic information (the names of objects need to be 
retrieved and uttered), and (c) description requires visual and 
linguistic processing to be integrated fully as sentences and scan 
patterns correlate closely (Coco &amp; Keller, <a reveal-id="i1534-7362-14-3-11-coco2" class="revealLink refLink">2012</a>).
 This key difference is predicted to lead to distinct eye-movement 
patterns, which would make it possible to classify all three tasks with 
an accuracy greater than chance. However, we also predict classification
 accuracy to degrade when more cross-modal processing is required. In 
that case, eye-movement features need to be used in conjunction with 
linguistic features to achieve good task classification 
performance.&nbsp;</div>
                </div>
                <a id="s2"></a>
                </div>
            <a id="87793197"></a>
                    <div class="h6" data-magellan-destination="87793197">The present study</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We conducted three eye-tracking 
experiments involving visual search, object naming, and scene 
description; each task was performed by a different group of 
participants. From the eye-movement data of each trial, we extracted the
 seven features used by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a> (we will refer to these features as <em>GF</em>) as well as other eye-movement features (referred to as <em>OF</em>); these will be explained in more detail in the Features section below.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We compare how visual search, 
object naming, and scene description affect eye movements using 
linear–mixed effect modeling. This analysis allows us to infer how the 
goals of a given task determine which objects need to be attended to 
perform the task, thus explaining why different eye-movement patterns 
emerge across tasks.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We then train three different 
types of regression models (multinomial regression, least-square angle 
regression, and support vector machines) on the eye-movement features in
 order to automatically classify the task used. When training the 
classifiers, we use either only <em>GF</em>, only <em>OF</em>, or all 
features. We test the accuracy of the classification models using a 
tenfold cross-validation procedure. The results show that all models, 
using all three feature sets, are able to predict the task with an 
accuracy well above chance.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Moreover, in order to test whether
 certain features are more predictive than others and to investigate how
 many features are needed to obtain a classification accuracy above 
chance, we run a stepwise forward model-building procedure in which 
features are ordered by their classification performance (best first) 
over the 10 cross-validation folds. We track how classification accuracy
 changes as a function of the features used and find that already a 
single feature is enough to distinguish among tasks above chance.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We conclude the study by 
demonstrating that eye-movement features insensitive to 
self-termination, such as initiation time and saccade length, are 
sufficient to classify the tasks well above chance. This additional 
analysis rules out the possibility that fixed termination is what caused
 the null result observed by the Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 study, hence suggesting that tasks can be accurately classified as long
 as they substantially differ in the types of cognitive processes needed
 to perform them.&nbsp;</div>
                </div>
                <a id="s2a"></a>
                </div>
            <a id="87793203"></a>
                    <div class="h7" data-magellan-destination="87793203">Tasks</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <a id="s2a1"></a>
                </div>
            <a id="87793204"></a>
                    <div class="h8" data-magellan-destination="87793204">Visual search</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Participants were asked to count how many instances (between one and three) of a target object are present in the scene.<a href="#n1"><sup>1</sup></a>
 The target object was cued prior to scene onset for 750 ms, using a 
word naming the object displayed in the center of the screen. The target
 object was either animate or inanimate (each half of the time). 
Participants could freely inspect the scene without any time constraints
 and then self-terminate the trial by pressing a response button to 
indicate the number of targets seen. Once every four trials, a 
comprehension question about the number of target objects present in the
 scene was asked. This data set was published by Dziemianko, Keller, and
 Coco <a reveal-id="i1534-7362-14-3-11-dziemianko1" class="revealLink refLink">(2009)</a>.&nbsp;</div>
                </div>
                <a id="s2a2"></a>
                </div>
            <a id="87793206"></a>
                    <div class="h8" data-magellan-destination="87793206">Scene description</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The same scenes and the same 
target objects were used as in the visual search task, but now 
participants were asked to generate a spoken description of the target 
object. The target object was cued in the same way (a word naming the 
object was displayed for 750 ms prior to scene onset). Again, 
participants could freely inspect the scene without any time 
constraints; once they had finished speaking, they pressed a response 
button to trigger the next trial. This data set was published by Coco 
and Keller <a reveal-id="i1534-7362-14-3-11-coco2" class="revealLink refLink">(2012)</a>.&nbsp;</div>
                </div>
                <a id="s2a3"></a>
                </div>
            <a id="87793208"></a>
                    <div class="h8" data-magellan-destination="87793208">Object naming</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The same scenes as in the previous
 two tasks were used, but now no cue was presented to participants. 
Instead, participants were asked to name at least five objects in the 
scene by speaking words describing them. Participants had 1500 ms 
preview of the scene after which a beep was played to prompt them to 
start naming. The scene was visually available during the whole trial, 
i.e., for preview and naming. Again, they were under no time pressure 
and had to press a response button to proceed to the next trial. This 
data set was collected as filler trials in Coco, Malcolm, and Keller's <a reveal-id="i1534-7362-14-3-11-coco3" class="revealLink refLink">(2013)</a> study.&nbsp;</div>
                </div>
                <a id="s2b"></a>
                </div>
            <a id="87793210"></a>
                    <div class="h7" data-magellan-destination="87793210">Materials</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We created 24 photo-realistic 
scenes drawn from six different indoor scenarios (e.g., bathroom, 
bedroom), four scenes per scenario, using Photoshop (see <a reveal-id="i1534-7362-14-3-11-f01" class="revealLink tablelink">Figure 1</a> for an example scene<a href="#n2"><sup>2</sup></a>).
 Scene clutter was manipulated: There were two versions of each scene 
with either low clutter or high clutter as estimated using the Feature 
Congestion measure of Rosenholtz, Li, and Nakano <a reveal-id="i1534-7362-14-3-11-rosenholtz1" class="revealLink refLink">(2007)</a>. Note that the clutter manipulation was part of the original studies that generated our data sets (Coco &amp; Keller, <a reveal-id="i1534-7362-14-3-11-coco2" class="revealLink refLink">2012</a>; Dziemianko et al., <a reveal-id="i1534-7362-14-3-11-dziemianko1" class="revealLink refLink">2009</a>);
 it is not of relevance for the present study. However, in order to make
 sure that classification performances were consistent in both versions 
of the scene, we trained and tested the classifiers on scenes split by 
clutter (high and low) and showed that the classification results were 
consistent between the two sets.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-f01"></a>
                <div class="content-section clearfix ">
                    
                    <div data-id="i1534-7362-14-3-11-f01" class="figure-section"><div class="title"><span class="label">Figure 1</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f01" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=pPIGkOxWuk4LDYr-JQ2Jjd~8FBP4cQpht4A7momyvmfhbDrtfmxnh4KgTsMdkMetbmWrxWuXWnL~Hwxagzwzz1v4JZil~OT0ZGCnb3IdLqdPLH2ZtbHGp9OlQcfFYTK~WIizsED03qIccBrv42eo4RRh7YpmPoiFr-vLAdNK~~g0rJ-kNZLlzsWUrEnJdRnW-kZkyn1sqXsPJiMJLGZPqh63c8di1JHjuBxk4WOYIvkE7-1Rb8WRQ7wMVRrgqKONmdSjonXkIW31sTpZQZPtLetmGMvQm7F4IowGBJs1RqYml0SXXDmRMs7cMtI1uOl54OJ4tz8x62GgVdp46WxmKQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="An example of a scene used in the different tasks: low-clutter condition on the left, high-clutter condition on the right. The cued target objects were GIRL and TEDDY. The face is blanked out to protect the identity of the photographed character. The image is an original created with PhotoshopC2 using components that are in the public domain sources (e.g., Flickr)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f01"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f01" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793212&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f01">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">An
 example of a scene used in the different tasks: low-clutter condition 
on the left, high-clutter condition on the right. The cued target 
objects were GIRL and TEDDY. The face is blanked out to protect the 
identity of the photographed character. The image is an original created
 with PhotoshopC2 using components that are in the public domain sources
 (e.g., Flickr).</div></div></div></div><div content-id="i1534-7362-14-3-11-f01" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 1</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">An
 example of a scene used in the different tasks: low-clutter condition 
on the left, high-clutter condition on the right. The cued target 
objects were GIRL and TEDDY. The face is blanked out to protect the 
identity of the photographed character. The image is an original created
 with PhotoshopC2 using components that are in the public domain sources
 (e.g., Flickr).</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="An example of a scene used in the different tasks: low-clutter condition on the left, high-clutter condition on the right. The cued target objects were GIRL and TEDDY. The face is blanked out to protect the identity of the photographed character. The image is an original created with PhotoshopC2 using components that are in the public domain sources (e.g., Flickr)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f01"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f01" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793212&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f01">Download Slide</a></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The cued object used in the visual
 search and scene-description task was either animate (GIRL in this 
example) or inanimate (TEDDY in this example). The cue was always 
referentially ambiguous with respect to the scene: In this case, two 
GIRLS and two TEDDIES are depicted. (Again, this feature was of interest
 in the studies that originated the data but will be ignored in the 
following.) A Latin square design was used to make sure that each scene 
was only seen in one of the four conditions (cue either animate or 
inanimate, clutter either low or high) by each participant.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We use LabelMe (Russell, Torralba, Murphy, &amp; Freeman, <a reveal-id="i1534-7362-14-3-11-russell1" class="revealLink refLink">2008</a>)
 to fully annotate each scene with the objects of which it is made. 
Objects at the border of the scene were annotated using background 
generic labels, such as wall or floor.<a href="#n3"><sup>3</sup></a> 
Low-clutter scenes had a mean density of 3.10 ± 0.22 and mean number of 
objects 27.42 ± 9.93 whereas, in high-clutter scenes, the mean density 
is 3.90 ± 0.24, and the number of objects is 28.65 ± 11.30. We mapped 
x-y fixation coordinates onto the corresponding objects. However, as 
objects can be nested, e.g., the TEDDY polygon is embedded into the GIRL
 polygon, we use the size of the object in pixels squared to assign the 
fixation to the smallest object, i.e., TEDDY in this working example. 
This makes sure that features at fixation are not redundantly computed 
over nested objects.&nbsp;</div>
                </div>
                <a id="s2c"></a>
                </div>
            <a id="87793215"></a>
                    <div class="h7" data-magellan-destination="87793215">Participants</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Seventy-four (25 each for search 
and description, 24 for naming) native speakers of English, all students
 of the University of Edinburgh, gave informed consent to take part to 
the experiments and were each paid five pounds.&nbsp;</div>
                </div>
                <a id="s2d"></a>
                </div>
            <a id="87793217"></a>
                    <div class="h7" data-magellan-destination="87793217">Apparatus and procedure</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">An EyeLink II head-mounted 
eye-tracker was used to monitor participants' eye movements with a 
sampling rate of 500 Hz. Images were presented on a 21-in. multiscan 
monitor at a resolution of 1024 × 768 pixels. Participants sat between 
60 and 70 cm from the computer screen, which subtended approximately 20°
 of visual angle. A nine-point randomized calibration was performed at 
the beginning of the experiment and repeated halfway through the 
experimental session. Drift correction was performed at the beginning of
 each trial. The tasks were explained to participants using written 
instructions; the experiment took about 30 min to complete.&nbsp;</div>
                </div>
                <a id="s2e"></a>
                </div>
            <a id="87793219"></a>
                    <div class="h7" data-magellan-destination="87793219">Features</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The full data set contains a total
 of 1,756 unique trials, which are divided across the three tasks as 
follows: search (580), description (600), naming (576). Approximately 3%
 (20 trials) of the visual search data was lost due to machine 
error.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">From the eye-movement data of each trial, we extract the seven features used by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>:
 (a) number of fixations, (b) mean fixation duration, (c) mean saccade 
amplitude, and (d) percent of image covered by fixations assuming a 1° 
circle around the fixation position (Castelhano et al., <a reveal-id="i1534-7362-14-3-11-castelhano2" class="revealLink refLink">2009</a>; Einhäuser et al., <a reveal-id="i1534-7362-14-3-11-einhauser1" class="revealLink refLink">2008</a>; Mills et al., <a reveal-id="i1534-7362-14-3-11-mills1" class="revealLink refLink">2011</a>).
 As two fixations can fall in close proximity to each other, the areas 
of the two circles may overlap. In this case, we subtract the area of 
the intersection. Following Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>,
 we also calculated the proportion of dwell time on (e) faces, (f) 
bodies, and (g) objects, i.e., any other region annotated in the scene 
that was not a human.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In addition to the Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 feature set (GF), we extracted another set of 15 features (OF): (a) 
latency of first fixation; (b) first fixation duration; (c) mean 
fixation duration; (d) total gaze duration on faces, bodies, and objects
 (four features for three different regions, i.e., 12 features in 
total); (e) the initiation time, which is the time spent after scene 
onset before the first saccade is launched (a measure used by Malcolm 
&amp; Henderson, <a reveal-id="i1534-7362-14-3-11-malcolm1" class="revealLink refLink">2009</a>); (f) mean saliency at the fixation location; and (g) the entropy of the attentional landscape.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">For the mean saliency measure, we 
computed a saliency map of each scene using the model developed by 
Torralba, Oliva, Castelhano, and Henderson <a reveal-id="i1534-7362-14-3-11-torralba1" class="revealLink refLink">(2006)</a><a href="#n4"><sup>4</sup></a>
 and mapped each x-y fixation position onto the saliency value at that 
location. We then took the mean saliency value of the fixations in the 
trial. To calculate the entropy measure, we first computed attentional 
landscapes by fitting 2-D Gaussians on the x-y coordinates of each 
fixation with the height of the Gaussian weighted by fixation duration 
and a radius of 1° of visual angle (roughly 27 pixels) to approximate 
the size of the fovea (e.g., Henderson, <a reveal-id="i1534-7362-14-3-11-henderson1" class="revealLink refLink">2003</a>; Pomplun, Ritter, &amp; Velichkvosky, <a reveal-id="i1534-7362-14-3-11-pomplun1" class="revealLink refLink">1996</a>). The entropy of the map was then calculated as ∑<em><sub>x</sub></em><sub>,<em>y</em></sub><em>p</em>(<em>L<sub>x</sub></em><sub>,<em>y</em></sub>)log<sub>2</sub><em>p</em>(<em>L<sub>x</sub></em><sub>,<em>y</em></sub>), where <em>p</em>(<em>L<sub>x</sub></em><sub>,<em>y</em></sub>) is the normalized fixation probability at point (<em>x</em>, <em>y</em>) in the landscape <em>L</em>.
 Conceptually, entropy measures the spread of information and the 
“uncertainty” we have about it. Thus, the higher the entropy, the more 
spread out fixations in the scene are, i.e., the more distinct locations
 have been attended.&nbsp;</div>
                </div>
                <a id="s2f"></a>
                </div>
            <a id="87793224"></a>
                    <div class="h7" data-magellan-destination="87793224">Methods for analysis and classification</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">First, we investigated how tasks 
differed in their associated eye-movement features and derived a causal 
task-based interpretation for it. In particular, we examined all GF 
features, and in the OF feature set, we focused on initiation time, 
saliency, and entropy, which are not region-specific. We built a 
linear–mixed effects model for each feature as a function of <em>Task</em> (Search, Naming, or Description) using the R package lme4 (Bates, Maechler, &amp; Bolker, <a reveal-id="i1534-7362-14-3-11-bates1" class="revealLink refLink">2011</a>). As <em>Task</em>
 is a three-level, categorical variable, we needed to create a contrast 
coding and chose one of the factors as a reference level. This factor 
could then be used to compare with the other two levels. We chose Naming
 as the reference level as it is the simplest linguistic task and 
contrast it with Search and Description. In building our models, we 
followed Barr, Levy, Scheepers, and Tily <a reveal-id="i1534-7362-14-3-11-barr1" class="revealLink refLink">(2013)</a>
 and chose a maximal-random structure, in which each random variable of 
the design (e.g., Participants) is introduced as intercept and as slope 
on the predictors of interest (e.g., Search vs. Naming). The random 
variables of our design are Participants (74) and Scenes (48 as we have 
24 scenes in two conditions of visual clutter). We report tables with 
the coefficients of the predictors, their standard error; significance 
is provided by computing <em>p</em> values using the function pvals.fnc from the languageR package (Baayen, Davidson, &amp; Bates, <a reveal-id="i1534-7362-14-3-11-baayen1" class="revealLink refLink">2008</a>).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Then, we used all sets of features
 described above to train three different types of classifiers, all 
implemented in R. We trained a multinomial log-linear neural networks 
model (MM; multinom function in R's nnet package, Venables &amp; Ripley,
 <a reveal-id="i1534-7362-14-3-11-venables1" class="revealLink refLink">2002</a>);
 a generalized linear model with penalized maximum likelihood, in which 
the regularization path is computed for the least angle (LASSO; glmnet 
in R's glmnet package, Friedman, Hastie, &amp; Tibshirani, <a reveal-id="i1534-7362-14-3-11-friedman1" class="revealLink refLink">2010</a>); and a support vector machine (SVM; ksvm in R's kernlab package, Chang &amp; Lin, <a reveal-id="i1534-7362-14-3-11-chang1" class="revealLink refLink">2011</a>).
 Note that we used the default setting for all three different models 
(higher classification accuracy could presumably be achieved by 
parameters tuning).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The classifiers were trained and 
tested using tenfold cross-validation, in which the model is trained on 
90% of the data and then tested in the remaining 10%; this process is 
repeated 10 times so that each fold functions as test data exactly once.
 This is a way to avoid overfitting the training data while still making
 maximal use of a small data set. We trained the classifiers on the data
 divided by clutter condition (low, high). This makes sure that we 
treated the two sets of related images independently.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We measured the accuracy of the classifiers using <em>F</em>-score, which is the geometric mean of precision and recall and is defined as <em>F</em> = 2 · (<em>P</em> · <em>R</em>)/(<em>P</em> + <em>R</em>). Precision (<em>P</em>)
 is the number of correctly classified instances over the total number 
of instances labeled as belonging to the class, defined as <em>tp</em>/(<em>tp</em> + <em>fp</em>). Here, <em>tp</em> is the number of true positives (i.e., instances of the class that were correctly identified as such), and <em>fp</em>
 is the number of false positives (i.e., instances that were labeled as 
members of the class even though they belong to a different class). 
Recall (<em>R</em>) is the number of correctly classified instances over the total number of instances in that class, defined as <em>tp</em>/(<em>tp</em> + <em>fn</em>), where <em>fn</em>
 is the number of false negatives, i.e., the number of instances that 
were labeled as nonmembers of the class even though they belong to the 
class. It is important to consider both precision and recall as a high 
precision can be achieved simply by underpredicting the class and being 
right most of the time when a prediction is made. However, in this case,
 recall will be low. The inverse is true for a classifier that 
overpredicts a class; in this case, recall will be high, but precision 
will be low.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Note also that precision and recall (and thus <em>F</em>-score) are relative to a given class; as we have three classes, we report a separate <em>F</em>-score
 value for each of them. Furthermore, we report results using three 
different sets of features: GF alone, OF alone, and all features. We ran
 this classification on all the data and made sure that data belonging 
to a scene in a certain clutter condition (high and low) was used either
 for training or testing and used a <em>t</em> test to determine whether a larger feature set led to a significant improvement in <em>F</em>-score.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">For comparability with Greene et al.'s <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 study, we also calculated the accuracy of the classifiers when trained 
to predict which image was viewed (here, we collapsed the clutter 
condition and considered only 24 scenes). Again following Greene et al.,
 we also trained the classifiers to predict which participant generated 
the data (74 participants).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Moreover, in order to provide a 
more detailed analysis of the impact of the features on classification 
performance, we used the classifier that gave us the best performance 
(SVM) and tested (a) how many features were needed to achieve a 
classification performance above chance, and (b) which features were 
important for discriminating between tasks. For the first analysis, we 
used a stepwise forward model-building procedure, and at each step we 
added the feature that maximized <em>F</em>-score classification performance and tracked changes in <em>F</em>-score as more features were included. We repeated this procedure over the 10 folds and then plotted the mean <em>F</em>-score
 obtained when each new feature was added. In the second analysis, we 
performed the same stepwise model-building procedure but evaluated 
whether the model with the added feature is significantly better than 
the one without it. If there was no significant improvement on the <em>F</em>-score,
 we retained the model without that feature. Again, we repeated this 
procedure over the 10 folds and retained the feature set of the final 
model obtained for each fold. We report the frequency of observing a 
certain feature in the final feature set over the 10 folds over 10 
iterations (100 final feature sets in total; the folds are randomly 
regenerated at every iteration to make sure that the data is 
homogeneously sampled). This measure gives us a rough estimate of how 
important a feature is for discriminating among tasks.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We conclude the <a href="#s3" class="sectionLink">Results</a>
 section by looking at whether features that are either independent of 
self-termination, such as initiation time, or purely spatial, such as 
mean saccade amplitude, are sufficient to obtain accurate classification
 performance. This test is there to make sure that it is not only the 
temporal component of the eye-movement data that enables us to 
accurately classify tasks. The temporal features (e.g., the number of 
fixations) are affected by self-determination, i.e., by whether the 
participant is able to terminate the task, or whether the task is of 
fixed length. (Recall that all our tasks used self-termination.)&nbsp;</div>
                </div>
                <a id="s3"></a>
                </div>
            <a id="87793233"></a>
                    <div class="h6" data-magellan-destination="87793233">Results and discussion</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We start by examining the patterns
 observed for the different eye-movement features in each task. Then, we
 move on to the results obtained using these features when performing 
task classification.&nbsp;</div>
                </div>
                <a id="s3a"></a>
                </div>
            <a id="87793235"></a>
                    <div class="h7" data-magellan-destination="87793235">Features mediating task variability</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In <a reveal-id="i1534-7362-14-3-11-f02" class="revealLink tablelink">Figure 2</a>, we plot mean and standard error for GF, the set of features proposed by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>, for our three different tasks.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-f02"></a>
                <div class="content-section clearfix ">
                    
                    <div data-id="i1534-7362-14-3-11-f02" class="figure-section"><div class="title"><span class="label">Figure 2</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f02" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=q82nxPp8byaD~Hnm8IJevSwSm0hKRMGR9J-KbIwy9yL5IclfvYfOvUS0~DYO4LOA4p0c6vmOwSDVAmPjsF2BRYPwKWvYnzcOvuN67MqWDr51Aei~B0TTOIBtzTozauiei2NEGB2qtLbHRlp-ciJFf~xAuMF3BqCyKmPGnZDR8zp1qNqJCROJcOwuM3km9c2JclaoyZN76ON3FmkMv8tO-RLKv6cpflczT4BdEgoXlwEOly-J1kXvLoxhQbHnJfcl3V2drL6~t8ljB8iQTh~4msnztAuoZyw-9bHTuSkB-tJJh5H0dqLOVHq0KRomqwSWK4ZnP9oIAxELj95Ns4z6eg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the features proposed by Greene et al. (2012). Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond or proportions. The exceptions are mean saccade amplitude, which is in pixels, and area fixated (in percentage)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f02"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f02" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793237&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f02">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean values for the features proposed by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>.
 Each feature is plotted in a separate panel for the three different 
tasks: Naming (N), Description (D), and Search (S). The error bars 
indicate the standard error. The unit for all eye-movement measures is 
the millisecond or proportions. The exceptions are mean saccade 
amplitude, which is in pixels, and area fixated (in percentage).</div></div></div></div><div content-id="i1534-7362-14-3-11-f02" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 2</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean values for the features proposed by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>.
 Each feature is plotted in a separate panel for the three different 
tasks: Naming (N), Description (D), and Search (S). The error bars 
indicate the standard error. The unit for all eye-movement measures is 
the millisecond or proportions. The exceptions are mean saccade 
amplitude, which is in pixels, and area fixated (in percentage).</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the features proposed by Greene et al. (2012). Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond or proportions. The exceptions are mean saccade amplitude, which is in pixels, and area fixated (in percentage)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f02"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f02" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793237&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f02">Download Slide</a></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para"><a reveal-id="i1534-7362-14-3-11-f02" class="revealLink tablelink">Figure 2</a> plots the mean values for the seven features computed by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a> for our three tasks. <a reveal-id="i1534-7362-14-3-11-t01" class="revealLink tablelink">Table 1</a>
 gives the estimates of coefficients obtained by the linear–mixed 
effects model. The results show that the three tasks produce distinct 
patterns for each feature. In particular, we find that the number of 
fixations is significantly higher for object naming than for description
 and visual search (lowest). In a naming task, many objects are 
evaluated as potential naming targets whereas, in search, only objects 
that are contextually relevant to the search will be inspected. The 
description task is situated somewhere in between these two extremes: 
Only objects that will be mentioned or that are contextually related to 
them are fixated. Importantly, these strategies are also reflected in 
the mean saccade amplitude: Saccades during object naming are short as 
objects that are in close proximity are evaluated compared to saccades 
in visual search, in which large sections of the scene are covered. 
Interestingly, we find that saccades during scene description are 
shorter than during naming. Descriptive sentences often have a simple 
subject-verb-object structure, e.g., <em>the girl is hugging a teddy</em>.
 Visually, this implies fixating the agent, determining the action 
performed, and then fixating the object of the action. This visual 
information tends to be spatially proximal.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-t01"></a>
                <div class="content-section clearfix ">
                    
                    <div class="table-section clearfix"><div class="title"><span class="label">Table 1</span><div data-id="i1534-7362-14-3-11-t01" class="table-graphic"><i class="icon-table">&nbsp;</i><div class="original-slide"><a reveal-id="i1534-7362-14-3-11-t01" class="revealLink tablelink">View Table</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Coefficients of linear–mixed effects models with maximal random structure (intercept and slopes on Participants and Scenes). <em>Notes</em>:
 Each feature is modeled as a function of Task, which is contrast coded 
with Naming as a reference level for Description and Search.</div></div></div></div></div><div content-id="i1534-7362-14-3-11-t01" class="hide"><div class="table-section"><div class="title"><span class="label">Table 1</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Coefficients of linear–mixed effects models with maximal random structure (intercept and slopes on Participants and Scenes). <em>Notes</em>:
 Each feature is modeled as a function of Task, which is contrast coded 
with Naming as a reference level for Description and Search.</div></div></div></div><div class="tableContainer"><div class="tTable"><table>             <thead> <tr> <td rowspan="2" align="left">Features</td> <td colspan="3" align="">Intercept</td> <td colspan="3" align="">Description versus naming</td> <td colspan="3" align="">Search versus naming</td> </tr> <tr> <td align=""><em>β</em></td> <td align=""><em>SE</em></td> <td align=""><em>p</em></td> <td align=""><em>β</em></td> <td align=""><em>SE</em></td> <td align=""><em>p</em></td> <td align=""><em>β</em></td> <td align=""><em>SE</em></td> <td align=""><em>p</em></td> </tr> </thead> <tbody> <tr> <td align="">Number of fixations</td> <td align="">26.85</td> <td align="">0.99</td> <td align="">0.0001</td> <td align="">−1.89</td> <td align="">2.90</td> <td align="">0.1</td> <td align="">−26</td> <td align="">2.38</td> <td align="">0.0001</td> </tr> <tr> <td align="">Area fixated</td> <td align="">3.52</td> <td align="">0.11</td> <td align="">0.0001</td> <td align="">−0.75</td> <td align="">0.30</td> <td align="">0.0001</td> <td align="">−2.62</td> <td align="">0.28</td> <td align="">0.0001</td> </tr> <tr> <td align="">Mean saccade amplitude</td> <td align="">175.04</td> <td align="">2.58</td> <td align="">0.0001</td> <td align="">−21.52</td> <td align="">6.64</td> <td align="">0.0001</td> <td align="">80.12</td> <td align="">7.23</td> <td align="">0.0001</td> </tr> <tr> <td align="">Mean gaze duration</td> <td align="">250.65</td> <td align="">4.06</td> <td align="">0.0001</td> <td align="">−17.51</td> <td align="">10.9</td> <td align="">0.003</td> <td align="">−57.81</td> <td align="">10.65</td> <td align="">0.0001</td> </tr> <tr> <td align="">Dwell body</td> <td align="">0.19</td> <td align="">0.004</td> <td align="">0.0001</td> <td align="">0.07</td> <td align="">0.01</td> <td align="">0.0001</td> <td align="">−0.02</td> <td align="">0.01</td> <td align="">0.02</td> </tr> <tr> <td align="">Dwell face</td> <td align="">0.11</td> <td align="">0.003</td> <td align="">0.0001</td> <td align="">0.02</td> <td align="">0.01</td> <td align="">0.04</td> <td align="">0.02</td> <td align="">0.01</td> <td align="">0.01</td> </tr> <tr> <td align="">Dwell object</td> <td align="">0.69</td> <td align="">0.005</td> <td align="">0.0001</td> <td align="">−0.09</td> <td align="">0.01</td> <td align="">0.0001</td> <td align="">0</td> <td align="">0.0</td> <td align="">0.9</td> </tr> <tr> <td align="">Initiation time</td> <td align="">312.01</td> <td align="">8.05</td> <td align="">0.0001</td> <td align="">97.39</td> <td align="">21.33</td> <td align="">0.0001</td> <td align="">63.79</td> <td align="">21.51</td> <td align="">0.0001</td> </tr> <tr> <td align="">Saliency</td> <td align="">247.41</td> <td align="">0.75</td> <td align="">0.0001</td> <td align="">5.70</td> <td align="">2.16</td> <td align="">0.005</td> <td align="">1.18</td> <td align="">2.15</td> <td align="">0.5</td> </tr> <tr> <td align="">Entropy</td> <td align="">11.47</td> <td align="">0.02</td> <td align="">0.0001</td> <td align="">−0.19</td> <td align="">0.06</td> <td align="">0.0001</td> <td align="">−0.38</td> <td align="">0.06</td> <td align="">0.0001</td> </tr> </tbody> </table></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We also find that mean gaze 
duration is longer in naming and description compared to search. A 
communicative task requires the processing of both linguistic and visual
 information, so gazes are longer when the information from both 
modalities needs to be evaluated for each object. Furthermore, we also 
observe a significant difference in the mean gaze duration between 
naming and description: Fixations are longer in naming than in 
description. This is presumably due to the fact that a naming task 
demands a more focused retrieval of lexical material associated with the
 visual object to be mentioned than in a description task in which 
dependencies among other objects (semantic relationships, syntactic 
correspondences) need to be established.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">When looking at the dwell time in 
the different regions, we find that the description task is the one in 
which most attention is allocated to humans (i.e., faces and especially 
bodies). As mentioned above, a description often entails that an action 
is verbalized; hence attention is allocated to the animate agent and to 
the action he or she is performing. This result corroborates the pattern
 we observed in mean saccade amplitude. In the search task, however, the
 recognition of an animate agent involves viewing his or her face, but 
not much attention needs to be spent on inspecting the body (see <a reveal-id="i1534-7362-14-3-11-t01" class="revealLink tablelink">Table 1</a>).
 This contrasts with naming, the task in which most of the attention is 
allocated to objects in the background that can be recognized and named:
 An animate agent is easy to name and hence does not require much 
attention. This difference is, however, significant only with respect to
 a description task. Search and naming do not significantly differ in 
their dwell time on objects.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We will discuss the remaining 
three features more thoroughly (initiation time, saliency, and entropy) 
as they are not region-specific.<a href="#n5"><sup>5</sup></a> These three features can be visualized in <a reveal-id="i1534-7362-14-3-11-f03" class="revealLink tablelink">Figure 3</a>, in which we plot the mean and standard errors for the set of additional features considered in this study (OF).&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-f03"></a>
                <div class="content-section clearfix ">
                    
                    <div data-id="i1534-7362-14-3-11-f03" class="figure-section"><div class="title"><span class="label">Figure 3</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f03" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=pNXpRkx-szrmaTTtWuX4f1bKMVvAum~h-oUs7gANDEttrLz1R8BkyeBZ~WZPbehM0IDGqy6hmfPrkgvexNQl3phlZOihLA6bp0JPZ34ZlJYVFJk-bdUAaSmj9dZdOCha5rdjybVfiyNoy4y7yskKjICRrYXASSwe7Uv9Von6mTL9MSciQkmCHdkaD5ioLNDSWJBU8uqeJW~8y12tFaxDWlslG~kvdfeQ5dpoorund1IGe09PWAjDXRJ2D1Biehen-eVO6Bs0aBRn5pcHPO0OkV3i2cG9plnFMCmp4~ONJdBWzkHxosnSHyDASYYHCDLvDX-Tbz8JNMclGxj5qu9omg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the additional features that were computed. Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond with the exceptions of saliency, the mean value of saliency of the image at the fixation location as returned by the model of Torralba et al. (2006), and entropy." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f03"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f03" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793243&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f03">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean
 values for the additional features that were computed. Each feature is 
plotted in a separate panel for the three different tasks: Naming (N), 
Description (D), and Search (S). The error bars indicate the standard 
error. The unit for all eye-movement measures is the millisecond with 
the exceptions of saliency, the mean value of saliency of the image at 
the fixation location as returned by the model of Torralba et al. <a reveal-id="i1534-7362-14-3-11-torralba1" class="revealLink refLink">(2006)</a>, and entropy.</div></div></div></div><div content-id="i1534-7362-14-3-11-f03" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 3</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean
 values for the additional features that were computed. Each feature is 
plotted in a separate panel for the three different tasks: Naming (N), 
Description (D), and Search (S). The error bars indicate the standard 
error. The unit for all eye-movement measures is the millisecond with 
the exceptions of saliency, the mean value of saliency of the image at 
the fixation location as returned by the model of Torralba et al. <a reveal-id="i1534-7362-14-3-11-torralba1" class="revealLink refLink">(2006)</a>, and entropy.</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the additional features that were computed. Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond with the exceptions of saliency, the mean value of saliency of the image at the fixation location as returned by the model of Torralba et al. (2006), and entropy." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f03"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f03" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793243&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f03">Download Slide</a></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We find that initiation time is 
longer in description and search, compared to naming. This is an 
interesting result as it shows that the processing of the cue, taking 
place in description and search but not in naming, increases the time 
required to launch the first eye movement. Presumably, the cue needs to 
be integrated with the scene gist to inform the first eye movement. When
 looking at saliency, we find that naming is the task that relies least 
on fixations in high-saliency areas, especially compared to the 
description task. As the task is to name as many objects as possible, 
visual attention is presumably allocated to objects that are easy to 
recognize rather than to objects that are salient in the scene. Finally,
 when looking at entropy of the attentional landscapes, we observe a 
pattern similar to both area of scene fixated and number of fixations 
(refer to <a reveal-id="i1534-7362-14-3-11-f02" class="revealLink tablelink">Figure 2</a>).
 Naming results in larger entropy than search and description as the 
scene needs to be explored more widely in order to identify as many 
objects as possible.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Overall, we find that each task is
 defined by a characteristic pattern of eye-movement responses. A visual
 search task is characterized by long exploratory saccades, relatively 
short fixations to verify the object fixated against the cue, and a 
focused distribution of fixation on areas contextually relevant to it. 
In contrast, an object-naming task is characterized by shorter saccades 
but longer fixations as the linguistic information associated with the 
object fixated is also evaluated and retrieved if the object is 
mentioned. A naming task also triggers a spread out distribution of 
attention over the scene as different objects could be candidates for 
naming.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Even if both object naming and 
scene description are communicative tasks, they generate distinct 
eye-movement patterns. During scene description, saccades are shorter 
than in naming, and fixations are longer and more focused on animate 
objects, their bodies, and the objects with which they interact. A 
sentence requires not only that the linguistic labels of the visual 
objects are retrieved, but also that dependencies between objects are 
evaluated, selected, and structured into a message. The deeper 
involvement of language processing mechanisms in scene description could
 imply a poorer classification performance compared to visual search and
 object naming. By using only features of visual attention to train the 
models, we fail to capture features that relate to ongoing linguistic 
processing.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In summary, the large and 
statistically significant differences observed across task on the 
associated eye-movement features strongly suggest that it should be 
possible to classify tasks accurately based on these features.&nbsp;</div>
                </div>
                <a id="s3b"></a>
                </div>
            <a id="87793248"></a>
                    <div class="h7" data-magellan-destination="87793248">Predicting the task using eye-movement features</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In <a reveal-id="i1534-7362-14-3-11-t02" class="revealLink tablelink">Table 2</a>, we report the performance in terms of <em>F</em>-score
 for the three classifiers using different sets of features trained on 
two distinct data sets, which are separated by the clutter of the scene.
 We obtain classification performance above chance (i.e., 0.33 given 
that there are three tasks) using any of the classifiers in both high- 
and low-clutter scenes. We achieve the highest accuracy of 0.88 on the 
visual search task with an SVM classifier in low-clutter scenes and the 
lowest accuracy of 0.61 on scene description with the LASSO classifier 
in high-clutter scenes. In terms of the impact of the features set,<a href="#n6"><sup>6</sup></a> we find that there is no significant improvement using GF (<em>F</em> = 0.76 averaged over all three tasks) compared to OF (average <em>F</em> = 0.78), <em>t</em>(177) = −1.43, <em>p</em> = 0.1. However, using all features (average <em>F</em> = 0.81) gives a significant improvement over both GF, <em>t</em>(177) = −4.18, <em>p</em> = 0.0001, and over OF, <em>t</em>(177) = −2.79, <em>p</em> = 0.005.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-t02"></a>
                <div class="content-section clearfix ">
                    
                    <div class="table-section clearfix"><div class="title"><span class="label">Table 2</span><div data-id="i1534-7362-14-3-11-t02" class="table-graphic"><i class="icon-table">&nbsp;</i><div class="original-slide"><a reveal-id="i1534-7362-14-3-11-t02" class="revealLink tablelink">View Table</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance for each task (object naming, scene 
description, and visual search) computed over 10 folds of the eye 
movement using different sets of eye-movement features: <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">Greene et al.'s (2012)</a> features (GF), other features (OF), and all features (All). <em>Notes</em>: <em>F</em>-scores
 are reported for three different classifiers: least-angle regression 
(LASSO), multinomial logistic regression (MM), and support-vector 
machine (SVM). Boldface indicates the best <em>F</em>-scores achieved for each task and classifier.</div></div></div></div></div><div content-id="i1534-7362-14-3-11-t02" class="hide"><div class="table-section"><div class="title"><span class="label">Table 2</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance for each task (object naming, scene 
description, and visual search) computed over 10 folds of the eye 
movement using different sets of eye-movement features: <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">Greene et al.'s (2012)</a> features (GF), other features (OF), and all features (All). <em>Notes</em>: <em>F</em>-scores
 are reported for three different classifiers: least-angle regression 
(LASSO), multinomial logistic regression (MM), and support-vector 
machine (SVM). Boldface indicates the best <em>F</em>-scores achieved for each task and classifier.</div></div></div></div><div class="tableContainer"><div class="tTable"><table>              <thead> <tr> <td rowspan="2" align="left">Scene clutter</td> <td rowspan="2" align="">Task</td> <td colspan="3" align="">LASSO</td> <td colspan="3" align="">MM</td> <td colspan="3" align="">SVM</td> </tr> <tr> <td align="">GF</td> <td align="">OF</td> <td align="">All</td> <td align="">GF</td> <td align="">OF</td> <td align="">All</td> <td align="">GF</td> <td align="">OF</td> <td align="">All</td> </tr> </thead> <tbody> <tr> <td rowspan="4" align="">High</td> <td align="">Naming</td> <td align="">.77</td> <td align="">.81</td> <td align="">.82</td> <td align="">.8</td> <td align="">.81</td> <td align="">.84</td> <td align="">.81</td> <td align="">.85</td> <td align=""><strong>.86</strong></td> </tr> <tr> <td align="">Description</td> <td align="">.61</td> <td align="">.65</td> <td align="">.66</td> <td align="">.65</td> <td align="">.67</td> <td align="">.71</td> <td align="">.68</td> <td align="">.71</td> <td align=""><strong>.74</strong></td> </tr> <tr> <td align="">Search</td> <td align="">.8</td> <td align="">.79</td> <td align="">.82</td> <td align="">.81</td> <td align="">.81</td> <td align=""><strong>.83</strong></td> <td align="">.82</td> <td align="">.8</td> <td align=""><strong>.83</strong></td> </tr> <tr> <td align="">Naming</td> <td align="">.75</td> <td align="">.8</td> <td align="">.8</td> <td align="">.77</td> <td align="">.82</td> <td align="">.83</td> <td align="">.79</td> <td align=""><strong>.86</strong></td> <td align=""><strong>.86</strong></td> </tr> <tr> <td rowspan="2" align="">Low</td> <td align="">Description</td> <td align="">.64</td> <td align="">.66</td> <td align="">.67</td> <td align="">.66</td> <td align="">.69</td> <td align="">.74</td> <td align="">.67</td> <td align="">.76</td> <td align=""><strong>.77</strong></td> </tr> <tr> <td align="">Search</td> <td align="">.86</td> <td align="">.85</td> <td align="">.86</td> <td align="">.86</td> <td align="">.86</td> <td align=""><strong>.88</strong></td> <td align="">.87</td> <td align="">.85</td> <td align=""><strong>.88</strong></td> </tr> </tbody> </table></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">When evaluating the performance of
 the different classifiers when all features are used, we find no 
significant difference between LASSO (average <em>F</em> = 0.78) and MM (average <em>F</em> = 0.81), <em>t</em>(57) = −1.42, <em>p</em> = 0.1. However, SVM (average <em>F</em> = 0.84) outperforms LASSO, <em>t</em>(53) = 3.07, <em>p</em> = 0.003, but not MM, <em>t</em>(56) = 1.68, <em>p</em>
 = 0.09. When comparing classification performance across the tasks 
using classifiers trained on all features, we find that scene 
description (average <em>F</em> = 0.73) is classified significantly less accurately than both visual search (average <em>F</em> = 0.86), <em>t</em>(47) = −11.53, <em>p</em> &lt; 0.0001, and naming task (average <em>F</em> = 0.84), <em>t</em>(55) = −9.01, p &lt; 0.0001. The difference between the classification accuracy for search and naming is only marginal, <em>t</em>(52) = −1.75, <em>p</em> &lt; 0.08.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The results observed in our 
classification analysis are consistent with our mixed-model analysis of 
the eye-movement features in the previous section. Each task generates a
 distinctive eye-movement signature, which makes it possible to detect 
the task from eye-movement information. Nevertheless, Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 found the opposite result, viz., they were not able to classify tasks 
based on eye-movement features. The crucial difference between their 
study and our study is presumably the nature of the tasks given to 
participants: Greene et al. used four purely visual tasks (memorize the 
picture, determine the decade the picture was taken in, determine 
people's wealth and social context). We, on the other hand, used one 
purely visual task (visual search) and two communicative tasks (scene 
description and object naming). In communicative tasks, additional 
cognitive modalities are needed to achieve the tasks goals. 
Communicative tasks situated in a visual context demand an inspection 
strategy that is also informed by language processing. Thus, searching 
for targets happens in the service of planning, encoding, and 
articulating linguistic output. The necessary guidance is simpler in 
naming than in description as the target does not need to be 
contextualized; this is reflected in the fact that scene description is 
the most difficult task to classify (see <a reveal-id="i1534-7362-14-3-11-t02" class="revealLink tablelink">Table 2</a>).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Interestingly, when we look at the
 percentages of misclassified trials and their distribution across 
different tasks, we find that visual search is typically misclassified 
as scene description (see <a reveal-id="i1534-7362-14-3-11-t03" class="revealLink tablelink">Table 3</a>).
 It seems plausible that the fact that they are both cued paradigms and 
both focus on a specific target object triggers relatively similar 
eye-movement patterns. Object naming, in contrast, is typically 
misclassified as scene description, which suggests that communicative 
tasks tend to share more features, and hence they are more often 
misclassified as each other.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-t03"></a>
                <div class="content-section clearfix ">
                    
                    <div class="table-section clearfix"><div class="title"><span class="label">Table 3</span><div data-id="i1534-7362-14-3-11-t03" class="table-graphic"><i class="icon-table">&nbsp;</i><div class="original-slide"><a reveal-id="i1534-7362-14-3-11-t03" class="revealLink tablelink">View Table</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Percentage of misclassified trials using an SVM classifier trained on all features. <em>Notes</em>:
 The columns indicate the correct class in the test set; the rows 
indicate the class predicted by the classifier. For instance, 
Search-Correct, Description-Predicted gives the percentage of visual 
search instances misclassified as scene description instances.</div></div></div></div></div><div content-id="i1534-7362-14-3-11-t03" class="hide"><div class="table-section"><div class="title"><span class="label">Table 3</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Percentage of misclassified trials using an SVM classifier trained on all features. <em>Notes</em>:
 The columns indicate the correct class in the test set; the rows 
indicate the class predicted by the classifier. For instance, 
Search-Correct, Description-Predicted gives the percentage of visual 
search instances misclassified as scene description instances.</div></div></div></div><div class="tableContainer"><div class="tTable"><table>       <thead> <tr> <td align="left"></td> <td align="">Search-correct</td> <td align="">Description-correct</td> <td align="">Naming-correct</td> </tr> </thead> <tbody> <tr> <td align="">Search-predicted</td> <td align="">0</td> <td align="">12</td> <td align="">5</td> </tr> <tr> <td align="">Description-predicted</td> <td align="">8</td> <td align="">0</td> <td align="">8</td> </tr> <tr> <td align="">Naming-predicted</td> <td align="">2</td> <td align="">13</td> <td align="">0</td> </tr> </tbody> </table></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In summary, we find differences in
 classification performance by task, by feature set, and by type of 
classifier, but overall, our results convincingly show that tasks can be
 accurately discriminated by using the associated eye-movement 
information.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">For comparability with Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>,
 we also conducted two analyses in which we trained the same three 
classifiers to predict the participants and the images viewed. Using the
 GF feature set with the multinomial log-linear neural networks model 
(MM) classifier, we obtained an above-chance classification performance 
with both participants, viz., 0.12 (when chance is 1/74 = 0.013), and 
images, viz., 0.14 (when chance is 1/24 = 0.04).<a href="#n7"><sup>7</sup></a>
 This result indicates that eye movements carry detailed information 
about both viewers and scenes, but also that classification is 
substantially worse than on tasks, on which we achieve a classification 
accuracy of almost 90% for visual search.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In the next section, we conclude 
the study by presenting three more analyses, which provide answers for 
three questions about classifying tasks given eye-movement information: 
(a) How many features do we need to achieve a classification performance
 above chance? (b) Which features are most predictive of the task being 
performed? (c) Can we accurately classify tasks using features that are 
independent of the fact that our tasks used self-termination?&nbsp;</div>
                </div>
                <a id="s3c"></a>
                </div>
            <a id="87793258"></a>
                    <div class="h7" data-magellan-destination="87793258">Feature analysis</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In <a reveal-id="i1534-7362-14-3-11-f04" class="revealLink tablelink">Figure 4</a>, we plot how <em>F</em>-score
 changes as a function of the features used. We find that, already with 
just one feature, the classifier is able to detect which task is 
performed with an accuracy above chance. Classification performance does
 not monotonically increase with the number of features added. Rather, 
we observe that accuracy peaks at eight features and then reduces 
slightly as more features are included. This is likely to indicate that 
the classifier suffers from data sparseness when it has to use a larger 
feature set.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-f04"></a>
                <div class="content-section clearfix ">
                    
                    <div data-id="i1534-7362-14-3-11-f04" class="figure-section"><div class="title"><span class="label">Figure 4</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f04" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=kUbbcV7CBGb6TeFLFtaSj~7i7Ot0~udskvUDt2N4RIrjRd~8f-6YLSQeYsam~5GeH8PAhhRdd10fb6-t8RCniIpF28lF1g1boWzQOaj1c155J9jzFWEKvmZgtTf-PQYxhBiIECOFtV3GHikbDNdZcvmuC~k2nWM2RSu0ralVRFCm2CUwSdoKtLRWqgc~A8zHDQ8bfDL9yeZD2Mi-Vp9oSoM-RR~mLJJnpRN7Aulw3lVEviIiJU9LbA5rwWqV-y8tUhq5akYU0ycqr27TlggzW2heGS1NFSmGCBXG4hAzwgpmckY33E9fEPeQHwqPh~MA8kwD7uoGxOK5hfY3p33-OA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean F-score classification performance (y-axis) as a function of the features used (x-axis). Features are selected one at a time using a forward stepwise procedure, which includes features based on their impact on classification performance (best first). Error bars show the standard error for the F-score obtained from tenfold cross-validation." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f04"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f04" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793260&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f04">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance (y-axis) as a function of the features used 
(x-axis). Features are selected one at a time using a forward stepwise 
procedure, which includes features based on their impact on 
classification performance (best first). Error bars show the standard 
error for the <em>F</em>-score obtained from tenfold cross-validation.</div></div></div></div><div content-id="i1534-7362-14-3-11-f04" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 4</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance (y-axis) as a function of the features used 
(x-axis). Features are selected one at a time using a forward stepwise 
procedure, which includes features based on their impact on 
classification performance (best first). Error bars show the standard 
error for the <em>F</em>-score obtained from tenfold cross-validation.</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean F-score classification performance (y-axis) as a function of the features used (x-axis). Features are selected one at a time using a forward stepwise procedure, which includes features based on their impact on classification performance (best first). Error bars show the standard error for the F-score obtained from tenfold cross-validation." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f04"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f04" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793260&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f04">Download Slide</a></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">When we perform stepwise 
selection, we evaluate whether the inclusion of a new feature improves 
the classification performance; if it does not, then we retain the model
 without the additional feature (refer to the section Methods for 
analysis and classification for details). Over 100 runs (10 iterations 
for 10 folds), the final feature sets we obtain contained an average of 
7.66 ± 1.93 features, which confirms the trend observed in <a reveal-id="i1534-7362-14-3-11-f04" class="revealLink tablelink">Figure 4</a>. The mean <em>F</em>-score
 performance over the 100 runs is 0.87 ± 0.02. In order to test whether 
more features implies a higher classification performance, we run a 
linear model in which the dependent measure is the <em>F</em>-score obtained and the predictor is the size of the associated feature set. We find that <em>F</em>-scores slightly improve if the feature set is larger (<em>β<sub>size</sub></em> = 0.001), but this improvement is not statistically significant in a one-way ANOVA, <em>F</em>(1, 98) = 2.37, <em>p</em> = 0.1. This clearly indicates that not all features have the same importance for discriminating between tasks.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In <a reveal-id="i1534-7362-14-3-11-f05" class="revealLink tablelink">Figure 5</a>,
 we plot how frequently a feature is selected to be in the final feature
 set. We find that the most discriminative feature is the initiation 
time, i.e., the time to program the first saccade after scene onset. As 
discussed in the section Features Mediating Task Variability above, both
 visual search and scene description are cued. Initiation time indicates
 the time needed to integrate the cue with the gist of the scene and 
plan the first saccade based on this. Thus, initiation is longer for 
search and especially scene description compared to naming (see <a reveal-id="i1534-7362-14-3-11-f03" class="revealLink tablelink">Figure 3</a>).
 Integration time therefore constitutes a major discriminant factor for 
distinguishing between noncued tasks, such as naming, and cued tasks, 
such as description and search.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-f05"></a>
                <div class="content-section clearfix ">
                    
                    <div data-id="i1534-7362-14-3-11-f05" class="figure-section"><div class="title"><span class="label">Figure 5</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f05" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=sgluWAToJlpBo-HGIokO3YlAoHnmFbizW-XfDwkpzYNnhq4GBzLMR1WImXKEUQzuissiThzmTr-VX9ry-nAKLdoMOm6~kiPD4wZX8uwV4vWR2SFgRpHWwwr9rTatdWh1oLkONyqu4hzCsVfHt~Q8KoYPg7ChaSYVxy7Bpk9mmfTQJy4F~ynaA5nXnAmI3C9VWn8xqhpvtjR4Lw6a8lySIABa92bbQt1dt2vl0~opWrg-o28Nx-fhLLMJRvTLMvQ9VzFp3qnqHWVkiwAOXnSmnQOwUYW0y0m9JJBA4OEsjNALLoT3mpDI501JsJ7NEVpDT25HzjI7HVEzs10V6SMkFg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Bar plot showing the frequency of inclusion of each feature during forward stepwise model selection over 10 iterations and 10 folds (100 feature sets in total)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f05"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f05" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793263&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f05">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Bar
 plot showing the frequency of inclusion of each feature during forward 
stepwise model selection over 10 iterations and 10 folds (100 feature 
sets in total).</div></div></div></div><div content-id="i1534-7362-14-3-11-f05" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 5</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Bar
 plot showing the frequency of inclusion of each feature during forward 
stepwise model selection over 10 iterations and 10 folds (100 feature 
sets in total).</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Bar plot showing the frequency of inclusion of each feature during forward stepwise model selection over 10 iterations and 10 folds (100 feature sets in total)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f05"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f05" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793263&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f05">Download Slide</a></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The second and third best features
 are the number of fixations and the entropy of the attentional 
landscape, i.e., features capturing the spatial distribution of visual 
attention. Just behind this, we find mean saccade amplitude, which 
reflects the exploration strategies employed to perform the task. 
Temporal features, such as the total number of fixations on objects, 
object dwell time, and mean gaze duration, also perform well, each being
 selected more than 50% of the time. This indicates that tasks can be 
differentiated in both the temporal and the spatial allocation of visual
 attention. It is interesting to note, moreover, that the mean visual 
saliency of fixations is selected only 17% percent of the time. This 
suggests that bottom-up scene information is accessed in similar ways 
for our three different tasks and hence might not be a key discriminant 
feature in the classification.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The final test regards the 
question of self-termination and the possibility that high 
classification performance could be driven by features related to the 
amount of viewing time, such as the total number of fixations. We train 
an SVM classifier with only two features: initiation time and mean 
saccade amplitude. The first feature reflects the cuing aspect of the 
task, the second one the spatial span covered as the scene is 
explored.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We achieve a classification 
accuracy of 0.79 for object naming, 0.65 for visual search, and 0.63 for
 scene description. This result is visualized in <a reveal-id="i1534-7362-14-3-11-f06" class="revealLink tablelink">Figure 6</a>,
 in which we show a scatter-plot of tasks as a function of mean saccade 
amplitude and initiation time; the full dots are observations, and the 
asterisks are the predicted values from the SVM. We find that the naming
 cluster is more clearly defined compared to visual search and scene 
description, and this explains why we find the highest classification 
accuracy here. Moreover, if we drop one feature and train the classifier
 with either mean saccade amplitude or initiation time, we still have an
 accuracy above chance, i.e., 0.33. With initiation time only, we 
achieve an accuracy of 0.78 for object naming, 0.51 for scene 
description, and 0.45 for visual search.&nbsp;</div>
                </div>
                <a id="i1534-7362-14-3-11-f06"></a>
                <div class="content-section clearfix ">
                    
                    <div data-id="i1534-7362-14-3-11-f06" class="figure-section"><div class="title"><span class="label">Figure 6</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f06" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=KLW7KxBZz1mDrs0NkbBwRcwlZiNqP0~yZU6tuIQ1vXZr4TYqVt34wQ4LWzk1-tuxJH4cbVHcMI9n1wWTOYIHwwvVCWW1c9LpG2Cpps~RqcJxhzsLJi2Di1WNWptaYfBEpOZn9hBp3g-jQUfxmpxWk5pFs9D0jhX8Qu6iINQCAo2zr~Bi5Q6052K6D8ye0jqg6BVUpg8V6PmoOTKNZwOD4VUt-gBKrr7ZrhRrT9wVu2NdUAMbXGG9Hh5FbOyXMtnhgXYWR8TehPTmpGsIv9QV6oF8XSQu7eOQpmUO7ro7-aO2~6hlBrjAnSjIWoaVI9aINwI7DKyKP3ovJU~PRH6HeA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Scatter plot of Tasks as a function of Mean Saccade Amplitude (pixels), and Initiation Time (ms). The observations are represented as full dots, and the predicted values are represented as asterisks. The three different tasks are color coded as Description (green), Naming (red), Search (blue). The colored contours represent the SVM classification distributions for the three tasks." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f06"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f06" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793267&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f06">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Scatter
 plot of Tasks as a function of Mean Saccade Amplitude (pixels), and 
Initiation Time (ms). The observations are represented as full dots, and
 the predicted values are represented as asterisks. The three different 
tasks are color coded as Description (green), Naming (red), Search 
(blue). The colored contours represent the SVM classification 
distributions for the three tasks.</div></div></div></div><div content-id="i1534-7362-14-3-11-f06" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 6</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Scatter
 plot of Tasks as a function of Mean Saccade Amplitude (pixels), and 
Initiation Time (ms). The observations are represented as full dots, and
 the predicted values are represented as asterisks. The three different 
tasks are color coded as Description (green), Naming (red), Search 
(blue). The colored contours represent the SVM classification 
distributions for the three tasks.</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Scatter plot of Tasks as a function of Mean Saccade Amplitude (pixels), and Initiation Time (ms). The observations are represented as full dots, and the predicted values are represented as asterisks. The three different tasks are color coded as Description (green), Naming (red), Search (blue). The colored contours represent the SVM classification distributions for the three tasks." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f06"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f06" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793267&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f06">Download Slide</a></div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Theoretically, this result 
demonstrates that tasks do not differ only in terms of their implicit 
timing as set by self-termination, but also in terms of other features 
germane to the task, such as whether prior information has to be 
integrated (cuing or not), or in terms of strategies used to sample the 
visual percept, such as the distance covered during a saccade. These 
task-driven routines contribute to the optimal completion of the task, 
e.g., long saccades during search, or serve other cognitive processes 
that are concurrently active, e.g., smaller saccades during language 
processing.&nbsp;</div>
                </div>
                <a id="s4"></a>
                </div>
            <a id="87793269"></a>
                    <div class="h6" data-magellan-destination="87793269">General discussion</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Since very early research in 
visual cognition, task has played a pivotal role in formulating causal 
explanations for the different eye-movement responses observed.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">First Buswell <a reveal-id="i1534-7362-14-3-11-buswell1" class="revealLink refLink">(1935)</a> and then, a few decades later, Yarbus <a reveal-id="i1534-7362-14-3-11-yarbus1" class="revealLink refLink">(1967)</a>,
 in his influential chapter “Eye-Movements during Complex Object 
Perception,” have discussed the role of expertise, task instructions, 
and object knowledge in the allocation of visual attention. The 
qualitative analyses of scan-path trajectories during different visual 
tasks presented in these studies have suggested that eye-movement 
information includes evidence of the task performed. Hence, scan paths 
observed in different tasks should differ. If a task entails different 
goals, then the underlying cognitive processes should differ, and the 
scan paths should reflect this difference.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">This idea has been extensively 
explored in subsequent research, in which visual attention was mainly 
investigated in the context of real-world tasks (Ballard &amp; Hayhoe, <a reveal-id="i1534-7362-14-3-11-ballard1" class="revealLink refLink">2009</a>; Ballard et al., <a reveal-id="i1534-7362-14-3-11-ballard2" class="revealLink refLink">1995</a>; Land &amp; Furneaux, <a reveal-id="i1534-7362-14-3-11-land1" class="revealLink refLink">1997</a>; Land &amp; Hayhoe, <a reveal-id="i1534-7362-14-3-11-land2" class="revealLink refLink">2001</a>; Pelz &amp; Canosa, <a reveal-id="i1534-7362-14-3-11-pelz1" class="revealLink refLink">2001</a>).
 This research demonstrates that task indeed drives eye-movement 
responses. In particular, eye movements are proactively employed to 
anticipate useful task information, e.g., looking at the spout and the 
kettle before pouring (Land &amp; Furneaux, <a reveal-id="i1534-7362-14-3-11-land1" class="revealLink refLink">1997</a>).
 Moreover, performing a sequence of tasks, e.g., “wash hands” versus 
“fill a cup” and then “wash hands,” can modulate the pattern of fixation
 observed for the same task (“wash hands” in this case, see Pelz &amp; 
Canosa, <a reveal-id="i1534-7362-14-3-11-pelz1" class="revealLink refLink">2001</a>).
 The memorability of information attended to is time-locked to precise 
phases of the task. Changes that occurred on attended objects, for 
example, are detected only if the object was important for the specific 
action being concurrently performed (Triesch et al., <a reveal-id="i1534-7362-14-3-11-triesch1" class="revealLink refLink">2003</a>).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The effect of tasks extends also 
to other visual tasks, such as search for a target or memorization of a 
scene. The amount of area inspected, for example, was found to be wider 
in memorization than in search (Castelhano et al., <a reveal-id="i1534-7362-14-3-11-castelhano2" class="revealLink refLink">2009</a>).
 Memorization benefits from a wider sampling of the scene compared to 
visual search, which instead focuses on precise segments of the scene, 
especially when animate objects are the search targets (Coco &amp; 
Keller, <a reveal-id="i1534-7362-14-3-11-coco1" class="revealLink refLink">2009</a>; Fletcher-Watson, Findlay, Leekam, &amp; Benson, <a reveal-id="i1534-7362-14-3-11-fletcherwatson1" class="revealLink refLink">2008</a>; Torralba et al., <a reveal-id="i1534-7362-14-3-11-torralba1" class="revealLink refLink">2006</a>).
 Task effects on visual attention are also manifested during other 
cognitive activities, such as reading (aloud vs. silent), and, in turn, 
eye movements in reading significantly differ from those observed in 
scene perception (Rayner, <a reveal-id="i1534-7362-14-3-11-rayner1" class="revealLink refLink">2009</a>).&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Taken together, prior research 
strongly suggests a mapping between eye movements and tasks, and hence 
it should be possible to determine which task was performed by an 
observer given his or her eye-movement pattern.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 tested precisely this hypothesis and found, contrary to expectations, 
that it was not possible to use eye-movement features to classify the 
task performed with an accuracy above chance. This study followed from 
DeAngelus and Pelz <a reveal-id="i1534-7362-14-3-11-deangelus1" class="revealLink refLink">(2009)</a>, in which the original Yarbus <a reveal-id="i1534-7362-14-3-11-yarbus1" class="revealLink refLink">(1967)</a>
 study was successfully replicated, and significant differences across 
tasks were found. In DeAngelus and Pelz, however, an important aspect 
that contributed to the differences observed might have been the 
spontaneous self-termination of the participants. Tasks were completed 
at different times, and this influenced the associated eye-movement 
responses, e.g., total number of fixations. Thus, the fixed viewing time
 in Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a> could have flattened the variability observed in the eye-movement responses, hence making classification impossible.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The alternative hypothesis we 
propose is that the differences in eye-movement patterns across tasks 
were not large enough in Greene et al.'s <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 study to be detected accurately. In particular, as there was no 
involvement of other cognitive processes beside visual attention, the 
different visual tasks might have had a common underlying pattern, hence
 making it difficult to separate them during classification. Task 
differences can be expected to be particularly prominent when visual 
attention is concurrently deployed with motor actions as discussed in 
the <a href="#s1" class="sectionLink">Introduction</a>. Thus, we 
hypothesized that tasks need to be more distinct in their underlying 
goals and cognitive processes for differences in eye-movement responses 
to emerge.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In our study, we tested this 
hypothesis and assumed that relevant differences would be observed when 
comparing purely visual tasks (e.g., visual search) with communicative 
tasks (e.g., object naming and scene description) in which visual and 
linguistic information are processed concurrently (Cooper, <a reveal-id="i1534-7362-14-3-11-cooper1" class="revealLink refLink">1974</a>; Tanenhaus et al., <a reveal-id="i1534-7362-14-3-11-tanenhaus1" class="revealLink refLink">1995</a>).
 Prior work on language processing situated in a visual context has 
convincingly demonstrated that visual responses are time-locked with 
linguistic processes during language comprehension (e.g., Altmann &amp; 
Kamide, <a reveal-id="i1534-7362-14-3-11-altmann1" class="revealLink refLink">1999</a>; Spivey-Knowlton, Tanenhaus, Eberhard, &amp; Sedivy, <a reveal-id="i1534-7362-14-3-11-spiveyknowlton1" class="revealLink refLink">2002</a>) and production (e.g., Gleitman et al., <a reveal-id="i1534-7362-14-3-11-gleitman1" class="revealLink refLink">2007</a>; Griffin &amp; Bock, <a reveal-id="i1534-7362-14-3-11-griffin1" class="revealLink refLink">2000</a>).
 Moreover, visual and linguistic responses are so closely intertwined 
that it is possible to retrieve the correct sentence based on the 
associated scan pattern with an accuracy above chance (Coco &amp; 
Keller, <a reveal-id="i1534-7362-14-3-11-coco2" class="revealLink refLink">2012</a>).
 If eye movements carry detailed information about a sentence being 
heard or spoken, then they should also carry discriminant information 
about the task performed.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">In the present study, we tested 
this claim using eye-movement data collected in three different tasks: 
visual search, object naming, and scene description. From the eye 
movements of each trial, we extracted the seven features used by Greene 
et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 and an additional 15 new features, which we included in the analysis to
 widen the range of eye-movement properties investigated.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">A linear–mixed effect modeling 
analysis showed that tasks significantly differ in a wide range of 
eye-movement responses, including ones that depend on the time to 
complete the task (e.g., total number of fixations) and ones that do not
 (e.g., initiation time). In particular, each task is characterized by a
 distinctive eye-movement pattern: Visual search requires long 
exploratory saccades to quickly sample the visual scene along with short
 fixation durations to verify object identity against a cued target. 
During object naming, saccades are much shorter to quickly attend as 
many objects as possible, and fixations are longer to retrieve and 
activate lexical information associated with the fixated object. Scene 
description also has its characteristic pattern, in which saccades are 
shorter than search but longer than naming. During scene description, 
objects mostly related to the sentence being produced are attended 
whereas, in naming, all objects are possible naming candidates. 
Fixations are longer than during search, which highlights the 
involvement of language processing, but they are shorter than during 
naming. Structuring a sentence requires a deeper involvement of 
language-processing mechanisms, which presumably reduces the amount of 
attentional resources available for processing specific objects.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">The large differences observed 
suggest that an automatic classification of tasks using eye-movement 
features should be possible. Therefore, we trained three different 
classifiers, a least-angle regression classifier, a multinomial logistic
 model, and a support vector machine, with three different sets of 
features, Greene et al.'s <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>
 features, our additional features, and both, and demonstrated that we 
can classify tasks using each of these feature sets with an accuracy 
well above chance (a maximum <em>F</em>-score of 0.88 was achieved). In a
 nutshell, we show that tasks are associated with distinctive 
eye-movement patterns and that these patterns can be successfully used 
to perform task classification.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">This evidence for task-specific 
eye-movement patterns provides broad support for the active vision 
hypothesis, according to which task goals play a fundamental role in the
 allocation of visual attention. However, not all tasks can be easily 
classified by relying on eye-movement information. In our analysis, we 
found that the most difficult task to classify was scene description and
 speculated that, in order to achieve higher accuracy in this task, we 
might need to include linguistic features of the sentences produced in 
the training data. If the eye-movement information is dependent on 
concurrently processed information (such as linguistic information), 
then it would be insufficient to correctly characterize the observed 
task.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We also conducted three additional
 analyses to identify (a) how many features are needed to achieve 
maximal classification performance; (b) which of the 22 features we 
considered were most useful for classification; and (c) whether features
 independent of self-termination, such as initiation time and mean 
saccade amplitude, are sufficient to discriminate tasks above 
chance.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We found that already just one 
feature is able to classify the tasks with an accuracy above chance and 
that maximum performance was achieved with seven to eight features. When
 looking at which features were most important, we found that initiation
 time, i.e., the time to launch the first eye movement; the number of 
fixations; the mean saccade amplitude; the total amount of fixation on 
objects; and the entropy of the attentional landscape were selected as 
the best performing more than 50% of the time. Interestingly, these are 
measures covering both spatial aspects (i.e., the amount of the scene 
inspected) and temporal aspects (i.e., the time spent looking) of visual
 attention. The best performing feature overall was initiation time as 
it allows the classifier to distinguish between cued and noncued tasks. 
We concluded that, when the task is cued, the cue needs to be integrated
 with the visual percept of the scene (such as the gist), delaying the 
programming of the first saccade. This effect is more prominent for 
description than for search; in description, the cued target does not 
just need to be located in the scene, but also verbally contextualized 
within the scene. Finally, we showed that features that are independent 
of self-termination, such as initiation time and mean saccade amplitude,
 are also able to accurately classify the tasks. This result casts 
doubts on the hypothesis that the null result of Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a> was due to their use of fixed viewing times.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Our results open new intriguing 
questions regarding when different tasks show distinct patterns of eye 
movements on different tasks as some tasks clearly do not as per Greene 
et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>.
 Future research should investigate, more specifically, how the 
involvement of nonvisual cognitive processes, language in our study, can
 influence the pattern of eye movements observed and the accuracy of 
task classification.&nbsp;</div>
                </div>
                </div>
            <a id="87793285"></a>
                    <div class="h6" data-magellan-destination="87793285">Acknowledgments</div>
            <div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">We thank Monica Castelhano, the 
editor, and two anonymous reviewers for their insightful feedback on 
earlier versions of this manuscript. European Research Council under 
award number 203427 “Synchronous Linguistic and Visual Processing” to FK
 and Fundação para a Ciência e Tecnologia under award number 
SFRH/BDP/88374/2012 to MIC are gratefully acknowledged.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Commercial relationships: none.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Corresponding author: Moreno I. Coco.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Email: micoco@fp.ul.pt.&nbsp;</div>
                </div>
                <div class="content-section clearfix ">
                    
                    <div class="para">Address: Faculdade de Psicologia, Universidade de Lisboa, Lisboa, Portugal.&nbsp;</div>
                </div>
                </div>
            <a id="87793291"></a>
                    <div class="h6" data-magellan-destination="87793291">References</div>
            <div>
                <div class="content-section clearfix ">
                    
                    <div class="reference-section"><div content-id="i1534-7362-14-3-11-altmann1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-altmann1" class="refContent ">

Altmann G.
Kamide Y.

(1999). 
Incremental interpretation at verbs: Restricting the domain of subsequent reference. 
<em>Cognition</em><em>,</em> 
73<em>,</em> 
247–264.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/S0010-0277(99)00059-1" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/S0010-0277(99)00059-1"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/10585516" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-baayen1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-baayen1" class="refContent ">

Baayen R.
Davidson D.
Bates D.

(2008). 
Mixed-effects modeling with crossed random effects for subjects and items. 
<em>Journal of Memory and Language</em><em>,</em> 
59<em>,</em> 
390–412.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.jml.2007.12.005" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/j.jml.2007.12.005"> </span></div></div></div><div content-id="i1534-7362-14-3-11-ballard1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-ballard1" class="refContent ">

Ballard D.
Hayhoe M.

(2009). 
Modeling the role of task in the control of gaze. 
<em>Visual Cognition</em><em>,</em> 
17<em>,</em> 
1185–1204.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1080/13506280902978477" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1080/13506280902978477"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/20411027" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-ballard2"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-ballard2" class="refContent ">

Ballard D.
Hayhoe M.
Pelz J.

(1995). 
Memory representations in natural tasks. 
<em>Journal of Cognitive Neuroscience</em>, 
7
(1), 
66–80.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1162/jocn.1995.7.1.66" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1162/jocn.1995.7.1.66"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/23961754" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-barr1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-barr1" class="refContent ">

Barr D.
Levy R.
Scheepers C.
Tily H.

(2013). 
Random effects structure for confirmatory hypothesis testing: Keep it maximal. 
<em>Journal of Memory and Language</em><em>,</em> 
68
(3),
255–278.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.jml.2012.11.001" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/j.jml.2012.11.001"> </span></div></div></div><div content-id="i1534-7362-14-3-11-bates1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-bates1" class="refContent ">

Bates D.
Maechler M.
Bolker B.

(2011). 
<em>lme4: Linear mixed-effects models using s4 classes</em>. 
Retrieved from <a href="http://cran.r-project.org/web/packages/lme4/index.html" target="_blank">http://cran.r-project.org/web/packages/lme4/index.html</a>.
</div></div></div><div content-id="i1534-7362-14-3-11-buswell1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-buswell1" class="refContent ">

Buswell G. T.

(1935). 
<em>How people look at pictures: A study of the psychology and perception of art</em>. 
Chicago: 
University of Chicago Press.
</div></div></div><div content-id="i1534-7362-14-3-11-castelhano1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-castelhano1" class="refContent ">

Castelhano M.
Heaven C.

(2010). 
The relative contribution of scene context and target features to visual search in real-world scenes. 
<em>Attention, Perception and Psychophysics</em><em>,</em> 
72<em>,</em> 
1283–1297.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.3758/APP.72.5.1283" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.3758/APP.72.5.1283"> </span></div></div></div><div content-id="i1534-7362-14-3-11-castelhano2"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-castelhano2" class="refContent ">

Castelhano M.
Mack M.
Henderson J.

(2009). 
Viewing task influences eye movement control during active scene perception. 
<em>Journal of Vision</em><em>,</em> 
9
(3):
6, 
1–15, 
<a href="http://www.journalofvision.org/content/9/3/6" target="_blank">http://www.journalofvision.org/content/9/3/6</a>, doi:10.1167/9.3.6. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/19757945" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/9/3/6.long" target="_blank">Article</a>]
<span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/19757945" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-chang1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-chang1" class="refContent ">

Chang C.
Lin C.

(2011). 
LIBSVM: A library for support vector machines. 
<em>ACM Transactions on Intelligent Systems and Technology, 2</em><em>,</em> 
27<em>,</em> 
1–27.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1145/1961189" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1145/1961189"> </span></div></div></div><div content-id="i1534-7362-14-3-11-coco1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-coco1" class="refContent ">

Coco M.
Keller F.

(2009). 
The impact of visual information on referent assignment in sentence production. 
In 

Taatgen N. A.
Van Rijn H.

(Eds.), 
Proceedings of the 31st Annual Conference of the Cognitive Science Society (pp. 274–279). Amsterdam: 
Cognitive Science Society.
</div></div></div><div content-id="i1534-7362-14-3-11-coco2"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-coco2" class="refContent ">

Coco M.
Keller F.

(2012). 
Scan patterns predict sentence production in the cross-modal processing of visual scenes. 
<em>Cognitive Science</em>, 
36
(7), 
1204–1223.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1111/cogs.2012.36.issue-7" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1111/cogs.2012.36.issue-7"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/22486717" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-coco3"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-coco3" class="refContent ">

Coco M.
Malcolm G.
Keller F.

(2013). 
The interplay of bottom-up and top-down mechanisms in visual guidance during object naming. 
<em>Quarterly Journal of Experimental Psychology</em><em>,</em> 
(ahead-of-print), 1-25, <a href="http://jov.arvojournals.org/content.aspx?legacySectionId=10.1080/17470218.2013.844843">10.1080/17470218.2013.844843</a>.
</div></div></div><div content-id="i1534-7362-14-3-11-cooper1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-cooper1" class="refContent ">

Cooper R.

(1974). 
The control of eye fixation by the meaning of spoken language: A new 
methodology for the real-time investigation of speech perception, 
memory, and language processing. 
<em>Cognitive Psychology</em>, 
6
(1), 
84–107.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/0010-0285(74)90005-X" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/0010-0285(74)90005-X"> </span></div></div></div><div content-id="i1534-7362-14-3-11-deangelus1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-deangelus1" class="refContent ">

DeAngelus M.
Pelz J.

(2009). 
Top-down control of eye movements: Yarbus revisited. 
<em>Visual Cognition</em>, 
17
(6–7), 
790–811.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1080/13506280902793843" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1080/13506280902793843"> </span></div></div></div><div content-id="i1534-7362-14-3-11-dziemianko1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-dziemianko1" class="refContent ">

Dziemianko M.
Keller F.
Coco M.

(2009). 
Incremental learning of target locations in visual search. 
In 

Carlson I.
Hoelscher C.
Shipley T. F.

(Eds.), 
Proceedings of the 33rd Annual Conference of the Cognitive Science Society (pp. 1729–1734). 
Boston: 
Cognitive Science Society.
</div></div></div><div content-id="i1534-7362-14-3-11-einhauser1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-einhauser1" class="refContent ">

Einhäuser W.
Spain M.
Perona P.

(2008). 
Objects predict fixations better than early saliency. 
<em>Journal of Vision</em><em>,</em> 
8
(14):
18, 
1–26, 
<a href="http://www.journalofvision.org/content/8/14/18" target="_blank">http://www.journalofvision.org/content/8/14/18</a>, doi:10.1167/8.14.18. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/19146319" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/8/14/18.long" target="_blank">Article</a>]
</div></div></div><div content-id="i1534-7362-14-3-11-fletcherwatson1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-fletcherwatson1" class="refContent ">

Fletcher-Watson S.
Findlay J.
Leekam S.
Benson V.

(2008). 
Rapid detection of person information in a naturalistic scene. 
<em>Perception</em>, 
37
(4), 
571–583.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p5705" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1068/p5705"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/18546664" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-friedman1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-friedman1" class="refContent ">

Friedman J.
Hastie T.
Tibshirani R.

(2010). 
Regularization paths for generalized linear models via coordinate descent. 
<em>Journal of Statistical Software</em>, 
33
(1), 
1–22.
<span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/20808728" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-glaholt1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-glaholt1" class="refContent ">

Glaholt M.
Reingold E.

(2012). 
Direct control of fixation times in scene viewing: Evidence from analysis of the distribution of first fixation duration. 
<em>Visual Cognition</em>, 
20
(6), 
605–626.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1080/13506285.2012.666295" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1080/13506285.2012.666295"> </span></div></div></div><div content-id="i1534-7362-14-3-11-gleitman1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-gleitman1" class="refContent ">

Gleitman L.
January D.
Nappa R.
Trueswell J.

(2007). 
On the give and take between event apprehension and utterance formulation. 
<em>Journal of Memory and Language</em><em>,</em> 
57<em>,</em> 
544–569.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.jml.2007.01.007" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/j.jml.2007.01.007"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/18978929" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-greene1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-greene1" class="refContent ">

Greene M.
Liu T.
Wolfe J.

(2012). 
Reconsidering Yarbus: A failure to predict observers' task from eye movement patterns. 
<em>Vision Research</em><em>,</em> 
62<em>,</em> 
1–8.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.visres.2012.03.019" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/j.visres.2012.03.019"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/22487718" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-griffin1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-griffin1" class="refContent ">

Griffin Z.
Bock K.

(2000). 
What the eyes say about speaking. 
<em>Psychological Science</em><em>,</em> 
11<em>,</em> 
274–279.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1111/1467-9280.00255" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1111/1467-9280.00255"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11273384" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-hagemann1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-hagemann1" class="refContent ">

Hagemann N.
Schorer J.
Cañal-Bruland R.
Lotz S.
Strauss B.

(2010). 
Visual perception in fencing: Do the eye movements of fencers represent their information pickup? 
<em>Attention, Perception, and Psychophysics</em><em>,</em> 
72
(8), 
2204–2214.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.3758/BF03196695" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.3758/BF03196695"> </span></div></div></div><div content-id="i1534-7362-14-3-11-henderson1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-henderson1" class="refContent ">

Henderson J.

(2003). 
Human gaze control during real-world scene perception. 
<em>Trends in Cognitive Sciences</em><em>,</em> 
7<em>,</em> 
498–504.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.tics.2003.09.006" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/j.tics.2003.09.006"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/14585447" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-henderson2"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-henderson2" class="refContent ">

Henderson J.
Shinkareva S.
Wang S.
Luke S. G.
Olejarczyk J.

(2013). 
Predicting cognitive state from eye movements. 
<em>PloS one</em><em>,</em> 
8
(5), 
e64937.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1371/journal.pone.0064937" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1371/journal.pone.0064937"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/23734228" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-inhoff1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-inhoff1" class="refContent ">

Inhoff A.
Rayner K.

(1986). 
Parafoveal word processing during eye fixations in reading: Effects of word frequency. 
<em>Perception &amp; Psychophysics</em>, 
40
(6), 
431–139.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.3758/BF03208203" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.3758/BF03208203"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/3808910" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-land1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-land1" class="refContent ">

Land M.
Furneaux S.

(1997). 
The knowledge base of the oculomotor system. <em>Philosophical Transactions of the Royal Society of London</em>. 
<em>Series B: Biological Sciences</em>, 
352
(1358), 
1231–1239.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1098/rstb.1997.0105" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1098/rstb.1997.0105"> </span></div></div></div><div content-id="i1534-7362-14-3-11-land2"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-land2" class="refContent ">

Land M.
Hayhoe M.

(2001). 
In what ways do eye movements contribute to everyday activities? 
<em>Vision Research</em><em>,</em> 
41<em>,</em> 
3559–3565.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/S0042-6989(01)00102-X" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/S0042-6989(01)00102-X"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11718795" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-land3"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-land3" class="refContent ">

Land M.
McLeod P.

(2000). 
From eye movements to actions: How batsmen hit the ball. 
<em>Nature Neuroscience</em><em>,</em> 
3<em>,</em> 
1340–1345.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1038/81887" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1038/81887"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11100157" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-land4"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-land4" class="refContent ">

Land M.
Mennie N.
Rusted J.

(1999). 
The roles of vision and eye movements in the control of activities of daily living. 
<em>Perception</em><em>,</em> 
28<em>,</em> 
1311–1328.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p2935" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1068/p2935"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/10755142" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-malcolm2"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-malcolm2" class="refContent ">

Malcolm G.
Henderson J.

(2010). 
Combining top-down processes to guide eye movements during real-world scene search. 
<em>Journal of Vision</em><em>,</em> 
10
(2):
4, 
1–11, 
<a href="http://www.journalofvision.org/content/10/2/4" target="_blank">http://www.journalofvision.org/content/10/2/4</a>, doi:10.1167/10.2.4. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/20462305" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/10/2/4.long" target="_blank">Article</a>]
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1167/10.2.4" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1167/10.2.4"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/20462305" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-malcolm1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-malcolm1" class="refContent ">

Malcolm G.
Henderson J.

(2009). 
The effects of target template specificity on visual search in real-world scenes. 
<em>Journal of Vision</em><em>,</em> 
9
(11):
8, 
1–13, 
<a href="http://www.journalofvision.org/content/9/11/8" target="_blank">http://www.journalofvision.org/content/9/11/8</a>, doi:10.1167/9.11.8. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/20053071" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/9/11/8.long" target="_blank">Article</a>]
<span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/20053071" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-mills1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-mills1" class="refContent ">

Mills M.
Hollingworth A.
Van der Stigchel S.
Hoffman L.
Dodd M.

(2011). 
Examining the influence of task set on eye movements and fixations. 
<em>Journal of Vision</em><em>,</em> 
11
(8):
17, 
1–15, 
<a href="http://www.journalofvision.org/content/11/8/17" target="_blank">http://www.journalofvision.org/content/11/8/17</a>, doi:10.1167/11.8.17. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/21799023" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/11/8/17.long" target="_blank">Article</a>]
</div></div></div><div content-id="i1534-7362-14-3-11-noton1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-noton1" class="refContent ">

Noton D.
Stark L.

(1971). 
Scanpaths in saccadic eye movements while viewing and recognizing patterns. 
<em>Vision Research</em><em>,</em> 
11<em>,</em> 
919–942.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/0042-6989(71)90213-6" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/0042-6989(71)90213-6"> </span></div></div></div><div content-id="i1534-7362-14-3-11-pelz1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-pelz1" class="refContent ">

Pelz J.
Canosa R.

(2001). 
Oculomotor behavior and perceptual strategies in complex tasks. 
<em>Vision Research</em><em>,</em> 
41
(25), 
3587–3596.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/S0042-6989(01)00245-0" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/S0042-6989(01)00245-0"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/11718797" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-pomplun1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-pomplun1" class="refContent ">

Pomplun M.
Ritter H.
Velichkvosky B.

(1996). 
Disambiguating complex visual information: Towards communication of personal views of a scene. 
<em>Perception</em><em>,</em> 
25<em>,</em> 
931–948.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1068/p250931" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1068/p250931"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/8938007" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-rayner1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-rayner1" class="refContent ">

Rayner K.

(2009). 
Eye movements and attention in reading, scene perception, and visual search. 
<em>The Quarterly Journal of Experimental Psychology</em><em>,</em> 
62
(8), 
1457–1506.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1080/17470210902816461" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1080/17470210902816461"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/19449261" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-rosenholtz1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-rosenholtz1" class="refContent ">

Rosenholtz R.
Li Y.
Nakano L.

(2007). 
Measuring visual clutter. 
<em>Journal of Vision, 7</em>
(2):
17<em>,</em> 
1–22, <a href="http://www.journalofvision.org/content/7/2/17" target="_blank">http://www.journalofvision.org/content/7/2/17</a>, doi:10.1167/7.2.17. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/18217832" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/7/2/17.long" target="_blank">Article</a>]
</div></div></div><div content-id="i1534-7362-14-3-11-rothkopf1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-rothkopf1" class="refContent ">

Rothkopf C.
Ballard D.
Hayhoe M.

(2007). 
Task and context determine where you look. 
<em>Journal of Vision, 7</em>
(14):
16<em>,</em> 
1–20, <a href="http://www.journalofvision.org/content/7/14/16" target="_blank">http://www.journalofvision.org/content/7/14/16</a>, doi:10.1167/7.14.16. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/18217811" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/7/14/16.long" target="_blank">Article</a>]
</div></div></div><div content-id="i1534-7362-14-3-11-russell1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-russell1" class="refContent ">

Russell B.
Torralba A.
Murphy K.
Freeman W.

(2008). 
Labelme: A database and web-based tool for image annotation. 
<em>International Journal of Computer Vision</em><em>,</em> 
77
(1–3), 
151–173.
</div></div></div><div content-id="i1534-7362-14-3-11-salverda1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-salverda1" class="refContent ">

Salverda A.
Altmann G.

(2011). 
Attentional capture of objects referred to by spoken language. 
<em>Journal of Experimental Psychology: Human Perception and Performance</em>, 
37
(4), 
1122.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1037/a0023101" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1037/a0023101"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/21517215" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-spiveyknowlton1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-spiveyknowlton1" class="refContent ">

Spivey-Knowlton M.
Tanenhaus M.
Eberhard K.
Sedivy J.

(2002). 
Eye movements and spoken language comprehension: Effects of syntactic context on syntactic ambiguity resolution. 
<em>Cognitive Psychology</em><em>,</em> 
(45), 
447–181.
</div></div></div><div content-id="i1534-7362-14-3-11-tanenhaus1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-tanenhaus1" class="refContent ">

Tanenhaus M. K.
Eberhard K.
Sedivy J.

(1995). 
Integration of visual and linguistic information in spoken language comprehension. 
<em>Science</em><em>,</em> 
268, 
632–634.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1126/science.7777863" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1126/science.7777863"> </span></div></div></div><div content-id="i1534-7362-14-3-11-tatler1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-tatler1" class="refContent ">

Tatler B. W.
Baddeley R. J.
Vincent B. T.

(2006). 
The long and the short of it: Spatial statistics at fixation vary with saccade amplitude and task. 
<em>Vision Research</em><em>,</em> 
46
(12), 
1857–1862.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1016/j.visres.2005.12.005" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1016/j.visres.2005.12.005"> </span><span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/16469349" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-torralba1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-torralba1" class="refContent ">

Torralba A.
Oliva A.
Castelhano M.
Henderson J.

(2006). 
Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search. 
<em>Psychological Review</em><em>,</em> 
4
(113), 
766–786.
<span class="crossrefDoi"> <a href="http://dx.doi.org/10.1037/0033-295X.113.4.766" target="_blank">[CrossRef]</a></span><span class="inst-open-url-holders" data-targetid="10.1037/0033-295X.113.4.766"> </span></div></div></div><div content-id="i1534-7362-14-3-11-triesch1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-triesch1" class="refContent ">

Triesch J.
Ballard D.
Hayhoe M.
Sullivan B.

(2003). 
What you see is what you need. 
<em>Journal of Vision</em><em>,</em> 
3
(1):
9, 
86–94, 
<a href="http://www.journalofvision.org/content/3/1/9" target="_blank">http://www.journalofvision.org/content/3/1/9</a>, doi:10.1167/3.1.9. [<a href="http://www.ncbi.nlm.nih.gov/pubmed/12678628" target="_blank">PubMed</a>] [<a href="http://www.journalofvision.org/content/3/1/9.long" target="_blank">Article</a>]
<span class="pubmedLink"> <a href="http://www.ncbi.nlm.nih.gov/pubmed/12678628" class="special sans" target="_blank">[PubMed]</a></span></div></div></div><div content-id="i1534-7362-14-3-11-venables1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-venables1" class="refContent ">

Venables W. N.
Ripley B. D.

(2002). 
<em>Modern applied statistics with s</em> (4th ed.). 
Berlin, 
Springer. 
</div></div></div><div content-id="i1534-7362-14-3-11-yarbus1"><div class="refRow  clearfix"><div role="i1534-7362-14-3-11-yarbus1" class="refContent ">

Yarbus A.

(1967). 
<em>Eye movements and vision</em>. 
New York: 
Plenum.
</div></div></div></div>
                </div>
                <div class="content-section clearfix ">
                    
                    
                </div>
                <a id="n1"></a>
                <div class="content-section clearfix ">
                    
                    <div class="h7 footnote">Footnotes</div><div class="para clearfix"> <span class="tableFootLabel"><sup>1</sup></span> <a id="" class="jumplink-placeholder">&nbsp;</a>We only use the two-targets condition to allow full comparability between the different experimental data sets. </div>
                </div>
                <a id="n2"></a>
                <div class="content-section clearfix ">
                    
                    <div class="h7 footnote" style="display: none;">Footnotes</div><div class="para clearfix"> <span class="tableFootLabel"><sup>2</sup></span> <a id="" class="jumplink-placeholder">&nbsp;</a>The images are all original, created with PhotoshopC2 using components that are in public domain sources (e.g., Flickr). </div>
                </div>
                <a id="n3"></a>
                <div class="content-section clearfix ">
                    
                    <div class="h7 footnote" style="display: none;">Footnotes</div><div class="para clearfix"> <span class="tableFootLabel"><sup>3</sup></span> <a id="" class="jumplink-placeholder">&nbsp;</a>In Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>, objects are defined as any “discrete artifact” not making up the boundary of the scene. </div>
                </div>
                <a id="n4"></a>
                <div class="content-section clearfix ">
                    
                    <div class="h7 footnote" style="display: none;">Footnotes</div><div class="para clearfix"> <span class="tableFootLabel"><sup>4</sup></span> <a id="" class="jumplink-placeholder">&nbsp;</a>We used only the saliency part of the model, not the context part. </div>
                </div>
                <a id="n5"></a>
                <div class="content-section clearfix ">
                    
                    <div class="h7 footnote" style="display: none;">Footnotes</div><div class="para clearfix"> <span class="tableFootLabel"><sup>5</sup></span> <a id="" class="jumplink-placeholder">&nbsp;</a>We
 do not present analyses for all features as most of them would not 
contribute substantially to a theoretical understanding of differences 
between tasks. </div>
                </div>
                <a id="n6"></a>
                <div class="content-section clearfix ">
                    
                    <div class="h7 footnote" style="display: none;">Footnotes</div><div class="para clearfix"> <span class="tableFootLabel"><sup>6</sup></span> <a id="" class="jumplink-placeholder">&nbsp;</a>As
 the classification results are very similar for the high- and 
low-clutter conditions, we measured the impact of the features on the 
models trained using a single data set. </div>
                </div>
                <a id="n7"></a>
                <div class="content-section clearfix ">
                    
                    <div class="h7 footnote" style="display: none;">Footnotes</div><div class="para clearfix"> <span class="tableFootLabel"><sup>7</sup></span> <a id="" class="jumplink-placeholder">&nbsp;</a>Choosing a LASSO classifier results in image classification performance close to chance with an accuracy of 0.05. </div>
                </div>
                </div> 


        <span id="UserHasAccess" data-userhasaccess="True"></span>

    </div>
</div>
<script type="text/javascript">
    // For un-authenticated users, replace reveal-modal links with spans, since target of link is 99.99% likely to be missing
    $(document).ready(function () {
        var hasAccess = $('div[data-widgetname="ArticleFulltext"] span#UserHasAccess').attr('data-userHasAccess');
        if (hasAccess && hasAccess.length && hasAccess.toLowerCase() == 'false') {
            var revealModalLinks = $('div[data-widgetname="ArticleFulltext"] a[data-reveal-id]');
            revealModalLinks.each(function () {
                $(this).replaceWith(function () {
                    var span = new HTMLSpanElement();
                    span.innerHTML = this.innerHTML;
                    return span;
                });
            });
        }
        // now call institutionopenurl
        if (SCM && SCM.InstitutionOpenUrl) {
            SCM.InstitutionOpenUrl.init();
        } else if (console.error) {
            console.error("InstitutionOpenUrl is undefined");
        }

        //Readspeaker - add rsbtn_preserve class for buttons we want to keep
        $('[id^=readSpeaker_] div.para').each(function () {
            $(this).parent().prev('div.content-section.clearfix').parent().prev('div').addClass('rsbtn_preserve');
        });
        //Readspeaker - remove Readspeaker buttons for nested headers we don't want to keep
        $('div.rsbtn').each(function () {
            if (!$(this).hasClass('rsbtn_preserve')) {
                $(this).remove();
            }
            else {
                $(this).removeClass('hide');
            }
        });
    });
</script> 
    </div>

                </div>
                <div id="FigureTab" class="content">
                        <div class="widget-ArticleFiguresAndTables widget-instance-ARVO_Article_Figures_Tab">
        
    <section class="figure-table-wrapper">
        <div data-id="i1534-7362-14-3-11-f01" class="figure-section"><div class="title"><span class="label">Figure 1</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f01" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=pPIGkOxWuk4LDYr-JQ2Jjd~8FBP4cQpht4A7momyvmfhbDrtfmxnh4KgTsMdkMetbmWrxWuXWnL~Hwxagzwzz1v4JZil~OT0ZGCnb3IdLqdPLH2ZtbHGp9OlQcfFYTK~WIizsED03qIccBrv42eo4RRh7YpmPoiFr-vLAdNK~~g0rJ-kNZLlzsWUrEnJdRnW-kZkyn1sqXsPJiMJLGZPqh63c8di1JHjuBxk4WOYIvkE7-1Rb8WRQ7wMVRrgqKONmdSjonXkIW31sTpZQZPtLetmGMvQm7F4IowGBJs1RqYml0SXXDmRMs7cMtI1uOl54OJ4tz8x62GgVdp46WxmKQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="An example of a scene used in the different tasks: low-clutter condition on the left, high-clutter condition on the right. The cued target objects were GIRL and TEDDY. The face is blanked out to protect the identity of the photographed character. The image is an original created with PhotoshopC2 using components that are in the public domain sources (e.g., Flickr)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f01"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f01" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793212&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f01">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">An
 example of a scene used in the different tasks: low-clutter condition 
on the left, high-clutter condition on the right. The cued target 
objects were GIRL and TEDDY. The face is blanked out to protect the 
identity of the photographed character. The image is an original created
 with PhotoshopC2 using components that are in the public domain sources
 (e.g., Flickr).</div></div></div></div><div content-id="i1534-7362-14-3-11-f01" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 1</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">An
 example of a scene used in the different tasks: low-clutter condition 
on the left, high-clutter condition on the right. The cued target 
objects were GIRL and TEDDY. The face is blanked out to protect the 
identity of the photographed character. The image is an original created
 with PhotoshopC2 using components that are in the public domain sources
 (e.g., Flickr).</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="An example of a scene used in the different tasks: low-clutter condition on the left, high-clutter condition on the right. The cued target objects were GIRL and TEDDY. The face is blanked out to protect the identity of the photographed character. The image is an original created with PhotoshopC2 using components that are in the public domain sources (e.g., Flickr)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f01"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f01" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f01.jpeg?Expires=1581154411&amp;Signature=cB1GRWqnst381S0tnwIjzV-ae22m3MSIMFmsVrHkWtYVKB4NgHTWDVmY8iX0ClO5ms124BHJCMcsCFiRVqvE6SpS~2DYtfTOMfNF7niq0UpmFhGMu8WOOOp7IPACLzg79n5~Abvhu0z61CK4c19C8CkR4~fCSTOt1HoQZXF87Nmg9sVwdWxYHtNHTIMgBPHYk9oBEkDYfxbT~OMxWty3vflT4ta4gLeqwx3gkym~2Csb7au61ufFTOEPylJRA6mgQjGpy655HilRZSp80AX7l79b1vgeK4kElohc1L6lXF3asmuXmJK0tBlPtR3e-FkejIXQ3d6isWzHspj0Yen7NQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793212&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f01">Download Slide</a></div></div></div></div>
    </section>
    <section class="figure-table-wrapper">
        <div data-id="i1534-7362-14-3-11-f02" class="figure-section"><div class="title"><span class="label">Figure 2</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f02" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=q82nxPp8byaD~Hnm8IJevSwSm0hKRMGR9J-KbIwy9yL5IclfvYfOvUS0~DYO4LOA4p0c6vmOwSDVAmPjsF2BRYPwKWvYnzcOvuN67MqWDr51Aei~B0TTOIBtzTozauiei2NEGB2qtLbHRlp-ciJFf~xAuMF3BqCyKmPGnZDR8zp1qNqJCROJcOwuM3km9c2JclaoyZN76ON3FmkMv8tO-RLKv6cpflczT4BdEgoXlwEOly-J1kXvLoxhQbHnJfcl3V2drL6~t8ljB8iQTh~4msnztAuoZyw-9bHTuSkB-tJJh5H0dqLOVHq0KRomqwSWK4ZnP9oIAxELj95Ns4z6eg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the features proposed by Greene et al. (2012). Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond or proportions. The exceptions are mean saccade amplitude, which is in pixels, and area fixated (in percentage)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f02"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f02" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793237&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f02">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean values for the features proposed by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>.
 Each feature is plotted in a separate panel for the three different 
tasks: Naming (N), Description (D), and Search (S). The error bars 
indicate the standard error. The unit for all eye-movement measures is 
the millisecond or proportions. The exceptions are mean saccade 
amplitude, which is in pixels, and area fixated (in percentage).</div></div></div></div><div content-id="i1534-7362-14-3-11-f02" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 2</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean values for the features proposed by Greene et al. <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">(2012)</a>.
 Each feature is plotted in a separate panel for the three different 
tasks: Naming (N), Description (D), and Search (S). The error bars 
indicate the standard error. The unit for all eye-movement measures is 
the millisecond or proportions. The exceptions are mean saccade 
amplitude, which is in pixels, and area fixated (in percentage).</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the features proposed by Greene et al. (2012). Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond or proportions. The exceptions are mean saccade amplitude, which is in pixels, and area fixated (in percentage)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f02"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f02" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f02.jpeg?Expires=1581154411&amp;Signature=sgRxMdGM5-noPI-W1rlQO7J0RvhpPcqbqnupdKeKB9G5EKrqyVhBy8d0YhnOei0qIlNv436V~fbYS3DPcw6qnWQOoi66i2HAQnSJknU2qIhxolXFhoIDiMKWfDqQO9k05JN5tA2pvfMryRMx-lizMfb~jZFiGUBHhaTkaM0OfnzJqt2DhhPB~hk-b9DweVbKu-wtkH9ZQLtXOdi5kcAr2J~hxEL7un9NzxVh3F21Pc1O~Q-4kGTbe0TqNxQl03pFva5Z1CHNrGuDo4NEgfQLU1WJdVpF0vrjreB~EM0FWlf0jesNpgIpYhkXyfSnk38xAg9F2X~4E76eKSvdbF~gZg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793237&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f02">Download Slide</a></div></div></div></div>
    </section>
    <section class="figure-table-wrapper">
        <div data-id="i1534-7362-14-3-11-f03" class="figure-section"><div class="title"><span class="label">Figure 3</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f03" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=pNXpRkx-szrmaTTtWuX4f1bKMVvAum~h-oUs7gANDEttrLz1R8BkyeBZ~WZPbehM0IDGqy6hmfPrkgvexNQl3phlZOihLA6bp0JPZ34ZlJYVFJk-bdUAaSmj9dZdOCha5rdjybVfiyNoy4y7yskKjICRrYXASSwe7Uv9Von6mTL9MSciQkmCHdkaD5ioLNDSWJBU8uqeJW~8y12tFaxDWlslG~kvdfeQ5dpoorund1IGe09PWAjDXRJ2D1Biehen-eVO6Bs0aBRn5pcHPO0OkV3i2cG9plnFMCmp4~ONJdBWzkHxosnSHyDASYYHCDLvDX-Tbz8JNMclGxj5qu9omg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the additional features that were computed. Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond with the exceptions of saliency, the mean value of saliency of the image at the fixation location as returned by the model of Torralba et al. (2006), and entropy." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f03"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f03" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793243&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f03">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean
 values for the additional features that were computed. Each feature is 
plotted in a separate panel for the three different tasks: Naming (N), 
Description (D), and Search (S). The error bars indicate the standard 
error. The unit for all eye-movement measures is the millisecond with 
the exceptions of saliency, the mean value of saliency of the image at 
the fixation location as returned by the model of Torralba et al. <a reveal-id="i1534-7362-14-3-11-torralba1" class="revealLink refLink">(2006)</a>, and entropy.</div></div></div></div><div content-id="i1534-7362-14-3-11-f03" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 3</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean
 values for the additional features that were computed. Each feature is 
plotted in a separate panel for the three different tasks: Naming (N), 
Description (D), and Search (S). The error bars indicate the standard 
error. The unit for all eye-movement measures is the millisecond with 
the exceptions of saliency, the mean value of saliency of the image at 
the fixation location as returned by the model of Torralba et al. <a reveal-id="i1534-7362-14-3-11-torralba1" class="revealLink refLink">(2006)</a>, and entropy.</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean values for the additional features that were computed. Each feature is plotted in a separate panel for the three different tasks: Naming (N), Description (D), and Search (S). The error bars indicate the standard error. The unit for all eye-movement measures is the millisecond with the exceptions of saliency, the mean value of saliency of the image at the fixation location as returned by the model of Torralba et al. (2006), and entropy." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f03"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f03" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f03.jpeg?Expires=1581154411&amp;Signature=scMoKH~F~CYIEldCkkfmru63ioUNIhgnSnsevKy6TQV9HKOU6-wRMU~snrQn6uPqI~FZJhE5UuN2QSvemOeUCYxxRHfKvvF9-iFWdFwPnxS54nExavM-jnrC~c7ZuIo-ls4Ee34MQFYrGSmzQsgUqmMKxjg0PPpF4kYujYCC4G~tNlpPQ4Jw~xYNTb6NjOJtFOhROkLEW0HItT1QfXebwKLzrYY~FDCPGGONVK0CFdveUJoYxP2h2v4KsF7RvP4SqyzQnbGuohDmk9yekv1o~YzP4ImGe8PQzjj2ESMylqYW1D1kNVRTO~GWFS30AcRQP7k3Pq6a9y3xtL3me0yHDg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793243&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f03">Download Slide</a></div></div></div></div>
    </section>
    <section class="figure-table-wrapper">
        <div data-id="i1534-7362-14-3-11-f04" class="figure-section"><div class="title"><span class="label">Figure 4</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f04" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=kUbbcV7CBGb6TeFLFtaSj~7i7Ot0~udskvUDt2N4RIrjRd~8f-6YLSQeYsam~5GeH8PAhhRdd10fb6-t8RCniIpF28lF1g1boWzQOaj1c155J9jzFWEKvmZgtTf-PQYxhBiIECOFtV3GHikbDNdZcvmuC~k2nWM2RSu0ralVRFCm2CUwSdoKtLRWqgc~A8zHDQ8bfDL9yeZD2Mi-Vp9oSoM-RR~mLJJnpRN7Aulw3lVEviIiJU9LbA5rwWqV-y8tUhq5akYU0ycqr27TlggzW2heGS1NFSmGCBXG4hAzwgpmckY33E9fEPeQHwqPh~MA8kwD7uoGxOK5hfY3p33-OA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean F-score classification performance (y-axis) as a function of the features used (x-axis). Features are selected one at a time using a forward stepwise procedure, which includes features based on their impact on classification performance (best first). Error bars show the standard error for the F-score obtained from tenfold cross-validation." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f04"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f04" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793260&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f04">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance (y-axis) as a function of the features used 
(x-axis). Features are selected one at a time using a forward stepwise 
procedure, which includes features based on their impact on 
classification performance (best first). Error bars show the standard 
error for the <em>F</em>-score obtained from tenfold cross-validation.</div></div></div></div><div content-id="i1534-7362-14-3-11-f04" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 4</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance (y-axis) as a function of the features used 
(x-axis). Features are selected one at a time using a forward stepwise 
procedure, which includes features based on their impact on 
classification performance (best first). Error bars show the standard 
error for the <em>F</em>-score obtained from tenfold cross-validation.</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Mean F-score classification performance (y-axis) as a function of the features used (x-axis). Features are selected one at a time using a forward stepwise procedure, which includes features based on their impact on classification performance (best first). Error bars show the standard error for the F-score obtained from tenfold cross-validation." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f04"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f04" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f04.jpeg?Expires=1581154411&amp;Signature=zELtgIEmgwtsCwDWOa6mV3uBQt9tesKS1hR5B9mNx67Zwpx1bYyGGnKfpq3RGzHhkuY3DvjRQM5h22xHyMVIcS89gp1D-0AtJmXmVp23tIvGLjX0pJIp1iBP3Rtq~eZSaopU9OxJBPV2-UGuKbKOw5BB1N6v20q~bfUIjbQMsMm9~rR8BuQndhcHtOxamZb0-tZ3ekEaXxIEyv7ddwWYKh62BkmxN5Bgzbh6HIkEtV8Zw4pXOmQXtWYMCTjkwOyOLXiSFu5ku5V0esZRe97iaoPVAQmQyQS5aiIjFRLierFWPa~YnyodJt0WrLuwZ3OnjrG2Pm1oh2rMm5cEvnSWVw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793260&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f04">Download Slide</a></div></div></div></div>
    </section>
    <section class="figure-table-wrapper">
        <div data-id="i1534-7362-14-3-11-f05" class="figure-section"><div class="title"><span class="label">Figure 5</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f05" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=sgluWAToJlpBo-HGIokO3YlAoHnmFbizW-XfDwkpzYNnhq4GBzLMR1WImXKEUQzuissiThzmTr-VX9ry-nAKLdoMOm6~kiPD4wZX8uwV4vWR2SFgRpHWwwr9rTatdWh1oLkONyqu4hzCsVfHt~Q8KoYPg7ChaSYVxy7Bpk9mmfTQJy4F~ynaA5nXnAmI3C9VWn8xqhpvtjR4Lw6a8lySIABa92bbQt1dt2vl0~opWrg-o28Nx-fhLLMJRvTLMvQ9VzFp3qnqHWVkiwAOXnSmnQOwUYW0y0m9JJBA4OEsjNALLoT3mpDI501JsJ7NEVpDT25HzjI7HVEzs10V6SMkFg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Bar plot showing the frequency of inclusion of each feature during forward stepwise model selection over 10 iterations and 10 folds (100 feature sets in total)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f05"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f05" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793263&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f05">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Bar
 plot showing the frequency of inclusion of each feature during forward 
stepwise model selection over 10 iterations and 10 folds (100 feature 
sets in total).</div></div></div></div><div content-id="i1534-7362-14-3-11-f05" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 5</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Bar
 plot showing the frequency of inclusion of each feature during forward 
stepwise model selection over 10 iterations and 10 folds (100 feature 
sets in total).</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Bar plot showing the frequency of inclusion of each feature during forward stepwise model selection over 10 iterations and 10 folds (100 feature sets in total)." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f05"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f05" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f05.jpeg?Expires=1581154411&amp;Signature=e9uidvcCTf-qgygmEAnL-ME-PoS~D68Hlls3~NUxi4N3ox~bvbk-7KsgrnXf8f9x9P0X83lShs5jfwxTCc~SDW5vw8PG9LN7DjBdJG44d-kN6jBiYnSOZkc4eN~P4KjiNmTgeHwIL5xTFwujWgQ5wep5--clpws8jI3vxHYQ0cxzLIPAhi0Hx~llXC7OTSmwXuEnuHZrbwAj1E8orcBhCwAvaX1aXOvfKG3ZCics7XVyJnAbeutsxCRj-ZsU~wkiBacFCPskCZGeLm7Rf9ahYArKIs-MYoQ7pT5pxwoGRfO6h3wKnfQYfuZXBoJBQ1t~bQbf0P3e1BftSUSx9spnuQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793263&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f05">Download Slide</a></div></div></div></div>
    </section>
    <section class="figure-table-wrapper">
        <div data-id="i1534-7362-14-3-11-f06" class="figure-section"><div class="title"><span class="label">Figure 6</span></div><div class="graphic-wrapper"><a reveal-id="i1534-7362-14-3-11-f06" class="revealLink figLink"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/m_i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=KLW7KxBZz1mDrs0NkbBwRcwlZiNqP0~yZU6tuIQ1vXZr4TYqVt34wQ4LWzk1-tuxJH4cbVHcMI9n1wWTOYIHwwvVCWW1c9LpG2Cpps~RqcJxhzsLJi2Di1WNWptaYfBEpOZn9hBp3g-jQUfxmpxWk5pFs9D0jhX8Qu6iINQCAo2zr~Bi5Q6052K6D8ye0jqg6BVUpg8V6PmoOTKNZwOD4VUt-gBKrr7ZrhRrT9wVu2NdUAMbXGG9Hh5FbOyXMtnhgXYWR8TehPTmpGsIv9QV6oF8XSQu7eOQpmUO7ro7-aO2~6hlBrjAnSjIWoaVI9aINwI7DKyKP3ovJU~PRH6HeA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Scatter plot of Tasks as a function of Mean Saccade Amplitude (pixels), and Initiation Time (ms). The observations are represented as full dots, and the predicted values are represented as asterisks. The three different tasks are color coded as Description (green), Naming (red), Search (blue). The colored contours represent the SVM classification distributions for the three tasks." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f06"></a><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f06" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793267&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f06">Download Slide</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Scatter
 plot of Tasks as a function of Mean Saccade Amplitude (pixels), and 
Initiation Time (ms). The observations are represented as full dots, and
 the predicted values are represented as asterisks. The three different 
tasks are color coded as Description (green), Naming (red), Search 
(blue). The colored contours represent the SVM classification 
distributions for the three tasks.</div></div></div></div><div content-id="i1534-7362-14-3-11-f06" class="hide"><div class="figure-section"><div class="title"><span class="label">Figure 6</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Scatter
 plot of Tasks as a function of Mean Saccade Amplitude (pixels), and 
Initiation Time (ms). The observations are represented as full dots, and
 the predicted values are represented as asterisks. The three different 
tasks are color coded as Description (green), Naming (red), Search 
(blue). The colored contours represent the SVM classification 
distributions for the three tasks.</div></div></div></div><div class="graphic-wrapper"><img src="grey.html" data-original="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Scatter plot of Tasks as a function of Mean Saccade Amplitude (pixels), and Initiation Time (ms). The observations are represented as full dots, and the predicted values are represented as asterisks. The three different tasks are color coded as Description (green), Naming (red), Search (blue). The colored contours represent the SVM classification distributions for the three tasks." class="contentFigures lazy" path-from-xml="i1534-7362-14-3-11-f06"><div class="original-slide"><a section="[XSLTSectionID]" class="viewOriginalSlide" href="https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" path-from-xml="i1534-7362-14-3-11-f06" target="_blank">View Original</a><a section="[XSLTSectionID]" href="http://jov.arvojournals.org/downloadimage.aspx?image=https://arvo.silverchair-cdn.com/arvo/content_public/journal/jov/932817/i1534-7362-14-3-11-f06.jpeg?Expires=1581154411&amp;Signature=pjo~kZCndcHa5xX7TfehgkIG-5z8Uu3Fp9VL6kPgMrZNfPLrbdfdSsrY51KG-TJ7zwJPUkGzMV~Ly7vqRGBfp18jwJPLBpCr0byqD0nWP3iHuBoYzAXuTMu2T~SgsAI190SZEAwO1NK7CgWbhivxWS8aWOwzB5vxZJMssBPwifrcfvdqfSYY9hXldjrdsYiQl7NKVfI7GU3hfBBB-pBF8f2K6HdU6n4ZzxPqxbcMMOyp~TCGhaZP-zLHqOkwuWa9eK5CrtfIlHoG0TesXuCFEsrmWvCJ6ZQwLrTmqXk8As1q4BY0ps0-FP4R2yWSxf08Fjy~nfcVZGU3u54pIg7gag__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA&amp;sec=87793267&amp;ar=2121494&amp;imagename=&amp;siteId=170" class="downloadSlide" path-from-xml="i1534-7362-14-3-11-f06">Download Slide</a></div></div></div></div>
    </section>

 
    </div>

                </div>
                <div id="TableTab" class="content">
                        <div class="widget-ArticleFiguresAndTables widget-instance-ARVO_Article_Tables_Tab">
        
    <section class="figure-table-wrapper">
        <div class="table-section clearfix"><div class="title"><span class="label">Table 1</span><div data-id="i1534-7362-14-3-11-t01" class="table-graphic"><i class="icon-table">&nbsp;</i><div class="original-slide"><a reveal-id="i1534-7362-14-3-11-t01" class="revealLink tablelink">View Table</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Coefficients of linear–mixed effects models with maximal random structure (intercept and slopes on Participants and Scenes). <em>Notes</em>:
 Each feature is modeled as a function of Task, which is contrast coded 
with Naming as a reference level for Description and Search.</div></div></div></div></div><div content-id="i1534-7362-14-3-11-t01" class="hide"><div class="table-section"><div class="title"><span class="label">Table 1</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Coefficients of linear–mixed effects models with maximal random structure (intercept and slopes on Participants and Scenes). <em>Notes</em>:
 Each feature is modeled as a function of Task, which is contrast coded 
with Naming as a reference level for Description and Search.</div></div></div></div><div class="tableContainer"><div class="tTable"><table>             <thead> <tr> <td rowspan="2" align="left">Features</td> <td colspan="3" align="">Intercept</td> <td colspan="3" align="">Description versus naming</td> <td colspan="3" align="">Search versus naming</td> </tr> <tr> <td align=""><em>β</em></td> <td align=""><em>SE</em></td> <td align=""><em>p</em></td> <td align=""><em>β</em></td> <td align=""><em>SE</em></td> <td align=""><em>p</em></td> <td align=""><em>β</em></td> <td align=""><em>SE</em></td> <td align=""><em>p</em></td> </tr> </thead> <tbody> <tr> <td align="">Number of fixations</td> <td align="">26.85</td> <td align="">0.99</td> <td align="">0.0001</td> <td align="">−1.89</td> <td align="">2.90</td> <td align="">0.1</td> <td align="">−26</td> <td align="">2.38</td> <td align="">0.0001</td> </tr> <tr> <td align="">Area fixated</td> <td align="">3.52</td> <td align="">0.11</td> <td align="">0.0001</td> <td align="">−0.75</td> <td align="">0.30</td> <td align="">0.0001</td> <td align="">−2.62</td> <td align="">0.28</td> <td align="">0.0001</td> </tr> <tr> <td align="">Mean saccade amplitude</td> <td align="">175.04</td> <td align="">2.58</td> <td align="">0.0001</td> <td align="">−21.52</td> <td align="">6.64</td> <td align="">0.0001</td> <td align="">80.12</td> <td align="">7.23</td> <td align="">0.0001</td> </tr> <tr> <td align="">Mean gaze duration</td> <td align="">250.65</td> <td align="">4.06</td> <td align="">0.0001</td> <td align="">−17.51</td> <td align="">10.9</td> <td align="">0.003</td> <td align="">−57.81</td> <td align="">10.65</td> <td align="">0.0001</td> </tr> <tr> <td align="">Dwell body</td> <td align="">0.19</td> <td align="">0.004</td> <td align="">0.0001</td> <td align="">0.07</td> <td align="">0.01</td> <td align="">0.0001</td> <td align="">−0.02</td> <td align="">0.01</td> <td align="">0.02</td> </tr> <tr> <td align="">Dwell face</td> <td align="">0.11</td> <td align="">0.003</td> <td align="">0.0001</td> <td align="">0.02</td> <td align="">0.01</td> <td align="">0.04</td> <td align="">0.02</td> <td align="">0.01</td> <td align="">0.01</td> </tr> <tr> <td align="">Dwell object</td> <td align="">0.69</td> <td align="">0.005</td> <td align="">0.0001</td> <td align="">−0.09</td> <td align="">0.01</td> <td align="">0.0001</td> <td align="">0</td> <td align="">0.0</td> <td align="">0.9</td> </tr> <tr> <td align="">Initiation time</td> <td align="">312.01</td> <td align="">8.05</td> <td align="">0.0001</td> <td align="">97.39</td> <td align="">21.33</td> <td align="">0.0001</td> <td align="">63.79</td> <td align="">21.51</td> <td align="">0.0001</td> </tr> <tr> <td align="">Saliency</td> <td align="">247.41</td> <td align="">0.75</td> <td align="">0.0001</td> <td align="">5.70</td> <td align="">2.16</td> <td align="">0.005</td> <td align="">1.18</td> <td align="">2.15</td> <td align="">0.5</td> </tr> <tr> <td align="">Entropy</td> <td align="">11.47</td> <td align="">0.02</td> <td align="">0.0001</td> <td align="">−0.19</td> <td align="">0.06</td> <td align="">0.0001</td> <td align="">−0.38</td> <td align="">0.06</td> <td align="">0.0001</td> </tr> </tbody> </table></div></div></div></div>
    </section>
    <section class="figure-table-wrapper">
        <div class="table-section clearfix"><div class="title"><span class="label">Table 2</span><div data-id="i1534-7362-14-3-11-t02" class="table-graphic"><i class="icon-table">&nbsp;</i><div class="original-slide"><a reveal-id="i1534-7362-14-3-11-t02" class="revealLink tablelink">View Table</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance for each task (object naming, scene 
description, and visual search) computed over 10 folds of the eye 
movement using different sets of eye-movement features: <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">Greene et al.'s (2012)</a> features (GF), other features (OF), and all features (All). <em>Notes</em>: <em>F</em>-scores
 are reported for three different classifiers: least-angle regression 
(LASSO), multinomial logistic regression (MM), and support-vector 
machine (SVM). Boldface indicates the best <em>F</em>-scores achieved for each task and classifier.</div></div></div></div></div><div content-id="i1534-7362-14-3-11-t02" class="hide"><div class="table-section"><div class="title"><span class="label">Table 2</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Mean <em>F</em>-score
 classification performance for each task (object naming, scene 
description, and visual search) computed over 10 folds of the eye 
movement using different sets of eye-movement features: <a reveal-id="i1534-7362-14-3-11-greene1" class="revealLink refLink">Greene et al.'s (2012)</a> features (GF), other features (OF), and all features (All). <em>Notes</em>: <em>F</em>-scores
 are reported for three different classifiers: least-angle regression 
(LASSO), multinomial logistic regression (MM), and support-vector 
machine (SVM). Boldface indicates the best <em>F</em>-scores achieved for each task and classifier.</div></div></div></div><div class="tableContainer"><div class="tTable"><table>              <thead> <tr> <td rowspan="2" align="left">Scene clutter</td> <td rowspan="2" align="">Task</td> <td colspan="3" align="">LASSO</td> <td colspan="3" align="">MM</td> <td colspan="3" align="">SVM</td> </tr> <tr> <td align="">GF</td> <td align="">OF</td> <td align="">All</td> <td align="">GF</td> <td align="">OF</td> <td align="">All</td> <td align="">GF</td> <td align="">OF</td> <td align="">All</td> </tr> </thead> <tbody> <tr> <td rowspan="4" align="">High</td> <td align="">Naming</td> <td align="">.77</td> <td align="">.81</td> <td align="">.82</td> <td align="">.8</td> <td align="">.81</td> <td align="">.84</td> <td align="">.81</td> <td align="">.85</td> <td align=""><strong>.86</strong></td> </tr> <tr> <td align="">Description</td> <td align="">.61</td> <td align="">.65</td> <td align="">.66</td> <td align="">.65</td> <td align="">.67</td> <td align="">.71</td> <td align="">.68</td> <td align="">.71</td> <td align=""><strong>.74</strong></td> </tr> <tr> <td align="">Search</td> <td align="">.8</td> <td align="">.79</td> <td align="">.82</td> <td align="">.81</td> <td align="">.81</td> <td align=""><strong>.83</strong></td> <td align="">.82</td> <td align="">.8</td> <td align=""><strong>.83</strong></td> </tr> <tr> <td align="">Naming</td> <td align="">.75</td> <td align="">.8</td> <td align="">.8</td> <td align="">.77</td> <td align="">.82</td> <td align="">.83</td> <td align="">.79</td> <td align=""><strong>.86</strong></td> <td align=""><strong>.86</strong></td> </tr> <tr> <td rowspan="2" align="">Low</td> <td align="">Description</td> <td align="">.64</td> <td align="">.66</td> <td align="">.67</td> <td align="">.66</td> <td align="">.69</td> <td align="">.74</td> <td align="">.67</td> <td align="">.76</td> <td align=""><strong>.77</strong></td> </tr> <tr> <td align="">Search</td> <td align="">.86</td> <td align="">.85</td> <td align="">.86</td> <td align="">.86</td> <td align="">.86</td> <td align=""><strong>.88</strong></td> <td align="">.87</td> <td align="">.85</td> <td align=""><strong>.88</strong></td> </tr> </tbody> </table></div></div></div></div>
    </section>
    <section class="figure-table-wrapper">
        <div class="table-section clearfix"><div class="title"><span class="label">Table 3</span><div data-id="i1534-7362-14-3-11-t03" class="table-graphic"><i class="icon-table">&nbsp;</i><div class="original-slide"><a reveal-id="i1534-7362-14-3-11-t03" class="revealLink tablelink">View Table</a></div></div><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Percentage of misclassified trials using an SVM classifier trained on all features. <em>Notes</em>:
 The columns indicate the correct class in the test set; the rows 
indicate the class predicted by the classifier. For instance, 
Search-Correct, Description-Predicted gives the percentage of visual 
search instances misclassified as scene description instances.</div></div></div></div></div><div content-id="i1534-7362-14-3-11-t03" class="hide"><div class="table-section"><div class="title"><span class="label">Table 3</span><div class="caption"><div class="caption-legend"><a id="" class="jumplink-placeholder">&nbsp;</a><div class="para">Percentage of misclassified trials using an SVM classifier trained on all features. <em>Notes</em>:
 The columns indicate the correct class in the test set; the rows 
indicate the class predicted by the classifier. For instance, 
Search-Correct, Description-Predicted gives the percentage of visual 
search instances misclassified as scene description instances.</div></div></div></div><div class="tableContainer"><div class="tTable"><table>       <thead> <tr> <td align="left"></td> <td align="">Search-correct</td> <td align="">Description-correct</td> <td align="">Naming-correct</td> </tr> </thead> <tbody> <tr> <td align="">Search-predicted</td> <td align="">0</td> <td align="">12</td> <td align="">5</td> </tr> <tr> <td align="">Description-predicted</td> <td align="">8</td> <td align="">0</td> <td align="">8</td> </tr> <tr> <td align="">Naming-predicted</td> <td align="">2</td> <td align="">13</td> <td align="">0</td> </tr> </tbody> </table></div></div></div></div>
    </section>

 
    </div>

                </div>
                <div id="SupplementTab" class="content">
                    
                </div>
            </section>
             
            <div class="content articleCopyright text-center">
                <span id="BodyContent_PageContent_lblArticleCopyright"> <div class="copyright-statement">© 2014 ARVO</div>  </span>
            </div>
            <div class="inline-signin-module">
                


        </div>
        </div>

        <div id="SidebarColumn" class="large-3 medium-9 medium-push-3 large-push-0 end columns sidebar-column">
            <div class="sidebar-widget clearfix">
                    <div class="widget-ArticleLevelMetrics widget-instance-ARVO_ArticleMetrics">
        





    <div class="artmet-condensed-wrap clearfix">
        <div class="artmet-condensed-stats clearfix">

                <div class="artmet-item artmet-views">
                    <span class="artmet-number">1,816</span>
                    <span class="artmet-text">Views</span>
                </div>

            <div class="artmet-item artmet-citations">
                <span class="artmet-number">
                        <a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&amp;SrcApp=PARTNER_APP&amp;SrcAuth=LinksAMR&amp;KeyUT=WOS:000334344500011&amp;DestLinkType=CitingArticles&amp;DestApp=ALL_WOS&amp;UsrCustomerID=61f30d8ae69c46f86624c5f98a3bc13a" target="_blank">11</a>
                </span>
                <span class="artmet-text">Citations</span>
            </div>

                        <div class="artmet-item artmet-altmetric">
    <div class="widget-AltmetricLink widget-instance-ArticleLevelMetrics_AltmetricLinkSummary">
            <!-- Altmetrics -->
    <div id="altmetricEmbedId" runat="server" class="altmetric-embed" data-badge-type="donut" data-hide-no-mentions="false" data-doi="10.1167/14.3.11" data-link-target="_blank" data-uuid="da80a446-794c-2eb0-ce23-48777938ae6f"><a style="cursor: pointer; display: inline-block; background-image: url(&quot;https://badges.altmetric.com/?size=64&amp;score=?&amp;types=????????&amp;style=donut&quot;); background-repeat: no-repeat; width: 64px; height: 64px;" href="https://www.altmetric.com/details.php?domain=jov.arvojournals.org&amp;doi=10.1167%2F14.3.11" target="_blank"> </a></div>
         <script type="text/javascript" src="embed.js"></script>
 
    </div>

            </div>
        </div>

            <div class="artmet-modal-trigger-wrap clearfix">
                <a class="artmet-modal-trigger" data-article-id="2121494"><i class="icon-metrics"></i><span>View Metrics</span></a>
            </div>
    </div>
        <div class="artmet-modal" id="MetricsModal">
            <div class="artmet-modal-contents">
                <a class="artmet-close-modal">×</a>
            </div>
        </div>

 
    </div>

            </div>
            <div id="divSeeAlso" class="sidebar-widget clearfix">
                    <div class="widget-ArticleLinks widget-instance-ARVO_SeeAlso_ArticleLinks">
        
 
    </div>

            </div>
            <div class="sidebar-widget clearfix">
                    <div class="widget-EditorsChoice widget-instance-ARVO_Editors_Choice">
            
    
 
    </div>

            </div>
            <div class="sidebar-widget clearfix">
                    <div class="widget-RelatedContent widget-instance-ARVO_Article_RelatedContent">
            <h4>Related Articles</h4>

    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2121536">Salient collinear grouping diminishes local salience in visual search: An eye movement study</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2194113">A computational model for task inference in visual search</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2194100">Distinguishing between target and nontarget fixations in a visual search task using fixation-related potentials</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2297282">Fixation-related potentials in visual search: A combined EEG and eye tracking study</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2191967">Non-spatial sounds regulate eye movements and enhance visual search</a></div>
 
    </div>

            </div>
             <div class="sidebar-widget clearfix">
                     <div class="widget-RelatedContent widget-instance-ARVO_Article_RelatedContentOther">
            <h4>From Other Journals</h4>

    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2702314">Visual Search in Amblyopia: Abnormal Fixational Eye Movements and Suboptimal Sampling Strategies</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2687000">Does Glaucoma Alter Eye Movements When Viewing Images of Natural Scenes? A Between-Eye Study</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2671889">Eye Movement Control in the Argus II Retinal-Prosthesis Enables Reduced Head Movement and Better Localization Precision</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2648878">Identification
 of Characters and Localization of Images Using Direct 
Multiple-Electrode Stimulation With a Suprachoroidal Retinal Prosthesis</a></div>
    <div class="article-title"><a href="http://jov.arvojournals.org/article.aspx?articleid=2624242">Disrupted Eye Movements in Preperimetric Primary Open-Angle Glaucoma</a></div>
 
    </div>

            </div>
            <div class="sidebar-widget clearfix">
                    <div class="widget-RelatedTopics widget-instance-ARVO_Article_RelatedTopics">
            <h4 class="h4">Related Topics</h4>

<ul>
        <li>    <a href="http://jov.arvojournals.org/solr/topicresults.aspx?f_Categories=Eye+Anatomy+and+Disorders&amp;resourceid=38473" id="item_Link">Eye Anatomy and Disorders</a></li>
        <li>    <a href="http://jov.arvojournals.org/solr/topicresults.aspx?f_Categories=Eye+Movements%2c+Strabismus%2c+Amblyopia+and+Neuro-ophthalmology&amp;resourceid=38492" id="item_Link_1">Eye Movements, Strabismus, Amblyopia and Neuro-ophthalmology</a></li>
        <li>    <a href="http://jov.arvojournals.org/solr/topicresults.aspx?f_Categories=Visual+Psychophysics+and+Physiological+Optics&amp;resourceid=38488" id="item_Link_2">Visual Psychophysics and Physiological Optics</a></li>
</ul>
        
 
    </div>

            </div>
            
            





<div id="BodyContent_PageContent_ucAdvertisingBlock_adTextCenterDiv" class="advertisement text-center">
    <div class="ad-text">Advertisement</div>
     
    <div class="div-gpt-ad-content-towers">
    <script type="text/javascript">
        DisplayAdvertisingBlock();
    </script><div id="div-gpt-ad-content-tower"></div>
    </div>
     
</div>
        </div>
    </div>    

            </section>
            <section class="master-footer">
                

<footer class="footer">
    <div class="row collapse" data-equalizer="">
        
        <div id="FooterMicro" class="large-5 medium-12 columns footer-micro" data-equalizer-watch="" style="height: 396px;">
            <img src="arvo_jov_logo-white.png" id="BodyContent_footer_JournalLogo" class="journal-logo">
            
            
            <a href="https://twitter.com/arvojov" id="BodyContent_footer_JovTwitterFollow" class="twitter-follow-button clearfix" data-show-count="false" data-size="large" data-twitter-extracted-i1581060848963193171="true">Follow @ARVOjov</a>
            
            <script>!function (d, s, id) { var js, fjs = d.getElementsByTagName(s)[0], p = /^http:/.test(d.location) ? 'http' : 'https'; if (!d.getElementById(id)) { js = d.createElement(s); js.id = id; js.src = p + '://platform.twitter.com/widgets.js'; fjs.parentNode.insertBefore(js, fjs); } }(document, 'script', 'twitter-wjs');</script>

            <div class="row">
                <div class="medium-4 large-6 columns">
                        
                                <ul class="foot-menu">
                            
                                 <li id="liMenuItem"><a href="http://jov.arvojournals.org/index.aspx" id="aMenuItem">JOV Home</a>
                                 
                                 </li>
                            
                                 <li id="liMenuItem"><a href="http://jov.arvojournals.org/issues.aspx" id="aMenuItem">Issues</a>
                                 
                                 </li>
                            
                                 <li id="liMenuItem"><a href="http://jov.arvojournals.org/topics.aspx" id="aMenuItem">Topics</a>
                                 
                                 </li>
                            
                                 <li id="liMenuItem"><a href="http://jov.arvojournals.org/ss/forauthors.aspx" id="aMenuItem">For Authors</a>
                                 
                                 </li>
                            
                                </ul>
                            
                </div>
                <div class="medium-8 large-6 columns">
                        
                                <ul class="foot-menu">
                            
                                 <li id="liMenuItem" class="has-sub-menu"><a href="http://jov.arvojournals.org/ss/about.aspx" id="aMenuItem" class="no-link">About</a>
                                 
                                        <ul class="foot-sub-menu">
                                    
                                         <li><a href="http://jov.arvojournals.org/ss/editorial_board.aspx">Editorial Board</a></li>
                                    
                                        </ul>
                                    
                                 </li>
                            
                                </ul>
                            
                        <div class="issn">
                            Online ISSN: 1534-7362
                        </div>
                </div>
            </div>
            
            <i class="icon-arvo"></i>
        </div>

        
        <div id="FooterUmb" class="large-7 medium-12 columns footer-umb" data-equalizer-watch="" style="height: 396px;">
            <div class="social outside clearfix">
                <a href="https://www.facebook.com/ARVOinfo?ref=search&amp;sid=41805257.1733737545..1" target="_blank"><i class="icon-facebook"></i></a>
                <a href="https://twitter.com/ARVOinfo" target="_blank"><i class="icon-twitter"></i></a>
                <a href="https://www.linkedin.com/company/arvoinfo/" target="_blank"><i class="icon-linkedin"></i></a>
                <a href="https://www.youtube.com/arvoinfo" target="_blank"><i class="icon-youtube"></i></a>
            </div>

            <div class="row">
                <div class="medium-12 columns">
                    <a href="http://jov.arvojournals.org/index.aspx"><img class="journals-logo lazy" data-original="/UI/app/images/arvo_journals_logo.png"></a>
                    <div class="social inside clearfix">
                        <a href="https://www.facebook.com/ARVOinfo?ref=search&amp;sid=41805257.1733737545..1" target="_blank"><i class="icon-facebook"></i></a>
                        <a href="https://twitter.com/ARVOinfo" target="_blank"><i class="icon-twitter"></i></a>
                        <a href="https://www.linkedin.com/company/arvoinfo/" target="_blank"><i class="icon-linkedin"></i></a>
                        <a href="https://www.youtube.com/arvoinfo" target="_blank"><i class="icon-youtube"></i></a>
                    </div>
                </div>
            </div>

            <div class="row">
                <div id="JournalsFoot" class="medium-4 columns journals-foot">
                    <ul>
                        
                                <li><a href="https://iovs.arvojournals.org/">Investigative Ophthalmology &amp; Visual Science</a></li>
                            
                                <li><a href="https://jov.arvojournals.org/">Journal of Vision</a></li>
                            
                                <li><a href="https://tvst.arvojournals.org/">Translational Vision Science &amp; Technology</a></li>
                            
                    </ul>
                </div>
                <div id="JournalsExtraLinks" class="medium-8 columns">
                    <ul class="clearfix">
                        <li><a href="https://arvojournals.org/index.aspx">JOURNALS HOME</a></li>
                        <li><a href="https://arvojournals.org/topics.aspx">TOPICS</a></li>
                        <li class="has-sub-menu"><a href="https://arvojournals.org/ss/about.aspx">ABOUT ARVO JOURNALS</a>
                            <ul class="foot-sub-menu">
                                <li><a href="https://arvojournals.org/ss/terms.aspx">Rights &amp; Permissions</a></li>
                                <li><a href="https://arvojournals.org/ss/privacy.aspx">Privacy Statement</a></li>
                                <li><a target="_blank" href="https://arvojournals.org/ss/advertising.aspx">Advertising</a></li>
                                <li><a target="_blank" href="https://www.arvo.org/journals_and_publications/submit_your_article/">Submit a Manuscript</a></li>
                                <li><a href="https://arvojournals.org/ss/disclaimer.aspx">Disclaimer</a></li>
                                <li><a href="https://arvojournals.org/contact.aspx">Contact Us</a></li>
                                <li><a href="https://www.arvo.org/" target="_blank">ARVO.org</a></li>
                            </ul>
                        </li>
                    </ul>
                </div>
                
            </div>
        </div>
    </div>
</footer>

<div class="text-center footer-bar">
    <a target="_blank" href="https://www.arvo.org/">
        <img data-original="/UI/app/Images/arvo_logo.png" class="arvo-logo lazy">
    </a>
    <a target="_blank" href="https://www.silverchair.com/">
        <img data-original="/UI/app/Images/logo_silverchair_footer.png" class="sis-logo lazy">
    </a>
    <div class="copyright">Copyright © 2015 Association for Research in Vision and Ophthalmology.</div>
</div>

            </section>
        </div>
    </div>
    
    <div id="SignInModal" class="reveal-modal small" data-reveal="">
        <div id="AccessSignIn" class="signin-container">
    
    <input name="ctl00$ctl00$BodyContent$globalSignInMaster$hfARVOStoreUrl" id="hfARVOStoreUrl" value="https://store.arvojournals.org" type="hidden">
    
            <div id="pnlGlobalSignin" class="si-panel" onkeypress="javascript:return WebForm_FireDefaultButton(event, 'BodyContent_globalSignInMaster_ibSignIn')">
	
                <div class="si-form clearfix">
                    <div class="message-error" id="wrong-username-password"></div>
                    
                    <input name="ctl00$ctl00$BodyContent$globalSignInMaster$txtEmail" maxlength="50" id="txtEmail" class="requiredtxtusername" placeholder="Username" type="text">
                    <div class="message-error" id="reqEmailError"></div>

		            <input name="ctl00$ctl00$BodyContent$globalSignInMaster$txtPassword" maxlength="50" id="txtPassword" class="requiredtxtpassword" placeholder="Password" type="password">

                    <div class="message-error" id="reqPasswordError"></div>
                </div>
                <div class="si-forgot-pass clearfix text-center">
                    <a href="http://jov.arvojournals.org/account/forgotpassword.aspx" class="forgot-password">Forgot password?</a>
                </div>
                <div class="btn-group text-center"><input name="ctl00$ctl00$BodyContent$globalSignInMaster$ibSignIn" value="Sign In" id="BodyContent_globalSignInMaster_ibSignIn" class="dark-button signInBtn" type="submit"></div>
                
                 <div id="signin-loading-spinner" class="signin-loading inner-circle hide"></div>
            
</div> 
                    
            <div id="SubscribeBox" data-hideattribute="true" class="subscribe-box box" style="display: none;">
                <h2>To View More...</h2>
                <p id="BodyContent_globalSignInMaster_pPurchaseSubInstruction">Purchase this article with an account.</p>
                <div class="subscribe-btns">
                    
                            <a href="http://jov.arvojournals.org/account/createaccount.aspx"><span class="dark-button">Create an Account</span></a>
                        
                    <p style="text-align: center;">or</p>
                    <a href="http://jov.arvojournals.org/ss/subscriptions.aspx"><span class="dark-button">Subscribe Now</span></a>    
                </div>
                
            </div>
        
</div>


        <a class="close-reveal-modal">×</a>
    </div>
    


        <div id="revealModal" class="reveal-modal" data-reveal="">
            <div id="revealContent"></div>
            <a class="close-reveal-modal">×</a>
        </div>


        <div id="NoAccessReveal" class="reveal-modal tiny" data-reveal="">
            <div id="NoAcccessRevealContent">
                <p>This PDF is available to Subscribers Only</p>
                <a class="issue-pdf-link purchase-pdf-link" id="NewsletterPdfPurchaseLink" data-reveal=""><span>Sign in or purchase a subscription to access this content.</span></a>
                <a class="close-reveal-modal">×</a>
            </div>
        </div>

        <div id="NeedIndividualAcct" class="reveal-modal tiny" data-reveal="">
            <div class="subscription-needed-msg theme-bg-color">
                <h4>You must be signed into an individual account to use this feature.</h4>
            </div>

            <a class="close-reveal-modal">×</a>
        </div>

        
    
    



            <div class="widget-GdprCookieBanner widget-instance-GdprCookieBanner">
        <div class="gdpr-cookie-wrapper">
    <div class="gdpr-cookie-body">
        This site uses cookies. By continuing to use our website, you are agreeing to <a href="https://www.arvojournals.org/ss/privacy.aspx">our privacy policy.</a><span class="pipe"></span><a class="js-gdpr-cookie-acceptLink">Accept</a>
    </div>
	
    <input id="hdnDomainGdpr" value=".arvojournals.org" type="hidden">
    <input id="hdnClientIdGdpr" value="19" type="hidden">
</div> 
    </div>

<script type="text/javascript">
//<![CDATA[
var theForm = document.forms['webform'];
if (!theForm) {
    theForm = document.webform;
}
function __doPostBack(eventTarget, eventArgument) {
    if (!theForm.onsubmit || (theForm.onsubmit() != false)) {
        theForm.__EVENTTARGET.value = eventTarget;
        theForm.__EVENTARGUMENT.value = eventArgument;
        theForm.submit();
    }
}
//]]>
</script>


<script src="WebResource.js" type="text/javascript"></script>
</form>
    <div id="mathRevealModal" class="reveal-modal" data-reveal=""></div>

    
<script src="masterJS.js" type="text/javascript"></script>
<script src="contentJS.js" type="text/javascript"></script>
<script src="chartist.js" type="text/javascript"></script>
<script src="addthis_widget.js" type="text/javascript"></script><div style="visibility: hidden; height: 1px; width: 1px; position: absolute; top: -9999px; z-index: 100000;" id="_atssh"><iframe id="_atssh466" title="AddThis utility frame" style="height: 1px; width: 1px; position: absolute; top: 0px; z-index: 100000; border: 0px none; left: 0px;" src="https://s7.addthis.com/static/sh.f48a1a04fe8dbf021b4cda1d.html#rand=0.1848105881021591&amp;iit=1581060847781&amp;tmr=load%3D1581060847622%26core%3D1581060847752%26main%3D1581060847764%26ifr%3D1581060847794&amp;cb=0&amp;cdn=0&amp;md=0&amp;kw=&amp;ab=-&amp;dh=jov.arvojournals.org&amp;dr=&amp;du=http%3A%2F%2Fjov.arvojournals.org%2Farticle.aspx%3Farticleid%3D2121494&amp;href=http%3A%2F%2Fjov.arvojournals.org%2Farticle.aspx&amp;dt=Classification%20of%20visual%20and%20linguistic%20tasks%20using%20eye-movement%20features%20%7C%20JOV%20%7C%20ARVO%20Journals&amp;dbg=0&amp;cap=tc%3D0%26ab%3D0&amp;inst=1&amp;jsl=1&amp;prod=undefined&amp;lng=en&amp;ogt=&amp;pc=men&amp;pub=xa-5265518246c10183&amp;ssl=0&amp;sid=5e3d12ef3501ed62&amp;srf=0.01&amp;ver=300&amp;xck=0&amp;xtr=0&amp;og=&amp;csi=undefined&amp;rev=v8.28.3-wp&amp;ct=1&amp;xld=1&amp;xd=1"></iframe></div><style id="service-icons-0"></style>
<script src="MathJax.js" type="text/javascript"></script>


    <!--[if (gte IE 6)&(lte IE 8)]>
            <script type="text/javascript" src="/UI/app/scripts/polyfills/rem.min.js"></script>
        <![endif]-->

<iframe scrolling="no" allowtransparency="true" src="https://platform.twitter.com/widgets/widget_iframe.7303c29a8108bca4ac5c9ef008ed8164.html?origin=http%3A%2F%2Fjov.arvojournals.org" title="Twitter settings iframe" style="display: none;" frameborder="0"></iframe><script type="text/javascript" src="14.txt"></script></body></html>