\section{Methods}

\subsection{Participants}
Two separate datasets were used to develop and test the deep CNN architecture. The two datasets were collected from two separate experiments, which we will refer to as Exploratory and Confirmatory. The participants for both datasets consisted of college students (Exploratory _N_ = 124; Confirmatory _N_ = 77) from the University of Nebraska-Lincoln who participated in exchange for class credit. Participants who took part in the exploratory experiment did not participate in the confirmatory experiment. All procedures and materials were approved by the University of Nebraska-Lincoln Institutional Review Board prior to data collection.

\subsection{Materials and Procedures}
Each participant viewed a series of <!-- 75? --><!-- indoor and outdoor --> scene images while carrying out a search, memorization, or rating task. For the search task, participants were instructed to find a 'Z' or 'N' embedded in the image. If the letter was found, the participants were instructed to press a button, which terminated the trial. For the memorization task, participants were instructed to memorize the image for a test that would take place when the task was completed. For the rating task, participants were asked to think about how they would rate the image on a scale from 1 (very unpleasant) to 7 (very pleasant). The participants were prompted for their rating immediately after viewing the image. The same materials were used in both experiments with a minor variation in the procedures. In the confirmatory experiment, participants were directed as to where search targets might appear in the image (e.g., on flat surfaces). No such instructions were provided in the exploratory experiment. In both experiments, trials were presented in one mixed block, and three separate task blocks. For the mixed block, the trial types were randomly intermixed within the block. For the three separate task blocks, each block consisted entirely of one of the three tasks (search, memorize, rate). Each trial was presented for 10 seconds. The inter-trial interval lasted <!--2--> seconds.
<!-- size of images, type of images, any other descriptive information about the images -->
<!-- timing of the trial -->

\subsection{Datasets}
Eye movements were recorded using an SR Research EyeLink II eye tracker with a sampling rate of 1000Hz. On some of the search trials, a probe was presented on the screen at six seconds. To equate the data from all three conditions, only the first six seconds of each trial were analyzed. Trials that were missing data were excluded before analysis. For both datasets, the trials were pooled across participants. After removing bad trials, the exploratory dataset consisted of 12,177 trials, and the confirmatory dataset consisted of 9,301 trials.

The raw x-coordinate, y-coordinate, and pupil size data collected at every sampling time point in the trial were used as inputs to the deep learning classifier. This data was also used to develop plot image datasets that were classified separately from the raw timeline datasets. For the plot image datasets, the timeline data for each trial were converted into scatterplot diagrams. The x- and y- coordinates and pupil size were used to plot each sample onto a scatterplot diagram (e.g., see Figure \@ref(fig:ave-condition)). The coordinates were used to plot the location of the dot, pupil size was used to determine the relative size of the dot, and shading of the dot was used to indicate the time-course of the eye movements throughout the trial. The background of the plot images and first data point was white. The final data point was black. Each subsequent data point in between became incrementally darker until the final data point was reached. To ensure that every data point was fully represented within the scatterplot image, the pupil size value was divided by 10, and one unit was added to ensure the dot was at least one full unit. The plots were sized to match the dimensions of the data collection monitor (1024 x 768 pixels) then shrunk to (240 x 180 pixels) in an effort to reduce the dimensionality of the data.

<!-- Search Memorize Rate -->
```{r ave-condition, fig.cap = "The confusion matrices for the timeline format have shown the same pattern of results for the image set.", echo = FALSE}
knitr::include_graphics(path = "images/cond_imgs.png")
```

\subsubsection{Data Subsets.}
The full timeline dataset was structured into three columns representing the x- and y- coordinates, and pupil size for every sample collected in the first six seconds of each trial. To systematically assess the predictive value of each XYP (i.e., x-coordinates, y-coordinates, pupil size) component of the data, the timeline and image datasets were parcellated into subsets that excluded one of the components (i.e., XY$\varnothing$, X$\varnothing$Y, $\varnothing$YP), or contained only one of the components (i.e., X$\varnothing\varnothing$, $\varnothing$Y$\varnothing$, $\varnothing\varnothing$P). For the timeline datasets, this means that the columns to be excluded in each data subset were replaced with zeros. The data were replaced with zeros because removing the columns would change the structure of the data. The same systematic parcellation process was carried out for the image dataset. See Figure \@ref(fig:ave-subset) for an example of each of these image data subsets.

<!-- Image Subset Figures -->
```{r ave-subset, fig.cap = "The confusion matrices for the timeline format have shown the same pattern of results for the image set.", echo = FALSE}
knitr::include_graphics(path = "images/subset_imgs.png")
```

\subsection{Classification}
Deep CNN model architectures were implemented to classify the trials into search, memorize, or rate categories. Each model split the data into 70% training, 15% validation, and 15% testing. Each network was run through 10 iterations of the data. Because the structure of the data generally plays a large role in CNN inferences, the differences in the structure of the timeline and image data formats required different CNN model architectures. The model architectures were developed with the intent of developing a generalizable model suited to the structure of the data. The development of these models was not guided by any formal theoretical assumptions regarding the patterns or features likely to be extracted by the classifier. The models were developed and tested on the exploratory dataset. Model parameters were adjusted until the classification accuracies no longer immproved. The model architecture with the highest classification accuracy on the exploratory dataset was tested independently on the confirmatory dataset. The model architectures used for the timeline and image datasets are shown in Figure \@ref(fig:models).

<!-- Models -->
```{r models, fig.cap = "Differences in the structure of the timeline and image datasets meant the CNN models had to be different. A. Timeline model architecture. B. Image model architecture.", echo = FALSE}
knitr::include_graphics(path = "images/models.png")
```

\subsection{Analysis}
Results for the CNN architecture that resulted in the highest accuracy on the exploratory dataset are reported below. For every dataset tested, a one-sample _t_-test was used to compare the CNN accuracies against chance (33%). The Shapiro-Wilk test of normality was conducted to test the normality for each dataset. When normality was assumed, the mean accuracy for that dataset was compared against chance using Student's one-sample _t_-test. When normality could not be assumed, the median accuracy for that dataset was compared against chance using Wilcoxon's Signed Rank test.

To determine the relative value of the three components of the eye movement data, the data subsets were compared within the timeline and plot image data types. If classification accuracies were lower when the data was parcellated, the component that was removed was assumed to have some diagnostic contribution that the model was using to inform classification decisions. To determine the relative value of the contribution from each component, the accuracies from each subset with one component of the data removed were compared to the accuracies for the full dataset (XYP) using a one-way between-subjects Analysis of Variance (ANOVA). To further evaluate the decodability of each component independently, the accuracies from each subset containing only one component of the eye movement data were compared within a separate one-way between-subject ANOVA. All post-hoc comparisons were corrected using Tukey's _HSD_.
