\section{Methods}
\subsection{Participants}
Two separate datasets were used to develop and test the deep CNN architecture. The two datasets were collected from two separate experiments, which we refer to as Exploratory and Confirmatory. The participants for both datasets consisted of college students (Exploratory _N_ = 124; Confirmatory _N_ = 77) from the University of Nebraska-Lincoln who participated in exchange for class credit. Participants who took part in the Exploratory experiment did not participate in the Confirmatory experiment. All procedures and materials were approved by the University of Nebraska-Lincoln Institutional Review Board prior to data collection.

\subsection{Materials and Procedures}
Each participant viewed a series of **XX**<!-- 75? --> indoor and outdoor scene images while carrying out a search, memorization, or rating task. For the search task, participants were instructed to find a 'Z' or 'N' embedded in the image. If the letter was found, the participants were instructed to press a button, which terminated the trial. For the memorization task, participants were instructed to memorize the image for a test that would take place when the task was completed. For the rating task, participants were asked to think about how they would rate the image on a scale from 1 (very unpleasant) to 7 (very pleasant). The participants were prompted for their rating immediately after viewing the image. The same materials were used in both experiments with a minor variation in the procedures. In the Confirmatory experiment, participants were directed as to where search targets might appear in the image (e.g., on flat surfaces). No such instructions were provided in the Exploratory experiment.

In both experiments, trials were presented in one mixed block, and three separate task blocks. For the mixed block, the trial types were randomly intermixed within the block. For the three separate task blocks, each block was **XX**<!-- number of trials --> trials consisting entirely of one of the three conditions (Search, Memorize, Rate). Each trial was presented for 10 seconds. The inter-trial interval lasted **XX**<!--2--> seconds. The participants were seated **XX**<!-- distance --> inches from a **XXresolutionXX**<!-- resolution --> monitor. The pictures were 1024 x 768 pixels, subtending a visual angle of **XX**<!-- size --> degrees.

\subsection{Datasets}
Eye movements were recorded using an SR Research EyeLink II eye tracker with a sampling rate of 1000Hz. On some of the search trials, a probe was presented on the screen six seconds from the onset of the trial. To avoid confounds resulting from the probe, only the first six seconds of the data in all three conditions were analyzed. Trials that contained fewer than 6000 samples were excluded before analysis. For both datasets, the trials were pooled across participants. After excluding trials, the Exploratory dataset consisted of 12,177 trials and the Confirmatory dataset consisted of 9,301 trials.

The raw x-coordinate, y-coordinate, and pupil size data collected at every sampling time point in the trial were used as inputs to the deep learning classifier. These data were also used to develop plot image datasets that were classified separately from the raw timeline datasets. For the plot image datasets, the timeline data for each trial were converted into scatterplot diagrams. The x- and y- coordinates and pupil size were used to plot each data point onto a scatterplot (e.g., see Figure \@ref(fig:ave-condition)). The coordinates were used to plot the location of the dot, pupil size was used to determine the relative size of the dot, and shading of the dot was used to indicate the time-course of the eye movements throughout the trial. The background of the plot images and first data point was white. Each subsequent data point was one shade darker than the previous data point until the final data point was reached. The final data point was black. For standardization, pupil size was divided by 10, and one **XXunit?XX**<!-- what are the units?? --> was added. The plots were sized to match the dimensions of the data collection monitor (1024 x 768 pixels) then shrunk to (240 x 180 pixels) in an effort to reduce the dimensionality of the data.

<!-- Search Memorize Rate -->
```{r ave-condition, fig.cap = "Each trial was represented as an image. Each sample collected within the trial was plotted as a dot in the image. Pupil size was represented by the size of the dot. The time course of the eye movements was represented by the gradual darkening of the dot over time.", echo = FALSE}
knitr::include_graphics(path = "figures/cond_imgs.png")
```

\subsubsection{Data Subsets.}
The full timeline dataset was structured into three columns representing the x- and y- coordinates, and pupil size for each data point collected in the first six seconds of each trial. To systematically assess the predictive value of each XYP (i.e., x-coordinates, y-coordinates, pupil size) component of the data, the timeline and image datasets were batched into subsets that excluded one of the components (i.e., XY$\varnothing$, X$\varnothing$Y, $\varnothing$YP), or contained only one of the components (i.e., X$\varnothing\varnothing$, $\varnothing$Y$\varnothing$, $\varnothing\varnothing$P). For the timeline datasets, this means that the columns to be excluded in each data subset were replaced with zeros. The data were replaced with zeros because removing the columns would change the structure of the data. The same systematic batching process was carried out for the image dataset. See Figure \@ref(fig:ave-subset) for an example of each of these image data subsets.

<!-- Image Subset Figures -->
```{r ave-subset, fig.cap = "Plot images were used to represent each type of data subset. As with the trials in the full XYP dataset, the time course of the eye movements was represented by the shading of the dot. The first sample of each trial was white, and the last sample was black.", echo = FALSE}
knitr::include_graphics(path = "figures/subset_imgs.png")
```

\subsection{Classification}
Deep CNN model architectures were implemented to classify the trials into Search, Memorize, or Rate categories. Because CNNs act as a digital filter sensitive to the number of features in the data, the differences in the structure of the timeline and image data formats necessitated separate CNN model architectures. The model architectures were developed with the intent of establishing a generalizable approach<!--model suited--> to classifying cognitive processes from eye movement data<!--the structure of the data-->.

The development of these models were not guided by any formal theoretical assumptions regarding the patterns or features likely to be extracted by the classifier. The models were developed using version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend ([http://delineate.it](http://delineate.it)). Each implementation of the model randomly split the data so that 70\% of the trial data were allocated to training, 15\% of the trial data were allocated to validation, and 15\% of the trial data were allocated to testing. Training of the model was stopped when validation accuracy did not improve over the span of 100 iterations. Once the early stopping threshold was reached, the resulting model was tested on the held out test data. This process was repeated 10 times for each model, resulting in 10 classification accuracy scores for each implementation of the model. The average of the resulting accuracy scores were the subject of comparisons against chance and other datasets or data subsets.

The models were developed and tested on the Exploratory dataset. Model hyperparameters were adjusted until the classification accuracies appeared to peak. The model architecture with the highest classification accuracy on the Exploratory dataset was trained, validated, and tested independently on the Confirmatory dataset. This means that the model that was used to analyze the Confirmatory dataset was not trained on the Exploratory dataset. The model architectures used for the timeline and plot image datasets are shown in Figure \@ref(fig:models).

<!-- Models -->
```{r models, fig.cap = "Two different model architectures were used to classify the timeline and image data. Both models were compiled using a categorical crossentropy loss function, and optimized with the Adam algorithm.", echo = FALSE}
knitr::include_graphics(path = "figures/models.png")
```

\subsection{Analysis}
Results for the CNN architecture that resulted in the highest accuracy on the Exploratory dataset are reported below. For every dataset tested, a one-sample two-tailed _t_-test was used to compare the CNN accuracies against chance (33/%). The Shapiro-Wilk test was used to assess the normality for each dataset. When normality was assumed, the mean accuracy for that dataset was compared against chance using Student's one-sample two-tailed _t_-test. When normality could not be assumed, the median accuracy for that dataset was compared against chance using Wilcoxon's Signed Rank test.

To determine the relative value of the three components of the eye movement data, the data subsets were compared within the timeline and plot image data types. If classification accuracies were lower when the data was batched into subsets, the component that was removed was assumed to have some diagnostic contribution that the model was using to inform classification decisions. To determine the relative value of the contribution from each component, the accuracies from each subset with one component of the data removed were compared to the accuracies for the full dataset (XYP) using a one-way between-subjects Analysis of Variance (ANOVA). To further evaluate the decodability of each component independently, the accuracies from each subset containing only one component of the eye movement data were compared within a separate one-way between-subject ANOVA. All post-hoc comparisons were corrected using Tukey's _HSD_.
