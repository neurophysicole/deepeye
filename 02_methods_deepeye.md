\section{Method}
\subsection{Participants}
Two separate datasets were used to develop and test the deep CNN architecture. The two datasets were collected from two separate experiments, which we refer to as Exploratory and Confirmatory. The participants for both datasets consisted of college students (Exploratory _N_ = 124; Confirmatory _N_ = 77) from the University of Nebraska-Lincoln who participated in exchange for class credit. Participants who took part in the Exploratory experiment did not participate in the Confirmatory experiment. All materials and procedures were approved by the University of Nebraska-Lincoln Institutional Review Board prior to data collection.

\subsection{Materials and Procedures}
Each participant viewed a series of indoor and outdoor scene images while carrying out a search, memorization, or rating task. For the memorization task, participants were instructed to memorize the image in anticipation of a forced choice recognition test. At the end of each Memorize trial, the participants were prompted to indicate which of two images was just presented. The two images were identical outside of a small change in the display (e.g. object removed or added to the scene). For the rating task, participants were asked to think about how they would rate the image on a scale from 1 (very unpleasant) to 7 (very pleasant). The participants were prompted to provide a rating immediately after viewing the image. For the search task, participants were instructed to find a small 'Z' or 'N' embedded in the image. In reality, targets were not present in the images outside of a small subset of images (_n_ = 5) that were not analyzed but were included in the experiment design so participants belived a target was always present. Trials containing the target were excluded because search behavior was likely to stop if the target was found, adding considerable noise to the eye movement data. For consistency between trial types, participants were prompted to indicate if they found a 'Z' or 'N' at the end of each Search trial.

The same materials were used in both experiments with a minor variation in the procedures. In the Confirmatory experiment, participants were directed as to where search targets might appear in the image (e.g., on flat surfaces). No such instructions were provided in the Exploratory experiment.

In both experiments, participants completed one mixed block of 120 trials (task cued prior to each trial), or three uniform blocks of 40 trials (task cued prior to each block for a total of 120 trials). Block type was assigned  in counterbalanced order. When the blocks were mixed, the trial types were randomly intermixed within the block. For uniform blocks, each block consisted entirely of one of the three conditions (Search, Memorize, Rate), with block types presented in random order. Each stimulus image was presented for 8 seconds. The pictures were presented in color, with a size of 1024 x 768 pixels, subtending a visual angle of 23.8$^{\circ}$ x 18.0$^{\circ}$.

Eye movements were recorded using an SR Research EyeLink 1000 eye tracker with a sampling rate of 1000Hz. Only the right eye was recorded. The system was calibrated using a nine-point accuracy and validity test. Errors greater than 1$^{\circ}$ or averaging greater than 0.5$^{\circ}$ in total were re-calibrated.<!-- When eye movement velocities remained below 30$^{\circ}$/s for 10 consecutive samples, movement offset was detected. -->

\subsection{Datasets}
On some trials, a probe was presented on the screen six seconds after the onset of the trial, which required participants to fixate the probe once detected. To avoid confounds resulting from the probe, only the first six seconds of the data for each trial was analyzed. Trials that contained fewer than 6000 samples within the first six seconds of the trial were excluded before analysis. For both datasets, the trials were pooled across participants. After excluding trials, the Exploratory dataset consisted of 12,177 of the 16,740 total trials, and the Confirmatory dataset consisted of 9,301 of the 10,395 total trials.

The raw x-coordinate, y-coordinate, and pupil size data collected at every sampling time point in the trial were used as inputs to the deep learning classifier. These data were also used to develop plot image datasets that were classified separately from the raw timeline datasets. For the plot image datasets, the timeline data for each trial were converted into scatterplot diagrams. The x- and y- coordinates and pupil size were used to plot each data point onto a scatterplot (e.g., see Figure \@ref(fig:ave-condition)). The coordinates were used to plot the location of the dot, pupil size was used to determine the relative size of the dot, and shading of the dot was used to indicate the time-course of the eye movements throughout the trial. The background of the plot images and first data point were white. Each subsequent data point was one shade darker than the previous data point until the final data point was reached. The final data point was black. For standardization, pupil size was divided by 10, and one unit<!-- pupil size data is reported in arbitrary units http://sr-research.jp/support/EyeLink%201000%20User%20Manual%201.5.0.pdf p.95 --> was added. The plots were sized to match the dimensions of the data collection monitor (1024 x 768 pixels) and then shrunk to (240 x 180 pixels) in an effort to reduce the dimensionality of the data.

<!-- Search Memorize Rate -->
```{r ave-condition, fig.cap = "Each trial was represented as an image. Each sample collected within the trial was plotted as a dot in the image. Pupil size was represented by the size of the dot. The time course of the eye movements was represented by the gradual darkening of the dot over time.", echo = FALSE}
knitr::include_graphics(path = "figures/cond_imgs.pdf")
```

\subsubsection{Data Subsets.}
The full timeline dataset was structured into three columns representing the x- and y- coordinates, and pupil size for each data point collected in the first six seconds of each trial. To systematically assess the predictive value of each XYP (i.e., x-coordinates, y-coordinates, pupil size) component of the data, the timeline and image datasets were batched into subsets that excluded one of the components (i.e., XY$\varnothing$, X$\varnothing$P, $\varnothing$YP), or contained only one of the components (i.e., X$\varnothing\varnothing$, $\varnothing$Y$\varnothing$, $\varnothing\varnothing$P). For the timeline datasets, this means that the columns to be excluded in each data subset were replaced with zeros. The data were replaced with zeros because removing the columns would change the structure of the data. The same systematic batching process was carried out for the image dataset. See Figure \@ref(fig:ave-subset) for an example of each of these image data subsets.

(ref:ave-subset-caption) Plot images were used to represent data subsets that excluded one component of the eye movement data (i.e., XY$\varnothing$, X$\varnothing$P, $\varnothing$YP) or contained only one component (i.e., X$\varnothing\varnothing$, $\varnothing$Y$\varnothing$, $\varnothing\varnothing$P). As with the trials in the full XYP dataset, the time course of the eye movements was represented by the shading of the dot. The first sample of each trial was white, and the last sample was black.

<!-- Image Subset Figures -->
```{r ave-subset, fig.cap = "(ref:ave-subset-caption)", echo = FALSE}
knitr::include_graphics(path = "figures/subset_imgs.pdf")
```

\subsection{Classification}
Deep CNN model architectures were implemented to classify the trials into Search, Memorize, or Rate categories. Because CNNs act as a digital filter sensitive to the number of features in the data, the differences in the structure of the timeline and image data formats necessitated separate CNN model architectures. The model architectures were developed with the intent of establishing a generalizable approach to classifying cognitive processes from eye movement data.

The development of these models was not guided by any formal theoretical assumptions regarding the patterns or features likely to be extracted by the classifier. Like many HCI models, the development of these models followed general intuitions concerned with building a model architecture capable of transforming the data inputs into an interpretable feature set that would not overfit the dataset. The models were developed using version 0.3b of the DeLINEATE toolbox, which operates over a Keras backend ([http://delineate.it](http://delineate.it); Kuntzelman et al., under review). Each training/test iteration randomly split the data so that 70\% of the trials were allocated to training, 15\% to validation, and 15\% to testing. Training of the model was stopped when validation accuracy did not improve over the span of 100 epochs. Once the early stopping threshold was reached, the resulting model was tested on the held-out test data. This process was repeated 10 times for each model, resulting in 10 classification accuracy scores for each model. The resulting accuracy scores were used for the comparisons against chance and other datasets or data subsets.

The models were developed and tested on the Exploratory dataset. Model hyperparameters were adjusted until the classification accuracies appeared to peak. The model architecture with the highest classification accuracy on the Exploratory dataset was trained, validated, and tested independently on the Confirmatory dataset. This means that the model that was used to analyze the Confirmatory dataset was not trained on the Exploratory dataset. The model architectures used for the timeline and plot image datasets are shown in Figure \@ref(fig:models).

<!-- Models -->
```{r models, fig.cap = "Two different model architectures were used to classify the timeline and image data. Both models were compiled using a categorical crossentropy loss function, and optimized with the Adam algorithm.", echo = FALSE}
knitr::include_graphics(path = "figures/models.pdf")
```

\subsection{Analysis}
Results for the CNN architecture that resulted in the highest accuracy on the Exploratory dataset are reported below. For every dataset tested, a one-sample two-tailed _t_-test was used to compare the CNN accuracies against chance (33\%). The Shapiro-Wilk test was used to assess the normality for each dataset. When normality was assumed, the mean accuracy for that dataset was compared against chance using Student's one-sample two-tailed _t_-test. When normality could not be assumed, the median accuracy for that dataset was compared against chance using Wilcoxon's Signed Rank test.

To determine the relative value of the three components of the eye movement data, the data subsets were compared within the timeline and plot image data types. If classification accuracies were lower when the data were batched into subsets, the component that was removed was assumed to have some unique contribution that the model was using to inform classification decisions. To determine the relative value of the contribution from each component, the accuracies from each subset with one component of the data removed were compared to the accuracies for the full dataset (XYP) using a one-way between-subjects Analysis of Variance (ANOVA). To further evaluate the decodability of each component independently, the accuracies from each subset containing only one component of the eye movement data were compared within a separate one-way between-subjects ANOVA. All post-hoc comparisons were corrected using Tukey's HSD.
