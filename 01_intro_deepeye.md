\section{}

The association between eye movements and mental activity is a fundamental topic of interest in attention research that  has provided a foundation for developing a wide range of human assistive technologies. Foundational work by Yarbus (1967) showed that eye movement patterns appear to differ qualitatively depending on the task-at-hand [for a review of this work, see @tatlerYarbusEyeMovements2010]. A replication of this work by @deangelusTopdownControlEye2009 shows that the differences in eye movements between tasks can be quantified, and appear to be generalizable. Technological advances and improvements in computing power have allowed researchers to determine the mental state underlying eye movement data, also known as the "inverse Yarbus process" [@haji-abolhassaniInverseYarbusProcess2014]. Current state-of-the-art machine learning and neural network algorithms are capable of identifying diagnostic patterns in the data for purposes of classification, but the inner workings of the resulting model solutions are difficult or impossible to interpret. Algorthims that provide uninterpretable solutions are referred to as _black box_ models. Dissections of black box models have been largely uninformative [@zhouComparingInterpretabilityDeep2019], discouraging their implementation in basic research. Still, black box models provide a convenient solution for techonological applications such as human-computer interfaces [HCI; for a review, see @lukanderInferringIntentAction2017]. While the internal operations of the model solutions used for HCI applications do not necessarily need to be interpretable to serve their purpose, @lukanderInferringIntentAction2017 pointed out that "the black box nature of the resulting solution impedes generalizability, and makes applying methods across real life conditions more difficult" (p. 44). To ground these solutions, researchers guide black box decoding efforts by providing data and/or models with built-in theoretical assumptions. For instance, eye movement data is processed into meaningful aggregate properties such as fixations or saccades, or statistical features such as as fixation density, and the models used to decode these data are structured based on the current understanding of relevant cognitive or neurobiological processes [e.g., @macinnesjosephGenerativeModelCognitive2018].

At this point, there is no clear evidence to support the notion that the standard theoretically grounded inferences actually enhance or clarify black box solutions beyond what could be inferred from an unconstrained model. Consider the case of @greeneReconsideringYarbusFailure2012a, who failed to classify task from commonly used aggregate eye movement features (i.e., number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations) using three separate model architectures (see Table \@ref(tab:previous-studies). This led Greene and colleagues to question the robustness of Yarbus's (1967) findings, inspiring a slew of responses that successfully decoded the same dataset by aggregating the eye movements into different feature sets or implementing different model architectures [see Table \@ref(tab:previous-studies); i.e., @haji-abolhassaniInverseYarbusProcess2014; @kananPredictingObserverTask2014; @borjiDefendingYarbusEye2014a]. The subsequent re-analyses of these data support Yarbus (1967) and the notion that mental state can be decoded using a variety of combinations of data features and model architectures. Despite using theoretically informed models to classify the data, these re-analyses do not explain the failures of @greeneReconsideringYarbusFailure2012a anymore definitively than a black box approach.

Eye movements can only be differentiated to the extent that the cognitive processes underlying the tasks can be delineated [@krolRightLookJob2018]. Every task is associated with a unique set of cognitive processes [@krolRightLookJob2018; @cocoClassificationVisualLinguistic2014]. In some cases, the cognitive processes for different tasks may produce indistinguishable eye movement patterns. To distinguish the cognitive processes underlying task-evoked eye movements, some studies have chosen to classify tasks that rely on stimuli to prompt easily distinguishable eye movements, such as reading text and searching pictures [e.g., @hendersonPredictingCognitiveState2013a]. The eye movements elicited by salient stimulus features confound classifications of complex mental states because these eye movements are the consequence of a feature, or features, inherent to the stimulus rather than the goal directed shifting of attention [e.g., @hendersonPredictingCognitiveState2013a; @boisvertPredictingTaskEye2016]. Additionally, the distinct nature of exogenously elicited eye movements prompts decoding algorithms to prioritize these bottom-up patterns in the data over higher-level top-down effects [@borjiDefendingYarbusEye2014a]. This means that these models are identifying the type of information that is being processed, but are not necessarily reflecting the mental state of the individual observing the stimulus. Eye movements that are the product of bottom-up attentional processes can be reliably decoded, which is relevant for some HCI applications, but does not fit the explicit top-down nature of the inverse Yarbus problem.

The mental processes underlying eye movements elicited from top-down attentional processes remain relatively undefined. Prior evidence has shown that the task-at-hand is capable of producing distinguishable eye movement features such as the percentage of the stimulus fixated, total scan path length, total number of fixations, and the amount of time to the first saccade [@castelhanoViewingTaskInfluences2009; @deangelusTopdownControlEye2009]. Typical decoding accuracies within the context of determining task from eye movements typically range from chance performance to 59.64% (see Table \@ref(tab:previous-studies)). In one case, @cocoClassificationVisualLinguistic2014 categorized the same eye movement features used by @greeneReconsideringYarbusFailure2012a with respect to the relative contribution of visual or linguistic components of three tasks (visual search, name the picture, name objects in the picture). By identifying the latent factors differentiating the tasks (i.e., relative linguistive or visual input), @cocoClassificationVisualLinguistic2014 was able to decode the eye movement data with 84% accuracy. What stands out in the example of @cocoClassificationVisualLinguistic2014 is the use of a high level abstraction of the relevant task components that allowed for clear distinctions that were evident in the eye mvoement data. While this manipulation is remiscent of other experiments relying on the bottom-up influence of words and pictures [e.g., @hendersonPredictingCognitiveState2013a; @boisvertPredictingTaskEye2016] the eye movements in the @cocoClassificationVisualLinguistic2014 tasks were entirely the product of top-down attentional processes. A conceptually similar follow-up to this study classified tasks along two spatial and semantic dimensions, resulting in 51% classification accuracy [@krolRightLookJob2018]. A closer look at these results showed that the categories within the semantic dimension were consistently mixed up, suggesting that this level of distinction may require a more rich dataset, or a more powerful decoding algorithm. Altogether, this body of literature suggests that the use of tasks requiring distinct top-down attentional processes is an important factor to consider when classifying mental state from eye movement data.

\begin{table}[h]
    \centering
    \caption{Previous Studies}
    \label{tab:previous-studies}
    \begin{tabular}{p{.23\linewidth} p{.23\linewidth} p{.23\linewidth} p{.23\linewidth}}
        Study & Tasks & Model Architecture & Accuracy (Chance) \\
        \hline
        Greene et al. (2012) & memory, decade, people, wealth & linear discriminant, correlation, SVM & 25.9\% (25\%) \\
        Haji-Abolhassani \& James (2014)  & Greene et al. tasks & Hidden Markov Models & 59.64\% (25\%) \\
        Kanan et al. (2014) & Greene et al. tasks & multi-fixation pattern analysis & 37.9\% (25\%) \\
        Borji \& Itti (2014)  & Greene et al. tasks & kNN, RUSBoost & 34.34\% (25\%) \\
        Borji \& Itti (2014) & Yarbus tasks & kNN, RUSBoost & 24.21\% (14.29\%) \\
        Coco \& Keller (2014) & visual search, picture naming, object naming & MM, LASSO, SVM & 84\% (33\%) \\
        MacInnes et al. (2018) & view, memorize, search, preference & augmented Naive Bayes Network & 53.9\% (25\%) \\
        Król \& Król (2018) & people, indoors/outdoors, white/black, dot search & feed forward neural network & 51.4\% (25\%) \\
        \hline
    \end{tabular}
\end{table}

As shown in Table \@ref(tab:previous-studies), when eye movement data are prepared for classification, fixation and saccade statistics are typically aggregated along spatial or temporal dimensions, resulting variables such as fixation density or saccade amplitude [@millsExaminingInfluenceTask2011; @macinnesjosephGenerativeModelCognitive2018; @castelhanoViewingTaskInfluences2009]. The implementation of these statistical methods is meant to explicitly provide the decoding algorithm with characteristics of the eye movement data that are representative of theoretically relevant cognitive processes. For example, @macinnesjosephGenerativeModelCognitive2018 attempted to provide an algorithm with data assumed to be representative of inputs to the frontal eye fields. In some instances, such as the case of @krolRightLookJob2018, grounding the data using theoretically driven aggregation methods may require sacrificing resolution in the dataset. This means that aggregating the data has the potential to wash out any fine-grained distinctinctions that could otherwise be detected. Data structures of any kind can only be decoded to the extent which the data is capable of representing differences between categories<!-- is there a Gallant referenece for this? -->. Given the cognitive processes underlying distinct tasks are often overlapping [@cocoClassificationVisualLinguistic2014], decreasing the resolution of the data may actually limit the potential of the algorithm to classify the task.

The current state of the literature does not provide any coherent guidelines for determining what eye movement features are most meaningful, or what model architectures are most suited for determining mental state from eye movements. The theoretically informed models shown in Table \@ref(tab:previous-studies) utilized a variety of eye movement features and model architectures, most of which were effective to a similar extent [with the exception of @greeneReconsideringYarbusFailure2012a]. The complexities underlying these findings are not yet well-defined or understood. Basic research has provided a foundation of understanding, but has not provided any coherent guidelines to support generalizable applications of this research [@lukanderInferringIntentAction2017]. In an attempt to support practical applications of this body of research, the current study explored pragmatic solutions to the problem of classifying task from eye movement data<!-- inverse Yarbus problem -->.

The current study aimed to maximize the resolution of the data by submitting unprocessed x-coordinate, y-coordinate, and pupil size data to a convolutional neural network (CNN) model. Instead of transforming the data into theoretically defined meaningful units, we allowed the network to establish its own meaningful patterns in the data. CNNs have a natural propensity to develop low level feature detectors similar to primary visual cortex [e.g., @seeligerConvolutionalNeuralNetworkbased2018]. For this reason, CNNs are commonly implemented for image classification. To test the possibility that the image data are better suited to the CNN classifier, the data will be decoded in raw timeline and image formats. To our knowledge, no study has attempted to address the inverse Yarbus problem<!-- decode mental task from endogenously oriented eye movement data --> using: (1) Non-aggregate data, (2) image data format, or (3) a CNN architecture. Given that CNN classification performance is robust to multidimensional, non-structured data<!--REF?-->, we expect the non-theoretically-constrained CNN architecture to decode both data types at levels consistent with the current state-of-the-art. Furthermore, we expect that despite evidence that black box approaches to the inverse Yarbus problem can be unreliable [@lukanderInferringIntentAction2017], our initial findings will replicate when tested on an entirely separate dataset.
