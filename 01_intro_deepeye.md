\section{Introduction}
The association between eye movements and mental activity is a fundamental topic of interest in attention research that has provided a foundation for developing a wide range of human assistive technologies. Early work by @yarbusEyeMovementsVision1967a showed that eye movement patterns appear to differ qualitatively depending on the task-at-hand [for a review of this work, see @tatlerYarbusEyeMovements2010a]. A replication of this work by @deangelusTopdownControlEye2009a showed that the differences in eye movements between tasks can be quantified, and appear to be somewhat generalizable. Technological advances and improvements in computing power have allowed researchers to make inferences regarding the task using eye movement data, also known as the "inverse Yarbus process" [@haji-abolhassaniInverseYarbusProcess2014c].

Current state-of-the-art machine learning and neural network algorithms are capable of identifying diagnostic patterns for the purpose of decoding a variety of data types, but the inner workings of the resulting model solutions are difficult or impossible to interpret. Algorithms that provide such solutions are referred to as _black box_ models. Dissections of black box models have been largely uninformative [@zhouComparingInterpretabilityDeep2019a], limiting the potential for researchers to apply the mechanisms underlying successful classification of the data. Still, black box models provide a powerful solution for technological applications such as human-computer interfaces [HCI; for a review, see @lukanderInferringIntentAction2017c]. While the internal operations of the model solutions used for HCI applications do not necessarily need to be interpretable to serve their purpose, @lukanderInferringIntentAction2017c pointed out that the inability to interpret the mechanisms underlying the function of black box solutions impedes the generalizability of these methods, and increases the difficulty of expanding these findings to real life applications<!--"the black box nature of the resulting solution impedes generalizability, and makes applying methods across real life conditions more difficult" (p. 44)-->. To ground these solutions, researchers guide decoding efforts by using eye movement data and/or models with built-in theoretical assumptions. For instance, eye movement data is processed into meaningful aggregate properties such as fixations or saccades, or statistical features such as fixation density, and the models used to decode these data are structured based on the current understanding of relevant cognitive or neurobiological processes [e.g., @macinnesjosephGenerativeModelCognitive2018a]. Despite the proposed disadvantages of black box approaches to classifying eye movement data, there is no clear evidence to support the notion that the grounded solutions described above are actually more valid or definitive than a black box solution.

The scope of theoretically informed solutions to decoding eye movement data is limited to the extent of the current theoretical knowledge linking eye movements to cognitive and neurobiological processes. As our theoretical understanding of these processes develops, older theoretically informed models become outdated. Furthermore, these solutions are susceptible to any inaccurate preconceptions that are built into the theory. Consider the case of @greeneReconsideringYarbusFailure2012c, who were not able to classify task from commonly used aggregate eye movement features (i.e., number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations) using correlations, a linear discriminant model, and a support vector machine (see Table \@ref(tab:previous-studies)). This led Greene and colleagues to question the robustness of Yarbus's (1967) findings, inspiring a slew of responses that successfully decoded the same dataset by aggregating the eye movements into different feature sets or implementing different model architectures [see Table \@ref(tab:previous-studies); @haji-abolhassaniInverseYarbusProcess2014c; @kananPredictingObserverTask2014a; @borjiDefendingYarbusEye2014]. The subsequent re-analyses of these data support @yarbusEyeMovementsVision1967a and the notion that task can be decoded from eye movement data using a variety of combinations of data features and model architectures. Collectively, these re-analyses did not point to an obvious global solution capable of clarifying future approaches to the inverse Yarbus problem beyond what could be inferred from black box model solutions, but did provide a wide-ranging survey of a variety of methodological features that can be applied to theoretical or black box approaches.

Eye movements can only delineate tasks to the extent that the cognitive processes underlying the tasks can be differentiated [@krolRightLookJob2018a]. Every task is associated with a unique set of cognitive processes [@krolRightLookJob2018a; @cocoClassificationVisualLinguistic2014a], but in some cases, the cognitive processes for different tasks may produce indistinguishable eye movement patterns. Others may define these terms differently, but for present purposes, our working definitions are that cognitive "processes" are theoretical constructs that could be difficult to isolate in practice, whereas a "task" is a more concrete/explicit set of goals and behaviors imposed by the experimenter in an effort to operationalize one or more cognitive processes. A "mental state," in contrast, is also a more theoretical term that is a bit more general and could include goals and cognitive processes, but could also presumably encompass other elements like mood or distraction.

To differentiate the cognitive processes underlying task-evoked eye movements, some studies have chosen to classify tasks that rely on stimuli that prompt easily distinguishable eye movements, such as reading text [e.g., @hendersonPredictingCognitiveState2013c]. The eye movements elicited by salient stimulus features facilitate task classifications; however, because these eye movements are the consequence of a feature (or features) inherent to the stimulus rather than the task, it is unclear if these classifications are attributable to the stimulus or a complex mental state [e.g., @hendersonPredictingCognitiveState2013c; @boisvertPredictingTaskEye2016a]. Additionally, the distinct nature of exogenously elicited eye movements prompts decoding algorithms to prioritize these bottom-up patterns in the data over higher-level top-down effects [@borjiDefendingYarbusEye2014]. This means that these models are identifying the type of information that is being processed, but are not necessarily reflecting the mental state of the individual observing the stimulus. Eye movements that are the product of bottom-up processes have been reliably decoded, which is relevant for some HCI applications; however, in our view such efforts do not fit the spirit of the inverse Yarbus problem, as most groups seem to construe it. Namely, most attempts at addressing the inverse Yarbus problem are concerned with decoding higher-level abstract mental operations that can be applied to virtually any naturalistic image and are not necessarily dependent on specific structural elements of the stimuli (e.g., the highly regular, linear patterns of written text).

Currently, there is not a clearly established upper limit to how well cognitive task can be classified from eye movement data. Prior evidence has shown that the task-at-hand is capable of producing distinguishable eye movement features such as the total scan path length, total number of fixations, and the amount of time to the first saccade [@castelhanoViewingTaskInfluences2009a; @deangelusTopdownControlEye2009a]. Decoding accuracies within the context of determining task from eye movements typically range from chance performance<!-- (between 14.29\% and 33\%)--> to <!--59.64\% (when chance was 25\%;-->relatively robust classification (see Table \@ref(tab:previous-studies)). In one case, @cocoClassificationVisualLinguistic2014a categorized the same eye movement features used by @greeneReconsideringYarbusFailure2012c with respect to the relative contribution of latent visual or linguistic components of three tasks (visual search, name the picture, name objects in the picture) with 84\% accuracy (chance = 33\%). While this manipulation is reminiscent of other experiments relying on the bottom-up influence of words and pictures [e.g., @hendersonPredictingCognitiveState2013c; @boisvertPredictingTaskEye2016a] the eye movements in the @cocoClassificationVisualLinguistic2014a tasks can be attributed to the occurrence of top-down attentional processes. A conceptually related follow-up to this study classified tasks along two spatial and semantic dimensions, resulting in 51\% classification accuracy (chance = 25\%) [@krolRightLookJob2018a]. A closer look at these results showed that the categories within the semantic dimension were consistently misclassified, suggesting that this level of distinction may require a richer dataset, or a more powerful decoding algorithm. Altogether, there is no measurable index of relative top-down or bottom-up influence, but this body of literature suggests that the relative influence of top-down and bottom-up attentional processes may have a role in determining the decodability of the eye movement data.

\begin{table}[!h]
    \centering
    \caption{Previous Attempts to Classify Cognitive Task Using Eye Movement Data}
    \label{tab:previous-studies}
    \resizebox{!}{0.35\paperheight}{
        \begin{tabular}{>{\raggedright\arraybackslash}p{.13\textwidth} >{\raggedright\arraybackslash}p{.17\textwidth} p{.35\textwidth} >{\raggedright\arraybackslash}p{.2\textwidth} >{\centering\arraybackslash}p{.13\textwidth}}
            \multicolumn{1}{c}{Study} & \multicolumn{1}{c}{Tasks} & \multicolumn{1}{c}{Features} & \multicolumn{1}{c}{Model Architecture} & Accuracy (Chance) \\
            \hline
            Greene et al. (2012) & memorize, decade, people, wealth & number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations, dwell times & linear discriminant, correlation, SVM & 25.9\% (25\%) \\
            Haji-Abolhassani \& James (2014) & Greene et al. tasks & fixation clusters & Hidden Markov Models & 59.64\% (25\%) \\
            Kanan et al. (2014) & Greene et al. tasks & mean fixation durations, number of fixations & multi-fixation pattern analysis & 37.9\% (25\%) \\
            Borji \& Itti (2014) & Greene et al. tasks & number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations, first five fixations, fixation density & kNN, RUSBoost & 34.34\% (25\%) \\
            Borji \& Itti (2014) & Yarbus tasks (i.e., view, wealth, age, prior activity, clothes, location, time away) & number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations, first five fixations, fixation density & kNN, RUSBoost & 24.21\% (14.29\%) \\
            Coco \& Keller (2014) & search, name picture, name object & Greene et al. features, latency of first fixation, first fixation duration, mean fixation duration, total gaze duration, initiation time, mean saliency at fixation, entropy of attentional landscape & MM, LASSO, SVM & 84\% (33\%) \\
            MacInnes et al. (2018) & view, memorize, search, rate & saccade latency, saccade duration, saccade amplitude, peak saccade velocity, absolute saccade angle, pupil size & augmented Naive Bayes Network & 53.9\% (25\%) \\
            Król \& Król (2018) & people, indoors/outdoors, white/black, search & eccentricity, screen coverage & feed forward neural network & 51.4\% (25\%) \\
            \hline
    \end{tabular}}
\end{table}

As shown in Table \@ref(tab:previous-studies), when eye movement data are prepared for classification, fixation and saccade statistics are typically aggregated along spatial or temporal dimensions, resulting in variables such as fixation density or saccade amplitude [@millsExaminingInfluenceTask2011a; @macinnesjosephGenerativeModelCognitive2018a; @castelhanoViewingTaskInfluences2009a]. The implementation of these statistical methods is meant to explicitly provide the decoding algorithm with characteristics of the eye movement data that are representative of theoretically relevant cognitive processes. For example, @macinnesjosephGenerativeModelCognitive2018a attempted to provide an algorithm with data designed to be representative of inputs to the frontal eye fields. In some instances, such as the case of @krolRightLookJob2018a, grounding the data using theoretically driven aggregation methods may require sacrificing granularity in the dataset. This means that aggregating the data has the potential to wash out certain fine-grained distinctions that could otherwise be detected. Data structures of any kind can only be decoded to the extent to which the data are capable of representing differences between categories. Given that the cognitive processes underlying distinct tasks are often overlapping [@cocoClassificationVisualLinguistic2014a], decreasing the granularity of the data may actually limit the potential of the algorithm to make fine-grained distinctions between diagnostic components underlying the tasks to be decoded.

The current state of the literature does not provide any firm guidelines for determining what eye movement features are most meaningful, or what model architectures are best suited for determining task from eye movements. The examples provided in Table \@ref(tab:previous-studies) used a variety of eye movement features and model architectures, most of which were effective to some extent. A proper comparison of these outcomes is difficult because these datasets vary in levels of chance and data quality. Datasets with more tasks to be classified have lower levels of chance, lowering the threshold for successful classification. Additionally, datasets with a lower signal-to-noise ratio will have a lower achievable classification accuracy. For these reasons, outside of re-analyzing the same datasets, there is no consensus on how to establish direct comparisons of these model architectures. Given the inability to directly compare the relative effectiveness of the various theoretical approaches present in the literature, the current study addressed the inverse Yarbus problem by allowing a black box model to self-determine the most informative features from minimally processed eye movement data.

The current study explored pragmatic solutions to the problem of classifying task from eye movement data by submitting minimally processed x-coordinate, y-coordinate, and pupil size data to a convolutional neural network (CNN) model. Instead of transforming the data into theoretically defined units, we allowed the network to learn meaningful patterns in the data on its own. CNNs have a natural propensity to develop low-level feature detectors similar to the primary visual cortex [e.g., @seeligerConvolutionalNeuralNetworkbased2018a]; for this reason, they<!--CNNs--> are commonly implemented for image classification. In some cases, researchers have found success classifying data that natively exist in a timeline format by first transforming the data to an image-based format and then passing it to a deep neural network classifier [e.g., @bashivanLearningRepresentationsEEG2016]; however, it is not always obvious a priori which representation of a particular type of data is best-suited for neural network classifiers to be able to detect informative features, and the ideal representational format must be determined empirically. Thus, to test the possibility that image data might be better suited to the CNN classifier in our eye movement data as well, we also transformed our dataset from raw timelines into simple image representations and compared CNN-based classification of timeline data to that of image data. The image representations we generated also matched the eye movement trace images classically associated with the work of Yarbus (1967) and others, which were the original forays into this line of inquiry.

To our knowledge, no study has attempted to address the inverse Yarbus problem using any combination of the following methods: (1) Non-aggregated data, (2) image data format, and (3) a black-box CNN architecture. Given that CNN architectures are capable of learning features represented in raw data formats, and are well-suited to decoding multidimensional data that have a distinct spatial or temporal structure, we expected that a non-theoretically-constrained CNN architecture could be capable of decoding data at levels consistent with the current state of the art. Furthermore, despite evidence that black box approaches to the inverse Yarbus problem can impede generalizability [@lukanderInferringIntentAction2017c], we expected that when testing the approach on an entirely separate dataset, providing the model with minimally processed data and the flexibility to identify the unique features within each dataset would result in the replication of our initial findings.
