\section{Background}
The association between eye movements and mental activity is a fundamental topic of interest in attention research that has provided a foundation for developing a wide range of human assistive technologies. Early work by @yarbusEyeMovementsVision1967 showed that eye movement patterns appear to differ qualitatively depending on the task-at-hand [for a review of this work, see @tatlerYarbusEyeMovements2010]. A replication of this work by @deangelusTopdownControlEye2009 shows that the differences in eye movements between tasks can be quantified, and appear to be somewhat generalizable. Technological advances and improvements in computing power have allowed researchers to make inferences regarding the mental state underlying eye movement data, also known as the "inverse Yarbus process" [@haji-abolhassaniInverseYarbusProcess2014]. Current state-of-the-art machine learning and neural network algorithms are capable of identifying diagnostic patterns for the purpose of decoding a variety of data types, but the inner workings of the resulting model solutions are difficult or impossible to interpret. Algorithms that provide such solutions are referred to as _black box_ models. Dissections of black box models have been largely uninformative [@zhouComparingInterpretabilityDeep2019], limiting the potential for researchers to apply the mechanisms underlying successful classification of the data. Still, black box models provide a powerful solution for techonological applications such as human-computer interfaces [HCI; for a review, see @lukanderInferringIntentAction2017]. While the internal operations of the model solutions used for HCI applications do not necessarily need to be interpretable to serve their purpose, @lukanderInferringIntentAction2017 pointed out that "the black box nature of the resulting solution impedes generalizability, and makes applying methods across real life conditions more difficult" (p. 44). To ground these solutions, researchers guide decoding efforts by using eye movement data and/or models with built-in theoretical assumptions. For instance, eye movement data is processed into meaningful aggregate properties such as fixations or saccades, or statistical features such as as fixation density, and the models used to decode these data are structured based on the current understanding of relevant cognitive or neurobiological processes [e.g., @macinnesjosephGenerativeModelCognitive2018]. Despite the proposed disadvantages of black box approaches to classifying eye movement data, there is no clear evidence to support the notion that the grounded solutions described above are actually more valid or definitive than a black box solution.

The scope of theoretically informed solutions to decoding eye movement data are limited to the extent of the current theoretical knowledge linking eye movements to cognitive and neurobiological processes. As our theoretical understanding of these processes develops, older theoretically informed models become outdated. Furthermore, these solutions are suceptible to any inaccurate preconceptions that are built into the theory. Consider the case of @greeneReconsideringYarbusFailure2012a, who were not able to classify the task from commonly used aggregate eye movement features (i.e., number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations) using correlations, a linear discriminant model, and a support vector machine (see Table \@ref(tab:previous-studies)). This led Greene and colleagues to question the robustness of Yarbus's (1967) findings, inspiring a slew of responses that successfully decoded the same dataset by aggregating the eye movements into different feature sets or implementing different model architectures [see Table \@ref(tab:previous-studies); i.e., @haji-abolhassaniInverseYarbusProcess2014; @kananPredictingObserverTask2014; @borjiDefendingYarbusEye2014a]. The subsequent re-analyses of these data support @yarbusEyeMovementsVision1967 and the notion that mental state can be decoded from eye movement data using a variety of combinations of data features and model architectures. Altogether, these re-anlayses did not point to an obvious global solution capable of clarifying future approaches to the inverse Yarbus problem beyond what could be inferred from black box model solutions, but did provide a rigorous test of a variety of methodological features which can be applied to theoretical or black box approaches to the inverse Yarbus problem.

Eye movements can only delineate tasks to the extent that the cognitive processes underlying the tasks can be differentiated [@krolRightLookJob2018]. Every task is associated with a unique set of cognitive processes [@krolRightLookJob2018; @cocoClassificationVisualLinguistic2014], but in some cases, the cognitive processes for different tasks may produce indistinguishable eye movement patterns. To differentiate the cognitive processes underlying task-evoked eye movements, some studies have chosen to classify tasks that rely on stimuli that prompt easily distinguishable eye movements, such as reading text and searching pictures [e.g., @hendersonPredictingCognitiveState2013a]. The eye movements elicited by salient stimulus features facilitate task classifications, but because these eye movements are the consequence of a feature, or features, inherent to the stimulus rather than the task, it is unclear if these classifications are attributable to the stimulus or a complex mental state [e.g., @hendersonPredictingCognitiveState2013a; @boisvertPredictingTaskEye2016]. Additionally, the distinct nature of exogenously elicited eye movements prompts decoding algorithms to prioritize these bottom-up patterns in the data over higher-level top-down effects [@borjiDefendingYarbusEye2014a]. This means that these models are identifying the type of information that is being processed, but are not necessarily reflecting the mental state of the individual observing the stimulus. Eye movements that are the product of bottom-up processes have been reliably decoded, which is relevant for some HCI applications, but does not fit the nature of the inverse Yarbus problem, which is concerned with decoding high-level abstract mental operations that are not dependent on particular stimuli.

Currently, an upper limit to how well cognitive task can be classified from eye movement data has not been clearly established. Prior evidence has shown that the task-at-hand is capable of producing distinguishable eye movement features such as the total scan path length, total number of fixations, and the amount of time to the first saccade [@castelhanoViewingTaskInfluences2009; @deangelusTopdownControlEye2009]. Decoding accuracies within the context of determining task from eye movements typically range from chance performance (between 14.29\% and 33\%) to 59.64\% (see Table \@ref(tab:previous-studies)). In one case, @cocoClassificationVisualLinguistic2014 categorized the same eye movement features used by @greeneReconsideringYarbusFailure2012a with respect to the relative contribution of latent visual or linguistic components of three tasks (visual search, name the picture, name objects in the picture) with 84\% accuracy. While this manipulation is reminiscent of other experiments relying on the bottom-up influence of words and pictures [e.g., @hendersonPredictingCognitiveState2013a; @boisvertPredictingTaskEye2016] the eye movements in the @cocoClassificationVisualLinguistic2014 tasks were entirely the product of top-down processes. A conceptually similar follow-up to this study classified tasks along two spatial and semantic dimensions, resulting in 51\% classification accuracy [chance = 25\%; @krolRightLookJob2018]. A closer look at these results showed that the categories within the semantic dimension were consistently misclassified, suggesting that this level of distinction may require a richer dataset, or a more powerful decoding algorithm. Altogether, there is no measurable index of relative top-down or bottom-up influence, but this body of literature suggests that the relative influence of top-down and bottom-up attentional processes may have a role in determining the decodability of the eye movement data.

\begin{table}[!h]
    \centering
    \caption{Previous Attempts to Classify Cognitive Task Using Eye Movement Data}
    \label{tab:previous-studies}
    \resizebox{!}{0.35\paperheight}{
        \begin{tabular}{>{\raggedright\arraybackslash}p{.13\textwidth} >{\raggedright\arraybackslash}p{.17\textwidth} p{.35\textwidth} >{\raggedright\arraybackslash}p{.2\textwidth} >{\centering\arraybackslash}p{.13\textwidth}}
            \multicolumn{1}{c}{Study} & \multicolumn{1}{c}{Tasks} & \multicolumn{1}{c}{Features} & \multicolumn{1}{c}{Model Architecture} & Accuracy (Chance) \\
            \hline
            Greene et al. (2012) & memorize, decade, people, wealth & number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations, dwell times & linear discriminant, correlation, SVM & 25.9\% (25\%) \\
            Haji-Abolhassani \& James (2014) & Greene et al. tasks & fixation clusters & Hidden Markov Models & 59.64\% (25\%) \\
            Kanan et al. (2014) & Greene et al. tasks & mean fixation durations, number of fixations & multi-fixation pattern analysis & 37.9\% (25\%) \\
            Borji \& Itti (2014) & Greene et al. tasks & number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations, first five fixations, fixation density & kNN, RUSBoost & 34.34\% (25\%) \\
            Borji \& Itti (2014) & Yarbus tasks (i.e., view, wealth, age, prior activity, clothes, location, time away) & number of fixations, mean fixation duration, mean saccade amplitude, percent of image covered by fixations, first five fixations, fixation density & kNN, RUSBoost & 24.21\% (14.29\%) \\
            Coco \& Keller (2014) & search, name picture, name object & Greene et al. features, latency of first fixation, first fixation duration, mean fixation duration, total gaze duration, initiation time, mean saliency at fixation, entropy of attentional landscape & MM, LASSO, SVM & 84\% (33\%) \\
            MacInnes et al. (2018) & view, memorize, search, rate & saccade latency, saccade duration, saccade amplitude, peak saccade velocity, absolute saccade angle, pupil size & augmented Naive Bayes Network & 53.9\% (25\%) \\
            Król \& Król (2018) & people, indoors/outdoors, white/black, search & eccentricity, screen coverage & feed forward neural network & 51.4\% (25\%) \\
            \hline
    \end{tabular}}
\end{table}

As shown in Table \@ref(tab:previous-studies), when eye movement data are prepared for classification, fixation and saccade statistics are typically aggregated along spatial or temporal dimensions, resulting in variables such as fixation density or saccade amplitude [@millsExaminingInfluenceTask2011; @macinnesjosephGenerativeModelCognitive2018; @castelhanoViewingTaskInfluences2009]. The implementation of these statistical methods is meant to explicitly provide the decoding algorithm with characteristics of the eye movement data that are representative of theoretically relevant cognitive processes. For example, @macinnesjosephGenerativeModelCognitive2018 attempted to provide an algorithm with data designed to be representative of inputs to the frontal eye fields. In some instances, such as the case of @krolRightLookJob2018, grounding the data using theoretically driven aggregation methods may require sacrificing granularity in the dataset. This means that aggregating the data has the potential to wash out certain fine-grained distinctions that could otherwise be detected. Data structures of any kind can only be decoded to the extent at which the data are capable of representing differences between categories. Given that the cognitive processes underlying distinct tasks are often overlapping [@cocoClassificationVisualLinguistic2014], decreasing the granularity of the data may actually limit the potential of the algorithm to make fine-grained distinctions between diagnostic components underlying the target task and the other tasks.

The current state of the literature does not provide any firm guidelines for determining what eye movement features are most meaningful, or what model architectures are most suited for determining mental state from eye movements. The examples provided in Table \@ref(tab:previous-studies) used a variety of eye movement features and model architectures, most of which were effective to some extent [with the exception of @greeneReconsideringYarbusFailure2012a]. A proper comparison of these outcomes is difficult because these datasets vary in levels of chance and data quality. Datasets with more tasks to be classified have lower levels of chance, lowering the threshold for successful classification. Additionally, datasets with a lower signal-to-noise ratio will have a lower achievable classification accuracy. For these reasons, outside of re-analyzing the same datasets, there is no consensus on how to establish direct comparisons of these model architectures. Given the inability to diectly compare the relative effectiveness of the various theoretical approaches present in the literature, the current study addressed the inverse Yarbus problem by allowing a black box model to self-determine the most informative features from minimally processed eye movement data.

The current study explored pragmatic solutions to the problem of classifying task from eye movement data by submitting unprocessed x-coordinate, y-coordinate, and pupil size data to a convolutional neural network (CNN) model. Instead of transforming the data into theoretically defined units, we allowed the network to learn meaningful patterns in the data on its own. CNNs have a natural propensity to develop low-level feature detectors similar to primary visual cortex [e.g., @seeligerConvolutionalNeuralNetworkbased2018]; for this reason, they<!--CNNs--> are commonly implemented for image classification. To test the possibility that the image data are better suited to the CNN classifier, the data were also transformed into raw timeline and simple image representations. To our knowledge, no study has attempted to address the inverse Yarbus problem using any combination of the following methods: (1) Non-aggregated data, (2) image data format, and (3) a black-box CNN architecture. Given that CNN architectures are capable of learning features represented in raw data formats, and are well-suited to decoding multidimensional data that have a distinct spatial or temporal structure, we expected that a non-theoretically-constrained CNN architecture could be capable of decoding data at levels consistent with the current state of the art. Furthermore, despite evidence that black box approaches to the inverse Yarbus problem can impede generalizability [@lukanderInferringIntentAction2017], we expected that when testing the approach on an entirely separate dataset, providing the model with minimally processed data and the flexibility to identify the unique features within each dataset would result in the replication of our initial findings.
