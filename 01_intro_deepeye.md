\section{Introduction}

A goal of many cognitive neuroscientists is to infer mental activity from human eye movements [@deangelusTopdownControlEye2009]. Foundational work by Yarbus (1967) showed that eye movement patterns appear to differ qualitatively depending on the task at hand. Technological advances and improvements in computing power have allowed researchers to determine the mental state underlying eye movement data, also known as the "inverse Yarbus process" [@haji-abolhassaniInverseYarbusProcess2014]. Current state-of-the-art machine learning and neural network algorithms that are capable of identifying diagnostic patterns in the data <!--beyond the constraints of any grounded theoretical assumptions--> appear to trade one inverse problem for another [@lukanderInferringIntentAction2017]. As pointed out by @lukanderInferringIntentAction2017, "the black box nature of the resulting solution impedes generalizability, and makes applying methods across real life conditions more difficult" (p. 44). To ground these solutions, researchers guide "black box" decoding efforts by providing data and/or models with built-in theoretical assumptions. At this point, there is no clear evidence to support the notion that the commonly applied theoretical constraints actually enhance or clarify "black box" solutions beyond what could be inferred from an unconstrained model <!--NOTE: Lukander would say that unconstrained models are less reliable (generalizable), so maybe we need to argue that testing on multiple datasets provides the argument for generalizability-->.

Consider the case of @greeneReconsideringYarbusFailure2012a, who failed to classify task from commonly used aggregate eye movement features using three separate model architectures. This led Greene and colleagues to question the robustness of Yarbus's (1967) findings, inspiring a slew of responses that successfully decoded the same dataset using different eye movement features, or different model architectures [e.g., @haji-abolhassaniInverseYarbusProcess2014; @kananPredictingObserverTask2014; @borjiDefendingYarbusEye2014a]. The subsequent re-analysis of these data support Yarbus (1967), but lack a unifying theoretical framework that comprehensively explains the failures of Greene et al. and the successes of the subsequent re-analyses<!--in classifying the cognitive processes underlying the eye movement data-->.

In general, the selection of tasks, data, and decoding algorithms have been implicated separately in the successful decoding of eye movement data<!-- REFERENCE? -->. Eye movements can only be differentiated to the extent that the cognitive processes underlying the tasks can be delineated [@krolRightLookJob2018]. Every task is associated with a unique set of cognitive processes [@krolRightLookJob2018; @cocoClassificationVisualLinguistic2014]. To distinguish the cognitive processes underlying task invoked eye movements, some studies have chosen to classify tasks that rely on distinct exogenous influences on attention, such as reading text and searching pictures [e.g., @hendersonPredictingCognitiveState2013a]. When the cognitive impetus for the task is the product of exogenous attention, the resulting eye movements thought to reflect complex mental state may actually be confounded by the presence of salient stimulus features [@hendersonPredictingCognitiveState2013a; @boisvertPredictingTaskEye2016]. Additionally, decoding algorithms appear to prioritize these bottom-up patterns in the data over higher-level top-down effects [@borjiDefendingYarbusEye2014a]. For this reason, eye movements associated with exogenously oriented tasks can be reliably decoded<!-- REFERENCES?? -->, but do not fit the implied top-down nature of the inverse Yarbus problem.

Despite a lack of objective criteria differentiating the mental activity underlying these endogenously oriented task sets, a comparison of aggregate eye movement features confirmed that eye movements can be differentially influenced by the task at hand [@castelhanoViewingTaskInfluences2009; @deangelusTopdownControlEye2009]. Decoding of similar eye movement features under similar task conditions has produced classification accuracies typically ranging from chance performance to 59.64% (see Table x). In one case, @cocoClassificationVisualLinguistic2014 categorized eye movements based on visual or linguistic components of three tasks, resulting in 84% accuracy. A recent follow-up using a different task set categorized four tasks according to two objective spatial and semantic processing dimensions with 51% accuracy [@krolRightLookJob2018]. A closer look at these results showed that the categories within the semantic dimension were consistently mixed up, suggesting that this level distinction may require a more rich dataset, or a more powerful decoding algorithm.

<!-- \insert{table_x} -->
<!-- TABLE X: breakdown of other studies: the tasks they looked at, the algorithms they used, and the acc
> Greene et al.: memory, decade, people, wealth; linear discriminant, correlation, SVM; 25.9% - chance = 25%
> Haji-Abolhassani & Clark: Greene et al. tasks; Hidden Markov Models; 59.64% - chance = 25%
> Kanan et al.: Greene et al. tasks; multi-fixation pattern analysis; 37.9% - chance = 25%
> Borji & Itti: Greene et al. tasks; kNN, RUSBoost; 34.24% - chance = 25%
> Borji & Itti: Yarbus tasks; kNN, RUSBoost; 24.21% - chance = 14.29%
> Coco & Keller: 84% - chance = 33%
> MacInnes et al.: view, memorize, search, preference; augmented Naive Bayes Network; 53.9% - chance = 25%
> Krol & Krol: people, indoors/outdoors, white/black, dot search; feed forward neural network (doesn't specify if CNN); 51.4% - chance = 25%
*this table is going to be very similar to Boisvert & Bruce -- but more up to date, and more specific about the studies that are included
-->

To prepare eye movement data for classification, fixation and saccade statistics are typically aggregated along spatial or temporal dimensions, resulting variables such as fixation density or saccade amplitude [@millExaminingInfluenceTask2011; @josephmacinnesGenerativeModelCognitive2018; @castelhanoViewingTaskInfluences2009]. Implementing these statistical methods is meant to explicitly focus the algorithm on characteristics of the eye movement data that are representative of theoretically relevant cognitive processes [e.g., @josephmacinnesGenerativeModelCognitive2018]. In some instances, such as the case of @krolRightLookJob2018, aggregating the data may ground the data structure theoretically while sacrificing resolution. Given the cognitive processes underlying distinct tasks are often overlapping [@cocoClassificationVisualLinguistic2014], decreasing the resolution of the data may actually limit the potential of the algorithm to decode the eye movement data<!-- is there a reference for this? or is this just me? -->.

The current study aims to maximize the potential of the data by submitting unprocessed x-coordinate, y-coordinate, and pupil size data to a convolutional neural network (CNN) model. CNNs have a natural propensity to develop low level feature detectors similar to primary visual cortex (<!-- REF -->). For this reason, CNNs are commonly implemented for image classification<!-- REF? -->. To test the possibility that the image data are more suited to the CNN classifier, the data will be decoded in raw timeline and image formats. To our knowledge, no study has attempted to address the inverse Yarbus problem <!--decode mental task from endogenously oriented eye movement data-->using: (1) non-aggregate data, (2) image data format, or (3) a CNN architecture. Given that CNN classification performance is robust to multidimensional, non-structured data (<!-- REF -->), we expect the non-theoretically-constrained CNN architecture to decode both data types at levels consistent with the current state-of-the-art. Furthermore, we expect that despite the claims that "black box" approaches to the inverse Yarbus problem are unreliable [@lukanderInferringIntentAction2017], our initial findings will replicate when tested on an entirely separate dataset.<!-- << should this paragraph be past-tense (except the hypotheses)? -->
